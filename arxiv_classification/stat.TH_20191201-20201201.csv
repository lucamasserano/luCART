"arxiv_category","arxiv_abstract"
"stat.TH","  The method of covariate adjustment is often used for estimation of population
average treatment effects in observational studies. Graphical rules for
determining all valid covariate adjustment sets from an assumed causal
graphical model are well known. Restricting attention to causal linear models,
a recent article derived two novel graphical criteria: one to compare the
asymptotic variance of linear regression treatment effect estimators that
control for certain distinct adjustment sets and another to identify the
optimal adjustment set that yields the least squares treatment effect estimator
with the smallest asymptotic variance among consistent adjusted least squares
estimators. In this paper we show that the same graphical criteria can be used
in non-parametric causal graphical models when treatment effects are estimated
by contrasts involving non-parametrically adjusted estimators of the
interventional means. We also provide a graphical criterion for determining the
optimal adjustment set among the minimal adjustment sets, which is valid for
both linear and non-parametric estimators. We provide a new graphical criterion
for comparing time dependent adjustment sets, that is, sets comprised by
covariates that adjust for future treatments and that are themselves affected
by earlier treatments. We show by example that uniformly optimal time dependent
adjustment sets do not always exist. In addition, for point interventions, we
provide a sound and complete graphical criterion for determining when a
non-parametric optimally adjusted estimator of an interventional mean, or of a
contrast of interventional means, is as efficient as an efficient estimator of
the same parameter that exploits the information in the conditional
independencies encoded in the non-parametric causal graphical model.
"
"stat.TH","  We develop algebraic tools for statistical inference from samples of rotation
matrices. This rests on the theory of D-modules in algebraic analysis.
Noncommutative Gr\""obner bases are used to design numerical algorithms for
maximum likelihood estimation, building on the holonomic gradient method of
Sei, Shibata, Takemura, Ohara, and Takayama. We study the Fisher model for
sampling from rotation matrices, and we apply our algorithms for data from the
applied sciences. On the theoretical side, we generalize the underlying
equivariant D-modules from SO(3) to arbitrary Lie groups. For compact groups,
our D-ideals encode the normalizing constant of the Fisher model.
"
"stat.TH","  Anisotropic functional deconvolution model is investigated in the bivariate
case under long-memory errors when the design points $t_i$, $i=1, 2, \cdots,
N$, and $x_l$, $l=1, 2, \cdots, M$, are irregular and follow known densities
$h_1$, $h_2$, respectively. In particular, we focus on the case when the
densities $h_1$ and $h_2$ have singularities, but $1/h_1$ and $1/h_2$ are still
integrable on $[0, 1]$. Under both Gaussian and sub-Gaussian errors, we
construct an adaptive wavelet estimator that attains asymptotically
near-optimal convergence rates that deteriorate as long-memory strengthens. The
convergence rates are completely new and depend on a balance between the
smoothness and the spatial homogeneity of the unknown function $f$, the degree
of ill-posed-ness of the convolution operator, the long-memory parameter in
addition to the degrees of spatial irregularity associated with $h_1$ and
$h_2$. Nevertheless, the spatial irregularity affects convergence rates only
when $f$ is spatially inhomogeneous in either direction.
"
"stat.TH","  The Jensen-Shannon divergence is a renown bounded symmetrization of the
Kullback-Leibler divergence which does not require probability densities to
have matching supports. In this paper, we introduce a vector-skew
generalization of the scalar $\alpha$-Jensen-Bregman divergences and derive
thereof the vector-skew $\alpha$-Jensen-Shannon divergences. We study the
properties of these novel divergences and show how to build parametric families
of symmetric Jensen-Shannon-type divergences. Finally, we report an iterative
algorithm to numerically compute the Jensen-Shannon-type centroids for a set of
probability densities belonging to a mixture family: This includes the case of
the Jensen-Shannon centroid of a set of categorical distributions or normalized
histograms.
"
"stat.TH","  We give a complete characterization of the sampling complexity of best
Markovian arm identification in one-parameter Markovian bandit models. We
derive instance specific nonasymptotic and asymptotic lower bounds which
generalize those of the IID setting. We analyze the Track-and-Stop strategy,
initially proposed for the IID setting, and we prove that asymptotically it is
at most a factor of four apart from the lower bound. Our one-parameter
Markovian bandit model is based on the notion of an exponential family of
stochastic matrices for which we establish many useful properties. For the
analysis of the Track-and-Stop strategy we derive a novel concentration
inequality for Markov chains that may be of interest in its own right.
"
"stat.TH","  Conditional mean embeddings (CMEs) have proven themselves to be a powerful
tool in many machine learning applications. They allow the efficient
conditioning of probability distributions within the corresponding reproducing
kernel Hilbert spaces (RKHSs) by providing a linear-algebraic relation for the
kernel mean embeddings of the respective joint and conditional probability
distributions. Both centred and uncentred covariance operators have been used
to define CMEs in the existing literature. In this paper, we develop a
mathematically rigorous theory for both variants, discuss the merits and
problems of each, and significantly weaken the conditions for applicability of
CMEs. In the course of this, we demonstrate a beautiful connection to Gaussian
conditioning in Hilbert spaces.
"
"stat.TH","  Bayesian inference problems require sampling or approximating
high-dimensional probability distributions. The focus of this paper is on the
recently introduced Stein variational gradient descent methodology, a class of
algorithms that rely on iterated steepest descent steps with respect to a
reproducing kernel Hilbert space norm. This construction leads to interacting
particle systems, the mean-field limit of which is a gradient flow on the space
of probability distributions equipped with a certain geometrical structure. We
leverage this viewpoint to shed some light on the convergence properties of the
algorithm, in particular addressing the problem of choosing a suitable positive
definite kernel function. Our analysis leads us to considering certain
nondifferentiable kernels with adjusted tails. We demonstrate significant
performs gains of these in various numerical experiments.
"
"stat.TH","  Measuring conditional independence is one of the important tasks in
statistical inference and is fundamental in causal discovery, feature
selection, dimensionality reduction, Bayesian network learning, and others. In
this work, we explore the connection between conditional independence measures
induced by distances on a metric space and reproducing kernels associated with
a reproducing kernel Hilbert space (RKHS). For certain distance and kernel
pairs, we show the distance-based conditional independence measures to be
equivalent to that of kernel-based measures. On the other hand, we also show
that some popular---in machine learning---kernel conditional independence
measures based on the Hilbert-Schmidt norm of a certain cross-conditional
covariance operator, do not have a simple distance representation, except in
some limiting cases. This paper, therefore, shows the distance and kernel
measures of conditional independence to be not quite equivalent unlike in the
case of joint independence as shown by Sejdinovic et al. (2013).
"
"stat.TH","  Chi-squared tests for lack of fit are traditionally employed to find evidence
against a hypothesized model, with the model accepted if the Karl Pearson
statistic comparing observed and expected numbers of observations falling
within cells is not significantly large. However, if one really wants evidence
for goodness of fit, it is better to adopt an equivalence testing approach in
which small values of the chi-squared statistic are evidence for the desired
model. This method requires one to define what is meant by equivalence to the
desired model, and guidelines are proposed. Then a simple extension of the
classical normalizing transformation for the non-central chi-squared
distribution places these values on a simple to interpret calibration scale for
evidence. It is shown that the evidence can distinguish between normal and
nearby models, as well between the Poisson and over-dispersed models.
Applications to evaluation of random number generators and to uniformity of the
digits of pi are included. Sample sizes required to obtain a desired expected
evidence for goodness of fit are also provided.
"
"stat.TH","  Sure screening technique has been considered as a powerful tool to handle the
ultrahigh dimensional variable selection problems, where the dimensionality p
and the sample size n can satisfy the NP dimensionality log p=O(n^a) for some
a>0 (Fan & Lv 2008). The current paper aims to simultaneously tackle the
""universality"" and ""effectiveness"" of sure screening procedures. For the
""universality"", we develop a general and unified framework for nonparametric
screening methods from a loss function perspective. Consider a loss function to
measure the divergence of the response variable and the underlying
nonparametric function of covariates. We newly propose a class of loss
functions called conditional strictly convex loss, which contains, but is not
limited to, negative log likelihood loss from one-parameter exponential
families, exponential loss for binary classification and quantile regression
loss. The sure screening property and model selection size control will be
established within this class of loss functions. For the ``effectiveness"", we
focus on a goodness of fit nonparametric screening (Goffins) method under
conditional strictly convex loss. Interestingly, we can achieve a better
convergence probability of containing the true model compared with related
literature. The superior performance of our proposed method has been further
demonstrated by extensive simulation studies and some real scientific data
example.
"
"stat.TH","  This paper proposes a new estimator for selecting weights to average over
least squares estimates obtained from a set of models. Our proposed estimator
builds on the Mallows model average (MMA) estimator of Hansen (2007), but,
unlike MMA, simultaneously controls for location bias and regression error
through a common constant. We show that our proposed estimator-- the mean-shift
Mallows model average (MSA) estimator-- is asymptotically optimal to the
original MMA estimator in terms of mean squared error. A simulation study is
presented, where we show that our proposed estimator uniformly outperforms the
MMA estimator.
"
"stat.TH","  Mediation analysis is concerned with the decomposition of the total effect of
an exposure on an outcome into the indirect effect through a given mediator,
and the remaining direct effect. This is ideally done using longitudinal
measurements of the mediator, as these capture the mediator process more
finely. However, longitudinal measurements pose challenges for mediation
analysis. This is because the mediators and outcomes measured at a given
time-point can act as confounders for the association between mediators and
outcomes at a later time-point; these confounders are themselves affected by
the prior exposure and outcome. Such post-treatment confounding cannot be dealt
with using standard methods (e.g. generalized estimating equations). Analysis
is further complicated by the need for so-called cross-world counterfactuals to
decompose the total effect. This article addresses these challenges. In
particular, we introduce so-called natural effect models, which parameterize
the direct and indirect effect of a baseline exposure w.r.t. a longitudinal
mediator and outcome. These can be viewed as a generalization of marginal
structural models to enable effect decomposition. We introduce inverse
probability weighting techniques for fitting these models, adjusting for
(measured) time-varying confounding of the mediator-outcome association.
Application of this methodology uses data from the Millennium Cohort Study, UK.
"
"stat.TH","  This note presents a simple proof of the characteristic function of Student's
$t$-distribution. The method of proof, which involves finding a differential
equation satisfied by the characteristic function, is applicable to many other
distributions.
"
"stat.TH","  We address the new problem of estimating a piece-wise constant signal with
the purpose of detecting its change points and the levels of clusters. Our
approach is to model it as a nonparametric penalized least square model
selection on a family of models indexed over the collection of partitions of
the design points and propose a computationally efficient algorithm to
approximately solve it. Statistically, minimizing such a penalized criterion
yields an approximation to the maximum a posteriori probability (MAP)
estimator. The criterion is then analyzed and an oracle inequality is derived
using a Gaussian concentration inequality. The oracle inequality is used to
derive on one hand conditions for consistency and on the other hand an adaptive
upper bound on the expected square risk of the estimator, which statistically
motivates our approximation. Finally, we apply our algorithm to simulated data
to experimentally validate the statistical guarantees and illustrate its
behavior.
"
"stat.TH","  In this note, we derive the closed form formulae for moments of Student's
t-distribution in the one dimensional case as well as in higher dimensions
through a unified probability framework. Interestingly, the closed form
expressions for the moments of Student's t-distribution can be written in terms
of the familiar Gamma function, Kummer's confluent hypergeometric function, and
the hypergeometric function.
"
"stat.TH","  The multivariate Hilbert-Schmidt-Independence-Criterion (dHSIC) and distance
multivariance allow to measure and test independence of an arbitrary number of
random vectors with arbitrary dimensions. Here we define versions which only
depend on an underlying copula. The approach is based on the distributional
transform, yielding dependence measures which always feature a natural
invariance with respect to scalings and translations. Moreover, it requires no
distributional assumptions, i.e., the distributions can be of pure type or any
mixture of discrete and continuous distributions and (in our setting) no
existence of moments is required.
  Empirical estimators and tests, which are consistent against all
alternatives, are provided based on a Monte Carlo distributional transform. In
particular, it is shown that the new estimators inherit the exact limiting
distributional properties of the original estimators. Examples illustrate that
tests based on the new measures can be more powerful than tests based on other
copula dependence measures.
"
"stat.TH","  We consider a sparse multi-task regression framework for fitting a collection
of related sparse models. Representing models as nodes in a graph with edges
between related models, a framework that fuses lasso regressions with the total
variation penalty is investigated. Under a form of restricted eigenvalue
assumption, bounds on prediction and squared error are given that depend upon
the sparsity of each model and the differences between related models. This
assumption relates to the smallest eigenvalue restricted to the intersection of
two cone sets of the covariance matrix constructed from each of the agents'
covariances. We show that this assumption can be satisfied if the constructed
covariance matrix satisfies a restricted isometry property. In the case of a
grid topology high-probability bounds are given that match, up to log factors,
the no-communication setting of fitting a lasso on each model, divided by the
number of agents. A decentralised dual method that exploits a convex-concave
formulation of the penalised problem is proposed to fit the models and its
effectiveness demonstrated on simulations against the group lasso and variants.
"
"stat.TH","  This paper deals with the problem of inference associated with linear
fractional diffusion process with random effects in the drift. In particular we
are concerned with the maximum likelihood estimators (MLE) of the random effect
parameters. First of all, we estimate the Hurst parameter H from one single
subject. Second, assuming the Hurst index H is known, we derive the MLE and
examine their asymptotic behavior as the number of subjects under study becomes
large, with random effects normally distributed.
"
"stat.TH","  In this paper, we obtain fundamental $\mathcal{L}_{p}$ bounds in sequential
prediction and recursive algorithms via an entropic analysis. Both classes of
problems are examined by investigating the underlying entropic relationships of
the data and/or noises involved, and the derived lower bounds may all be
quantified in a conditional entropy characterization. We also study the
conditions to achieve the generic bounds from an innovations' viewpoint.
"
"stat.TH","  We consider the teacher-student setting of learning shallow neural networks
with quadratic activations and planted weight matrix $W^*\in\mathbb{R}^{m\times
d}$, where $m$ is the width of the hidden layer and $d\le m$ is the data
dimension. We study the optimization landscape associated with the empirical
and the population squared risk of the problem. Under the assumption the
planted weights are full-rank we obtain the following results. First, we
establish that the landscape of the empirical risk admits an ""energy barrier""
separating rank-deficient $W$ from $W^*$: if $W$ is rank deficient, then its
risk is bounded away from zero by an amount we quantify. We then couple this
result by showing that, assuming number $N$ of samples grows at least like a
polynomial function of $d$, all full-rank approximate stationary points of the
empirical risk are nearly global optimum. These two results allow us to prove
that gradient descent, when initialized below the energy barrier, approximately
minimizes the empirical risk and recovers the planted weights in
polynomial-time. Next, we show that initializing below this barrier is in fact
easily achieved when the weights are randomly generated under relatively weak
assumptions. We show that provided the network is sufficiently
overparametrized, initializing with an appropriate multiple of the identity
suffices to obtain a risk below the energy barrier. At a technical level, the
last result is a consequence of the semicircle law for the Wishart ensemble and
could be of independent interest. Finally, we study the minimizers of the
empirical risk and identify a simple necessary and sufficient geometric
condition on the training data under which any minimizer has necessarily zero
generalization error. We show that as soon as $N\ge N^*=d(d+1)/2$, randomly
generated data enjoys this geometric condition almost surely, while that ceases
to be true if $N<N^*$.
"
"stat.TH","  Recent years have seen the rise of convolutional neural network techniques in
exemplar-based image synthesis. These methods often rely on the minimization of
some variational formulation on the image space for which the minimizers are
assumed to be the solutions of the synthesis problem. In this paper we
investigate, both theoretically and experimentally, another framework to deal
with this problem using an alternate sampling/minimization scheme. First, we
use results from information geometry to assess that our method yields a
probability measure which has maximum entropy under some constraints in
expectation. Then, we turn to the analysis of our method and we show, using
recent results from the Markov chain literature, that its error can be
explicitly bounded with constants which depend polynomially in the dimension
even in the non-convex setting. This includes the case where the constraints
are defined via a differentiable neural network. Finally, we present an
extensive experimental study of the model, including a comparison with
state-of-the-art methods and an extension to style transfer.
"
"stat.TH","  Saddle-point optimization problems are an important class of optimization
problems with applications to game theory, multi-agent reinforcement learning
and machine learning. A majority of the rich literature available for
saddle-point optimization has focused on the offline setting. In this paper, we
study nonstationary versions of stochastic, smooth, strongly-convex and
strongly-concave saddle-point optimization problem, in both online (or
first-order) and multi-point bandit (or zeroth-order) settings. We first
propose natural notions of regret for such nonstationary saddle-point
optimization problems. We then analyze extragradient and Frank-Wolfe
algorithms, for the unconstrained and constrained settings respectively, for
the above class of nonstationary saddle-point optimization problems. We
establish sub-linear regret bounds on the proposed notions of regret in both
the online and bandit setting.
"
"stat.TH","  We consider the problem of compressed sensing and of (real-valued) phase
retrieval with random measurement matrix. We derive sharp asymptotics for the
information-theoretically optimal performance and for the best known polynomial
algorithm for an ensemble of generative priors consisting of fully connected
deep neural networks with random weight matrices and arbitrary activations. We
compare the performance to sparse separable priors and conclude that generative
priors might be advantageous in terms of algorithmic performance. In
particular, while sparsity does not allow to perform compressive phase
retrieval efficiently close to its information-theoretic limit, it is found
that under the random generative prior compressed phase retrieval becomes
tractable.
"
"stat.TH","  In this paper, we consider Strassen's version of optimal transport (OT)
problem. That is, we minimize the excess-cost probability (i.e., the
probability that the cost is larger than a given value) over all couplings of
two given distributions. We derive large deviation, moderate deviation, and
central limit theorems for this problem. Our proof is based on Strassen's dual
formulation of the OT problem, Sanov's theorem on the large deviation principle
(LDP) of empirical measures, as well as the moderate deviation principle (MDP)
and central limit theorems (CLT) of empirical measures. In order to apply the
LDP, MDP, and CLT to Strassen's OT problem, two nested optimal transport
formulas for Strassen's OT problem are derived. Based on these nested formulas
and using a splitting technique, we carefully design asymptotically optimal
solutions to Strassen's OT problem and its dual formulation.
"
"stat.TH","  In this note we introduce the notion of a $C^k$-diffeological statistical
model, which allows us to apply the theory of diffeological spaces to (possibly
singular) statistical models. In particular, we introduce a class of almost
2-integrable $C^k$-diffeological statistical models that encompasses all known
statistical models for which the Fisher metric is defined. This class contains
a statistical model which does not appear in the Ay-Jost-L\^e-Schwachh\""ofer
theory of parametrized measure models. Then we show that for any positive
integer $k$ the class of almost 2-integrable $C^k$-diffeological statistical
models is preserved under probabilistic mappings. Furthermore, the monotonicity
theorem for the Fisher metric also holds for this class. As a consequence, the
Fisher metric on an almost 2-integrable $C^k$-diffeological statistical model
$P \subset {\cal P}({\cal X})$ is preserved under any probabilistic mapping $T:
{\cal X}\leadsto {\cal Y}$ that is sufficient w.r.t. $P$. Finally we extend the
Cram\'er-Rao inequality to the class of 2-integrable $C^k$-diffeological
statistical models.
"
"stat.TH","  We propose a generalization of the linear panel quantile regression model to
accommodate both \textit{sparse} and \textit{dense} parts: sparse means while
the number of covariates available is large, potentially only a much smaller
number of them have a nonzero impact on each conditional quantile of the
response variable; while the dense part is represent by a low-rank matrix that
can be approximated by latent factors and their loadings. Such a structure
poses problems for traditional sparse estimators, such as the
$\ell_1$-penalised Quantile Regression, and for traditional latent factor
estimator, such as PCA. We propose a new estimation procedure, based on the
ADMM algorithm, consists of combining the quantile loss function with $\ell_1$
\textit{and} nuclear norm regularization. We show, under general conditions,
that our estimator can consistently estimate both the nonzero coefficients of
the covariates and the latent low-rank matrix.
  Our proposed model has a ""Characteristics + Latent Factors"" Asset Pricing
Model interpretation: we apply our model and estimator with a large-dimensional
panel of financial data and find that (i) characteristics have sparser
predictive power once latent factors were controlled (ii) the factors and
coefficients at upper and lower quantiles are different from the median.
"
"stat.TH","  Gaussian graphical models are semi-algebraic subsets of the cone of positive
definite covariance matrices. They are widely used throughout natural sciences,
computational biology and many other fields. Computing the vanishing ideal of
the model gives us an implicit description of the model.
  In this paper, we resolve two conjectures of Sturmfels and Uhler from
\cite{BS n CU}. In particular, we characterize those graphs for which the
vanishing ideal of the Gaussian graphical model is generated in degree $1$ and
$2$. These turn out to be the Gaussian graphical models whose ideals are toric
ideals, and the resulting graphs are the $1$-clique sums of complete graphs.
"
"stat.TH","  We consider the problem of matrix approximation and denoising induced by the
Kronecker product decomposition. Specifically, we propose to approximate a
given matrix by the sum of a few Kronecker products of matrices, which we refer
to as the Kronecker product approximation (KoPA). Because the Kronecker product
is an extension of the outer product from vectors to matrices, KoPA extends the
low rank matrix approximation, and includes it as a special case. Comparing
with the latter, KoPA also offers a greater flexibility, since it allows the
user to choose the configuration, which are the dimensions of the two smaller
matrices forming the Kronecker product. On the other hand, the configuration to
be used is usually unknown, and needs to be determined from the data in order
to achieve the optimal balance between accuracy and parsimony. We propose to
use extended information criteria to select the configuration. Under the
paradigm of high dimensional analysis, we show that the proposed procedure is
able to select the true configuration with probability tending to one, under
suitable conditions on the signal-to-noise ratio. We demonstrate the
superiority of KoPA over the low rank approximations through numerical studies,
and several benchmark image examples.
"
"stat.TH","  Lomax distribution has been widely used in economics, business and actuarial
sciences. Due to its importance, we consider the statistical inference of this
model under joint type-II censoring scenario. In order to estimate the
parameters, we derive the Newton-Raphson(NR) procedure and we observe that most
of the times in the simulation NR algorithm does not converge. Consequently, we
make use of the expectation-maximization (EM) algorithm. Moreover, Bayesian
estimations are also provided based on squared error, linear-exponential and
generalized entropy loss functions together with the importance sampling method
due to the structure of posterior density function. In the sequel, we perform a
Monte Carlo simulation experiment to compare the performances of the listed
methods. Mean squared error values, averages of estimated values as well as
coverage probabilities and average interval lengths are considered to compare
the performances of different methods. The approximate confidence intervals,
bootstrap-p and bootstrap-t confidence intervals are computed for EM
estimations. Also, Bayesian coverage probabilities and credible intervals are
obtained. Finally, we consider the Bladder Cancer data to illustrate the
applicability of the methods covered in the paper.
"
"stat.TH","  Whether an extreme observation is an outlier or not, depends strongly on the
corresponding tail behaviour of the underlying distribution. We develop an
automatic, data-driven method to identify extreme tail behaviour that deviates
from the intermediate and central characteristics. This allows for detecting
extreme outliers or sets of extreme data that show less spread than the bulk of
the data. To this end we extend a testing method proposed in Bhattacharya et al
2019 for the specific case of heavy tailed models, to all max-domains of
attraction. Consequently we propose a tail-adjusted boxplot which yields a more
accurate representation of possible outliers. Several examples and simulation
results illustrate the finite sample behaviour of this approach.
"
"stat.TH","  We describe a formal approach to identify 'root causes' of outliers observed
in $n$ variables $X_1,\dots,X_n$ in a scenario where the causal relation
between the variables is a known directed acyclic graph (DAG). To this end, we
first introduce a systematic way to define outlier scores. Further, we
introduce the concept of 'conditional outlier score' which measures whether a
value of some variable is unexpected *given the value of its parents* in the
DAG, if one were to assume that the causal structure and the corresponding
conditional distributions are also valid for the anomaly. Finally, we quantify
to what extent the high outlier score of some target variable can be attributed
to outliers of its ancestors. This quantification is defined via Shapley values
from cooperative game theory.
"
"stat.TH","  Sum-Product Networks (SPNs) can be regarded as a form of deep graphical
models that compactly represent deeply factored and mixed distributions. An SPN
is a rooted directed acyclic graph (DAG) consisting of a set of leaves
(corresponding to base distributions), a set of sum nodes (which represent
mixtures of their children distributions) and a set of product nodes
(representing the products of its children distributions).
  In this work, we initiate the study of the sample complexity of PAC-learning
the set of distributions that correspond to SPNs. We show that the sample
complexity of learning tree structured SPNs with the usual type of leaves
(i.e., Gaussian or discrete) grows at most linearly (up to logarithmic factors)
with the number of parameters of the SPN. More specifically, we show that the
class of distributions that corresponds to tree structured Gaussian SPNs with
$k$ mixing weights and $e$ ($d$-dimensional Gaussian) leaves can be learned
within Total Variation error $\epsilon$ using at most
$\widetilde{O}(\frac{ed^2+k}{\epsilon^2})$ samples. A similar result holds for
tree structured SPNs with discrete leaves.
  We obtain the upper bounds based on the recently proposed notion of
distribution compression schemes. More specifically, we show that if a (base)
class of distributions $\mathcal{F}$ admits an ""efficient"" compression, then
the class of tree structured SPNs with leaves from $\mathcal{F}$ also admits an
efficient compression.
"
"stat.TH","  Variable selection in sparse regression models is an important task as
applications ranging from biomedical research to econometrics have shown.
Especially for higher dimensional regression problems, for which the link
function between response and covariates cannot be directly detected, the
selection of informative variables is challenging. Under these circumstances,
the Random Forest method is a helpful tool to predict new outcomes while
delivering measures for variable selection. One common approach is the usage of
the permutation importance. Due to its intuitive idea and flexible usage, it is
important to explore circumstances, for which the permutation importance based
on Random Forest correctly indicates informative covariates. Regarding the
latter, we deliver theoretical guarantees for the validity of the permutation
importance measure under specific assumptions and prove its (asymptotic)
unbiasedness. An extensive simulation study verifies our findings.
"
"stat.TH","  Recently, Chen, Li and Zhang have established simple conditions
characterizing when latent factors are asymptotically identifiable within a
certain model of confirmatory factor analysis. In this note, we prove a related
characterization of when factor loadings are identifiable, under slightly
weaker modeling assumptions and a slightly stronger definition of
identifiability. We also check that the previously-established characterization
for latent factors still holds in our modified model.
"
"stat.TH","  This paper focuses on hypothesis testing for the input of a L\'evy-driven
storage system by sampling of the storage level. As the likelihood is not
explicit we propose two tests that rely on transformation of the data. The
first approach uses i.i.d. `quasi-busy-periods' between observations of zero
workload. The distribution of the duration of quasi-busy-periods is determined.
The second method is a conditional likelihood ratio test based on the Bernoulli
events of observing a zero or positive workload, conditional on the previous
workload. Performance analysis is presented for both tests along with
speed-of-convergence results, that are of independent interest.
"
"stat.TH","  A generalized spiked Fisher matrix is considered in this paper. We establish
a criterion for the description of the support of the limiting spectral
distribution of high-dimensional generalized Fisher matrix and study the almost
sure limits of the sample spiked eigenvalues where the population covariance
matrices are arbitrary which successively removed an unrealistic condition
posed in the previous works, that is, the covariance matrices are assumed to be
diagonal or diagonal block-wise structure. In addition, we also give a
consistent estimator of the population spiked eigenvalues. A series of
simulations are conducted that support the theoretical results and illustrate
the accuracy of our estimators.
"
"stat.TH","  In this paper physical multi-scale processes governed by their own principles
for evolution or equilibrium on each scale are coupled by matching the stored
and dissipated energy, in line with the Hill-Mandel principle. In our view the
correct representations of stored and dissipated energy is essential to the
representation irreversible material behaviour, and this matching is also used
for upscaling. The small scales, here the meso-scale, is assumed to be
described probabilistically, and so on the macroscale also a probabilistic
model is identified in a Bayesian setting, reflecting the randomness of the
meso-scale, the loss of resolution due to upscaling, and the uncertainty
involved in the Bayesian process. In this way multi-scale processes become
hierarchical systems in which the information is transferred across the scales
by Bayesian identification on coarser levels. The quantities to be matched on
the coarse-scale model are the stored and dissipated energies. In this way
probability distributions of macro-scale material parameters are determined,
and not only in the elastic region, but also for the irreversible and highly
nonlinear elasto-damage regimes, refelcting the aleatory uncetainty at the
meso-scale level. For this purpose high dimensional meso-scale stochastic
simulations in a non-intrusive functional approximation forms are mapped to the
macro-scale models in an approximative manner by employing a generalised
version of the Kalman filter. To reduce the overall computational cost, a model
reduction of the meso-scale simulation is achieved by combining the
unsupervised learning techniques based on the Bayesian copula variartional
inference with the classical functional approximation forms from the field of
uncertainty quantification.
"
"stat.TH","  When performing multiple testing, adjusting the distribution of the null
hypotheses is ubiquitous in applications. However, the effect of such an
operation remains largely unknown, especially in terms of false discovery
proportion (FDP) and true discovery proportion (TDP) of the resulting
procedure. We explore this issue in the most classical case where the null
distributions are Gaussian with an unknown rescaling parameters (mean and
variance) and where the Benjamini-Hochberg (BH) procedure is applied after a
data-rescaling step. Our main result shows the following sparsity boundary: an
asymptotically optimal rescaling (in some specific sense) exists if and only if
the sparsity parameter $k$ (number of false nulls) is of order less than
$n/\log(n)$, where $n$ is the total number of tests. Our proof relies on new
non-asymptotic lower bounds on FDP/TDP, which are of independent interest and
share similarities with those developed in the minimax robust statistical
theory. Further sparsity boundaries are derived for general location models
where the shape of the null distribution is not necessarily Gaussian.
"
"stat.TH","  A coupling method is developed for univariate extreme value theory ,
providing an alternative to the use of the tail empirical/quantile processes.
Emphasizing the Peak-over-Threshold approach that approximates the distribution
above high threshold by the Generalized Pareto distribution, we compare the
empirical distribution of exceedances and the empirical distribution associated
to the limit Generalized Pareto model and provide sharp bounds for their
Wasser-stein distance in the second order Wasserstein space. As an application
, we recover standard results on the asymptotic behavior of the Hill estimator,
the Weissman extreme quantile estimator or the probability weighted moment
estimators, shedding some new light on the theory.
"
"stat.TH","  Maximum-likelihood estimation (MLE) is arguably the most important tool for
statisticians, and many methods have been developed to find the MLE. We present
a new inequality involving posterior distributions of a latent variable that
holds under very general conditions. It is related to the EM algorithm and has
a clear potential for being used in a similar fashion.
"
"stat.TH","  Fields like public health, public policy, and social science often want to
quantify the degree of dependence between variables whose relationships take on
unknown functional forms. Typically, in fact, researchers in these fields are
attempting to evaluate causal theories, and so want to quantify dependence
after conditioning on other variables that might explain, mediate or confound
causal relations. One reason conditional mutual information is not more widely
used for these tasks is the lack of estimators which can handle combinations of
continuous and discrete random variables, common in applications. This paper
develops a new method for estimating mutual and conditional mutual information
for data samples containing a mix of discrete and continuous variables. We
prove that this estimator is consistent and show, via simulation, that it is
more accurate than similar estimators.
"
"stat.TH","  Rating systems are ubiquitous, with applications ranging from product
recommendation to teaching evaluations. Confidence intervals for functionals of
rating data such as empirical means or quantiles are critical to
decision-making in various applications including recommendation/ranking
algorithms. Confidence intervals derived from standard Hoeffding and Bernstein
bounds can be quite loose, especially in small sample regimes, since these
bounds do not exploit the geometric structure of the probability simplex. We
propose a new approach to deriving confidence intervals that are tailored to
the geometry associated with multi-star/value rating systems using a
combination of techniques from information theory, including Kullback-Leibler,
Sanov, and Csisz{\'a}r inequalities.
  The new confidence intervals are almost always as good or better than all
standard methods and are significantly tighter in many situations. The standard
bounds can require several times more samples than our new bounds to achieve
specified confidence interval widths.
"
"stat.TH","  The theocratical properties of the power of the conventional testing
hypotheses and the selection bias are usually unknown under covariate-adaptive
randomized clinical trials. In the literature, most studies are based on
simulations. In this article, we provide theoretical foundation of the power of
the hypothesis testing and the selection bias under covariate-adaptive
randomization based on linear models. We study the asymptotic relative loss of
power of hypothesis testing to compare the treatment effects and the asymptotic
selection bias. Under the covariate-adaptive randomization, (i) the hypothesis
testing usually losses power, the more covariates in testing model are not
incorporated in the randomization procedure, the more the power is lost; (ii)
the hypothesis testing is usually more powerful than the one under complete
randomization; and (iii) comparing to complete randomization, most of the
popular covariate-adaptive randomization procedures in the literature, for
example, Pocock and Simon's marginal procedure, stratified permuted block
design, etc, produce nontrivial selection bias. A new family of
covariate-adaptive randomization procedures are proposed for considering the
power and selection bias simultaneously, under which, the covariate imbalances
are small enough so that the power of testing the treatment effects would be
asymptotically the largest and at the same time, the selection bias is
asymptotically the optimal. The theocratical properties give a full picture how
the power of the hypothesis testing, the selection bias of the randomization
procedure, and the randomization method affect each other.
"
"stat.TH","  Recently, the binary expansion testing framework was introduced to test the
independence of two continuous random variables by utilizing symmetry
statistics that are complete sufficient statistics for dependence. We develop a
new test by an ensemble method that uses the sum of squared symmetry statistics
and distance correlation. Simulation studies suggest that this method improves
the power while preserving the clear interpretation of the binary expansion
testing. We extend this method to tests of independence of random vectors in
arbitrary dimension. By random projections, the proposed binary expansion
randomized ensemble test transforms the multivariate independence testing
problem into a univariate problem. Simulation studies and data example analyses
show that the proposed method provides relatively robust performance compared
with existing methods.
"
"stat.TH","  We propose a novel algorithm for large-scale regression problems named
histogram transform ensembles (HTE), composed of random rotations, stretchings,
and translations. First of all, we investigate the theoretical properties of
HTE when the regression function lies in the H\""{o}lder space $C^{k,\alpha}$,
$k \in \mathbb{N}_0$, $\alpha \in (0,1]$. In the case that $k=0, 1$, we adopt
the constant regressors and develop the na\""{i}ve histogram transforms (NHT).
Within the space $C^{0,\alpha}$, although almost optimal convergence rates can
be derived for both single and ensemble NHT, we fail to show the benefits of
ensembles over single estimators theoretically. In contrast, in the subspace
$C^{1,\alpha}$, we prove that if $d \geq 2(1+\alpha)/\alpha$, the lower bound
of the convergence rates for single NHT turns out to be worse than the upper
bound of the convergence rates for ensemble NHT. In the other case when $k \geq
2$, the NHT may no longer be appropriate in predicting smoother regression
functions. Instead, we apply kernel histogram transforms (KHT) equipped with
smoother regressors such as support vector machines (SVMs), and it turns out
that both single and ensemble KHT enjoy almost optimal convergence rates. Then
we validate the above theoretical results by numerical experiments. On the one
hand, simulations are conducted to elucidate that ensemble NHT outperform
single NHT. On the other hand, the effects of bin sizes on accuracy of both NHT
and KHT also accord with theoretical analysis. Last but not least, in the
real-data experiments, comparisons between the ensemble KHT, equipped with
adaptive histogram transforms, and other state-of-the-art large-scale
regression estimators verify the effectiveness and accuracy of our algorithm.
"
"stat.TH","  We consider the problem of locating the source of a network cascade, given a
noisy time-series of network data. Initially, the cascade starts with one
unknown, affected vertex and spreads deterministically at each time step. The
goal is to find an adaptive procedure that outputs an estimate for the source
as fast as possible, subject to a bound on the estimation error. For a general
class of graphs, we describe a family of matrix sequential probability ratio
tests (MSPRTs) that are first-order asymptotically optimal up to a constant
factor as the estimation error tends to zero. We apply our results to lattices
and regular trees, and show that MSPRTs are asymptotically optimal for regular
trees. We support our theoretical results with simulations.
"
"stat.TH","  In Gaussian graphical models, the zero entries in the precision matrix
determine the dependence structure, so estimating that sparse precision matrix
and, thereby, learning this underlying structure, is an important and
challenging problem. We propose an empirical version of the $G$-Wishart prior
for sparse precision matrices, where the prior mode is informed by the data in
a suitable way. Paired with a prior on the graph structure, a marginal
posterior distribution for the same is obtained that takes the form of a ratio
of two $G$-Wishart normalizing constants. We show that this ratio can be easily
and accurately computed using a Laplace approximation, which leads to fast and
efficient posterior sampling even in high-dimensions. Numerical results
demonstrate the proposed method's superior performance, in terms of speed and
accuracy, across a variety of settings, and theoretical support is provided in
the form of a posterior concentration rate theorem.
"
"stat.TH","  This paper discusses certain properties of heterogeneous hypergeometric
functions with two matrix arguments. These functions are newly defined but have
already appeared in statistical literature and are useful when dealing with the
deviation of certain distributions for the eigenvalues of singular beta-Wishart
matrices. The joint density function of the eigenvalues and the distribution of
the largest eigenvalue can be expressed in terms of certain heterogeneous
hypergeometric functions. Exact computation of the distribution of the largest
eigenvalue is conducted here for a real case.
"
"stat.TH","  Nonparametric regression with random design is considered. Estimates are
defined by minimzing a penalized empirical $L_2$ risk over a suitably chosen
class of neural networks with one hidden layer via gradient descent. Here, the
gradient descent procedure is repeated several times with randomly chosen
starting values for the weights, and from the list of constructed estimates the
one with the minimal empirical $L_2$ risk is chosen. Under the assumption that
the number of randomly chosen starting values and the number of steps for
gradient descent are sufficiently large it is shown that the resulting estimate
achieves (up to a logarithmic factor) the optimal rate of convergence in a
projection pursuit model. The final sample size performance of the estimates is
illustrated by using simulated data.
"
"stat.TH","  Recent results in nonparametric regression show that for deep learning, i.e.,
for neural network estimates with many hidden layers, we are able to achieve
good rates of convergence even in case of high-dimensional predictor variables,
provided suitable assumptions on the structure of the regression function are
imposed. The estimates are defined by minimizing the empirical $L_2$ risk over
a class of neural networks. In practice it is not clear how this can be done
exactly. In this article we introduce a new neural network regression estimate
where most of the weights are chosen regardless of the data motivated by some
recent approximation results for neural networks, and which is therefore easy
to implement. We show that for this estimate we can derive rates of convergence
results in case the regression function is smooth. We combine this estimate
with the projection pursuit, where we choose the directions randomly, and we
show that for sufficiently many repititions we get a neural network regression
estimate which is easy to implement and which achieves the one-dimensional rate
of convergence (up to some logarithmic factor) in case that the regression
function satisfies the assumptions of projection pursuit.
"
"stat.TH","  Recently it was shown in several papers that backpropagation is able to find
the global minimum of the empirical risk on the training data using
over-parametrized deep neural networks. In this paper a similar result is shown
for deep neural networks with the sigmoidal squasher activation function in a
regression setting, and a lower bound is presented which proves that these
networks do not generalize well on a new data in the sense that they do not
achieve the optimal minimax rate of convergence for estimation of smooth
regression functions.
"
"stat.TH","  Linear mixed effects models (LMMs) are a popular and powerful tool for
analyzing clustered or repeated observations for numeric outcomes. LMMs consist
of a fixed and a random component, specified in the model through their
respective design matrices. Checking if the two design matrices are correctly
specified is crucial since mis-specifying them can affect the validity and
efficiency of the analysis. We show how to use random processes defined as
cumulative sums of appropriately ordered model's residuals to test if the
functional form of the fitted LMM is correctly specified. We show how these
processes can be used to test goodness-of-fit of the functional form of the
entire model, or only its fixed and/or random component. Inspecting plots of
the proposed processes is shown to be highly informative about the potential
mis-specification of the functional form of the model, providing clues for
potential improvement of the model's fit. We show how the visual inspection can
be objectified by using a novel procedure for estimating $p$-values which can
be based on sign-flipping/bootstrap or simulations and show its validity by
using theoretical results and a large Monte Carlo simulation study. The
proposed methodology can be used with LMMs with multi-level or crossed random
effects.% and could also be extended to generalized LMMs.
"
"stat.TH","  Defining and identifying causal intervention effects for transmissible
infectious disease outcomes is challenging because a treatment -- such as a
vaccine -- given to one individual may affect the infection outcomes of others.
Epidemiologists have proposed causal estimands to quantify effects of
interventions under contagion using a two-person partnership model. These
simple conceptual models have helped researchers develop causal estimands
relevant to clinical evaluation of vaccine effects. However, many of these
partnership models are formulated under structural assumptions that preclude
realistic infectious disease transmission dynamics, limiting their conceptual
usefulness in defining and identifying causal treatment effects in empirical
intervention trials. In this paper, we propose causal intervention effects in
two-person partnerships under arbitrary infectious disease transmission
dynamics, and give nonparametric identification results showing how effects can
be estimated in empirical trials using time-to-infection or binary outcome
data. The key insight is that contagion is a causal phenomenon that induces
conditional independencies on infection outcomes that can be exploited for the
identification of clinically meaningful causal estimands. These new estimands
are compared to existing quantities, and results are illustrated using a
realistic simulation of an HIV vaccine trial.
"
"stat.TH","  Double descent refers to the phase transition that is exhibited by the
generalization error of unregularized learning models when varying the ratio
between the number of parameters and the number of training samples. The recent
success of highly over-parameterized machine learning models such as deep
neural networks has motivated a theoretical analysis of the double descent
phenomenon in classical models such as linear regression which can also
generalize well in the over-parameterized regime. We provide the first exact
non-asymptotic expressions for double descent of the minimum norm linear
estimator. Our approach involves constructing a special determinantal point
process which we call surrogate random design, to replace the standard i.i.d.
design of the training sample. This surrogate design admits exact expressions
for the mean squared error of the estimator while preserving the key properties
of the standard design. We also establish an exact implicit regularization
result for over-parameterized training samples. In particular, we show that,
for the surrogate design, the implicit bias of the unregularized minimum norm
estimator precisely corresponds to solving a ridge-regularized least squares
problem on the population distribution. In our analysis we introduce a new
mathematical tool of independent interest: the class of random matrices for
which determinant commutes with expectation.
"
"stat.TH","  We consider the binary classification problem in a setup that preserves the
privacy of the original sample. We provide a privacy mechanism that is locally
differentially private and then construct a classifier based on the private
sample that is universally consistent in Euclidean spaces. Under stronger
assumptions, we establish the minimax rates of convergence of the excess risk
and see that they are slower than in the case when the original sample is
available.
"
"stat.TH","  This paper studies methods for testing and estimating change-points in the
covariance structure of a high-dimensional linear time series. The assumed
framework allows for a large class of multivariate linear processes (including
vector autoregressive moving average (VARMA) models) of growing dimension and
spiked covariance models. The approach uses bilinear forms of the centered or
non-centered sample variance-covariance matrix. Change-point testing and
estimation are based on maximally selected weighted cumulated sum (CUSUM)
statistics. Large sample approximations under a change-point regime are
provided including a multivariate CUSUM transform of increasing dimension. For
the unknown asymptotic variance and covariance parameters associated to (pairs
of) CUSUM statistics we propose consistent estimators. Based on weak laws of
large numbers for their sequential versions, we also consider stopped sample
estimation where observations until the estimated change-point are used. Finite
sample properties of the procedures are investigated by simulations and their
application is illustrated by analyzing a real data set from environmetrics.
"
"stat.TH","  In this paper a class of statistics based on high frequency observations of
oscillating Brownian motions and skew Brownian motions is considered. Their
convergence rate towards the local time of the underling process is obtained in
form of a Central Limit Theorem.
"
"stat.TH","  We extend the theoretical study of a recently proposed nonparametric
clustering algorithm called Adaptive Weights Clustering (AWC). In particular,
we are interested in the case of high-dimensional data lying in the vicinity of
a lower-dimensional non-linear submanifold with positive reach. After a slight
adjustment and under rather general assumptions for the cluster structure, the
algorithm turns out to be nearly optimal in detecting local inhomogeneities,
while aggregating homogeneous data with a high probability. We also adress the
problem of parameter tuning.
"
"stat.TH","  This paper investigates Frequentist consistency properties of the posterior
distributions constructed via Generalized Variational Inference (GVI). A number
of generic and novel strategies are given for proving consistency, relying on
the theory of $\Gamma$-convergence. Specifically, this paper shows that under
minimal regularity conditions, the sequence of GVI posteriors is consistent and
collapses to a point mass at the population-optimal parameter value as the
number of observations goes to infinity. The results extend to the latent
variable case without additional assumptions and hold under misspecification.
Lastly, the paper explains how to apply the results to a selection of GVI
posteriors with especially popular variational families. For example,
consistency is established for GVI methods using the mean field normal
variational family, normal mixtures, Gaussian process variational families as
well as neural networks indexing a normal (mixture) distribution.
"
"stat.TH","  Two closely related discrete probability distributions are introduced. In
each case the support is a set of vectors in $\mathbb{R}^n$ obtained from the
partitions of the fixed positive integer $n$. These distributions arise
naturally when considering equally-likely random permutations on the set of $n$
letters. For one of the distributions, the expectation vector and covariance
matrix is derived. For the other distribution, conjectures for several elements
of the expectation vector are provided.
"
"stat.TH","  We prove the first explicit rate of convergence to the Tracy-Widom
distribution for the fluctuation of the largest eigenvalue of sample covariance
matrices that are not integrable. Our primary focus is matrices of type $ X^*X
$ and the proof follows the Erd\""{o}s-Schlein-Yau dynamical method. We use a
recent approach to the analysis of the Dyson Brownian motion from [5] to obtain
a quantitative error estimate for the local relaxation flow at the edge.
Together with a quantitative version of the Green function comparison theorem,
this gives the rate of convergence.
  Combined with a result of Lee-Schnelli [26], some quantitative estimates also
hold for more general separable sample covariance matrices $ X^* \Sigma X $
with general diagonal population $ \Sigma $.
"
"stat.TH","  We investigate the problem of conditional dependence graph estimation when
several pairs of nodes have no joint observation. For these pairs even the
simplest metric of covariability, the sample covariance, is unavailable. This
problem arises, for instance, in calcium imaging recordings where the
activities of a large population of neurons are typically observed by recording
from smaller subsets of cells at once, and several pairs of cells are never
recorded simultaneously. With no additional assumption, the unavailability of
parts of the covariance matrix translates into the unidentifiability of the
precision matrix that, in the Gaussian graphical model setting, specifies the
graph. Recovering a conditional dependence graph in such settings is
fundamentally an extremely hard challenge, because it requires to infer
conditional dependences between network nodes with no empirical evidence of
their covariability. We call this challenge the ""graph quilting problem"". We
demonstrate that, under mild conditions, it is possible to correctly identify
not only the edges connecting the observed pairs of nodes, but also a superset
of those connecting the variables that are never observed jointly. We propose
an $\ell_1$ regularized graph estimator based on a partially observed sample
covariance matrix and establish its rates of convergence in high-dimensions. We
finally present a simulation study and the analysis of calcium imaging data of
ten thousand neurons in mouse visual cortex.
"
"stat.TH","  Averages of proper scoring rules are often used to rank probabilistic
forecasts. In many cases, the individual observations and their predictive
distributions in these averages have variable scale (variance). We show that
some of the most popular proper scoring rules, such as the continuous ranked
probability score (CRPS), up-weight observations with large uncertainty which
can lead to unintuitive rankings. If a scoring rule has this property we say
that it is not locally scale invariant. To solve this problem, a scaled CRPS
(SCRPS) is proposed. This new proper scoring rule is locally scale invariant
and therefore works in the case of varying uncertainty, and it shares many of
the appealing properties with the CRPS.
  We also define robustness of scoring rules and introduce a new class of
scoring rules which, besides the CRPS and the SCRPS, contains scoring rules
that are robust against outliers as special cases. In three different
applications from spatial statistics, stochastic volatility models, and
regression for count data, we illustrate why scale invariance and robustness
are important properties, and show why the SCRPS should be used instead of the
CRPS.
"
"stat.TH","  In the manifold setting, we provide a series of spectral convergence results
quantifying how the eigenvectors and eigenvalues of the graph Laplacian
converge to the eigenfunctions and eigenvalues of the Laplace-Beltrami operator
in the $L^\infty$ sense. %The convergence rate is also provided. Based on these
results, convergence of the proposed heat kernel approximation algorithm, as
well as the convergence rate, to the exact heat kernel is guaranteed.To our
knowledge, this is the first work exploring the spectral convergence in the
$L^\infty$ sense and providing a numerical heat kernel reconstruction from the
point cloud with theoretical guarantees.
"
"stat.TH","  Many works in statistics aim at designing a universal estimation procedure,
that is, an estimator that would converge to the best approximation of the
(unknown) data generating distribution in a model, without any assumption on
this distribution. This question is of major interest, in particular because
the universality property leads to the robustness of the estimator. In this
paper, we tackle the problem of universal estimation using a minimum distance
estimator presented in Briol et al. (2019) based on the Maximum Mean
Discrepancy. We show that the estimator is robust to both dependence and to the
presence of outliers in the dataset. Finally, we provide a theoretical study of
the stochastic gradient descent algorithm used to compute the estimator, and we
support our findings with numerical simulations.
"
"stat.TH","  Bayesian nonparametric regression under a rescaled Gaussian process prior
offers smoothness-adaptive function estimation with near minimax-optimal error
rates. Hierarchical extensions of this approach, equipped with stochastic
variable selection, are known to also adapt to the unknown intrinsic dimension
of a sparse true regression function. But it remains unclear if such extensions
offer variable selection consistency, i.e., if the true subset of important
variables could be consistently learned from the data. It is shown here that
variable consistency may indeed be achieved with such models at least when the
true regression function has finite smoothness to induce a polynomially larger
penalty on inclusion of false positive predictors. Our result covers the high
dimensional asymptotic setting where the predictor dimension is allowed to grow
with the sample size. The proof utilizes Schwartz theory to establish that the
posterior probability of wrong selection vanishes asymptotically. A necessary
and challenging technical development involves providing sharp upper and lower
bounds to small ball probabilities at all rescaling levels of the Gaussian
process prior, a result that could be of independent interest.
"
"stat.TH","  Model misspecification is a long-standing enigma of the Bayesian inference
framework as posteriors tend to get overly concentrated on ill-informed
parameter values towards the large sample limit. Tempering of the likelihood
has been established as a safer way to do updates from prior to posterior in
the presence of model misspecification. At one extreme tempering can ignore the
data altogether and at the other extreme it provides the standard Bayes' update
when no misspecification is assumed to be present. However, it is an open issue
how to best recognize misspecification and choose a suitable level of tempering
without access to the true generating model. Here we show how probabilistic
classifiers can be employed to resolve this issue. By training a probabilistic
classifier to discriminate between simulated and observed data provides an
estimate of the ratio between the model likelihood and the likelihood of the
data under the unobserved true generative process, within the discriminatory
abilities of the classifier. The expectation of the logarithm of a ratio with
respect to the data generating process gives an estimation of the negative
Kullback-Leibler divergence between the statistical generative model and the
true generative distribution. Using a set of canonical examples we show that
this divergence provides a useful misspecification diagnostic, a model
comparison tool, and a method to inform a generalised Bayesian update in the
presence of misspecification for likelihood-based models.
"
"stat.TH","  Multiple testing of a single hypothesis and testing multiple hypotheses are
usually done in terms of p-values. In this paper we replace p-values with their
natural competitor, e-values, which are closely related to betting, Bayes
factors, and likelihood ratios. We demonstrate that e-values are often
mathematically more tractable; in particular, in multiple testing of a single
hypothesis, e-values can be merged simply by averaging them. This allows us to
develop efficient procedures using e-values for testing multiple hypotheses.
"
"stat.TH","  Time series regression analysis relies on the heteroskedasticity- and
autocorrelation-consistent (HAC) estimation of the asymptotic variance to
conduct proper inference. This paper develops such inferential methods for
high-dimensional time series regressions. To recognize the time series data
structures we focus on the sparse-group LASSO estimator. We establish the
debiased central limit theorem for low dimensional groups of regression
coefficients and study the HAC estimator of the long-run variance based on the
sparse-group LASSO residuals. The treatment relies on a new Fuk-Nagaev
inequality for a class of $\tau$-dependent processes with heavier than Gaussian
tails, which is of independent interest.
"
"stat.TH","  This paper is concerned with the limiting spectral behaviors of large
dimensional Kendall's rank correlation matrices generated by samples with
independent and continuous components. We do not require the components to be
identically distributed, and do not need any moment conditions, which is very
different from the assumptions imposed in the literature of random matrix
theory. The statistical setting in this paper covers a wide range of highly
skewed and heavy-tailed distributions. We establish the central limit theorem
(CLT) for the linear spectral statistics of the Kendall's rank correlation
matrices under the Marchenko-Pastur asymptotic regime, in which the dimension
diverges to infinity proportionally with the sample size. We further propose
three nonparametric procedures for high dimensional independent test and their
limiting null distributions are derived by implementing this CLT. Our numerical
comparisons demonstrate the robustness and superiority of our proposed test
statistics under various mixed and heavy-tailed cases.
"
"stat.TH","  The problem of constructing effective statistical tests for random number
generators (RNG) is considered. Currently, statistical tests for RNGs are a
mandatory part of cryptographic information protection systems, but their
effectiveness is mainly estimated based on experiments with various RNGs.
  We find an asymptotic estimate for the p-value of an optimal test in the case
where the alternative hypothesis is a known stationary ergodic source, and then
describe a family of tests each of which has the same asymptotic estimate of
the p-value for any (unknown) stationary ergodic source.
"
"stat.TH","  The quasi-maximum likelihood estimation is a commonly-used method for
estimating GARCH parameters. However, such estimators are sensitive to outliers
and their asymptotic normality is proved under the finite fourth moment
assumption on the underlying error distribution. In this paper, we propose a
novel class of estimators of the GARCH parameters based on ranks, called
R-estimators, with the property that they are asymptotic normal under the
existence of a more than second moment of the errors and are highly efficient.
We also consider the weighted bootstrap approximation of the finite sample
distributions of the R-estimators. We propose fast algorithms for computing the
R-estimators and their bootstrap replicates. Both real data analysis and
simulations show the superior performance of the proposed estimators under the
normal and heavy-tailed distributions. Our extensive simulations also reveal
excellent coverage rates of the weighted bootstrap approximations. In addition,
we discuss empirical and simulation results of the R-estimators for the higher
order GARCH models such as the GARCH~($2, 1$) and asymmetric models such as the
GJR model.
"
"stat.TH","  Gaussian Process Regression and Kernel Ridge Regression are popular
nonparametric regression approaches. Unfortunately, they suffer from high
computational complexity rendering them inapplicable to the modern massive
datasets. To that end a number of approximations have been suggested, some of
them allowing for a distributed implementation. One of them is the divide and
conquer approach, splitting the data into a number of partitions, obtaining the
local estimates and finally averaging them. In this paper we suggest a novel
computationally efficient fully data-driven algorithm, quantifying uncertainty
of this method, yielding frequentist $L_2$-confidence bands. We rigorously
demonstrate validity of the algorithm. Another contribution of the paper is a
minimax-optimal high-probability bound for the averaged estimator,
complementing and generalizing the known risk bounds.
"
"stat.TH","  Under ideal conditions, the probability density function (PDF) of a random
variable, such as a sensor measurement, would be well known and amenable to
computation and communication tasks. However, this is often not the case, so
the user looks for some other PDF that approximates the true but intractable
PDF. Conservativeness is a commonly sought property of this approximating PDF,
especially in distributed or unstructured data systems where the data being
fused may contain un-known correlations. Roughly, a conservative approximation
is one that overestimates the uncertainty of a system. While prior work has
introduced some definitions of conservativeness, these definitions either apply
only to normal distributions or violate some of the intuitive appeal of
(Gaussian) conservative definitions. This work provides a general and intuitive
definition of conservativeness that is applicable to any probability
distribution, including multi-modal and uniform distributions. Unfortunately,
we show that this \emph{strong} definition of conservative cannot be used to
evaluate data fusion techniques. Therefore, we also describe a weaker
definition of conservative and show it is preserved through common data fusion
methods such as the linear and log-linear opinion pool, and homogeneous
functionals. In addition, we show that after fusion, weak conservativeness is
preserved by Bayesian updates. These strong and weak definitions of
conservativeness can help design and evaluate potential correlation-agnostic
data fusion techniques.
"
"stat.TH","  Mobility management is a major challenge for the wide-spread deployment of
millimeter-wave (mmWave) cellular networks. In particular, directional
beamforming in mmWave devices renders high-speed mobility support very complex.
This complexity, however, is not limited to system design but also the
performance estimation and evaluation. Hence, some have turned their attention
to stochastic modeling of mmWave vehicular communication to derive closed-form
expressions characterizing the coverage and rate behavior of the network. In
this article, we model and analyze the beam management for mmWave vehicular
networks. To the best of our knowledge, this is the first work that goes beyond
coverage and rate analysis. Specifically, we focus on a multi-lane divided
highway scenario in which base stations and vehicles are present on both sides
of the highway. In addition to providing analytical expressions for the average
number of beam switching and handover events, we provide design insights for
the network operators to fine-tune their network within the flexibility
provided by the standard in the choice of system parameters, including the
number of resources dedicated to channel feedback and beam alignment
operations.
"
"stat.TH","  We study control variate methods for Markov chain Monte Carlo (MCMC) in the
setting of deterministic sweep sampling using $K\geq 2$ transition kernels. New
variance reduction results are provided for MCMC averages based on sweeps over
general transition kernels, leading to a particularly simple control variate
estimator in the setting of deterministic sweep Gibbs sampling. Theoretical
comparisons of our proposed control variate estimators with existing literature
are made, and a simulation study is performed to examine the amount of variance
reduction in some example cases. We also relate control variate approaches to
approaches based on conditioning (or Rao-Blackwellization), and show that the
latter can be viewed as an approximation of the former. Our theoretical results
hold for Markov chains under standard geometric drift assumptions.
"
"stat.TH","  The pseudo-Lindley distribution which was introduced in Zeghdoudi and Nedjar
(2016) is studied with regards to its upper tail. In that regard, and when the
underlying distribution function follows the Pseudo-Lindley law, we investigate
the behavior of its values, the asymptotic normality of the Hill estimator and
the double-indexed generalized Hill statistic process (Ngom and Lo), the
asymptotic normality of the records values and the moment problem.
"
"stat.TH","  We study the generalization properties of minimum-norm solutions for three
over-parametrized machine learning models including the random feature model,
the two-layer neural network model and the residual network model. We proved
that for all three models, the generalization error for the minimum-norm
solution is comparable to the Monte Carlo rate, up to some logarithmic terms,
as long as the models are sufficiently over-parametrized.
"
"stat.TH","  Skewness measures can be used to measure the level of asymmetry of a
distribution. Given the prevalence of statistical methods that assume
underlying symmetry, and also the desire for symmetry in order to make
meaningful judgements for common summary measures (e.g. the sample mean),
reliably quantifying asymmetry is an important problem. There are several
measures, among them generalizations of Bowley's well known skewness
coefficient, that use sample quartiles and other quantile-based measures. The
main drawbacks of many measures is that they are either limited to quartiles
and do not take into account more extreme tail behavior, or that they require
one to choose other quantiles (i.e. choose a value for $p$ different from 0.25)
in place of the quartiles. Our objective is to (i) average the skewness
measures over all $p$ and (ii) provide interval estimators for the new measure
with good coverage properties. Our simulation results show that the interval
estimators perform very well for all distributions considered.
"
"stat.TH","  Fractional Brownian motion (FBM) is the only Gaussian self-similar process
with stationary increments. Its increment process, called fractional Gaussian
noise, is ergodic and exhibits a property of power-like decaying
autocorrelation function (ACF) which leads to the notion of long memory. These
properties have made FBM important in modelling real-world data recorded in
different experiments ranging from biology to telecommunication. These
experiments are often disturbed by a noise which source can be just the
instrument error. In this paper we propose a rigorous statistical test based on
the ACF for FBM with added white Gaussian noise. To this end we derive a
distribution of the test statistic which is given explicitly by the generalized
chi-squared distribution. This allows us to find critical regions for the test
with a given significance level. We check the quality of the introduced test by
studying its power and comparing with other tests existing in the literature.
We also note that the introduced test procedure can be applied to an arbitrary
Gaussian process.
"
"stat.TH","  The test of homogeneity for normal mixtures has been conducted in diverse
research areas, but constructing a theory of the test of homogeneity is
challenging because the parameter set for the null hypothesis corresponds to
singular points in the parameter space. In this paper, we examine this problem
from a new perspective and offer a theory of hypothesis testing for homogeneity
based on a variational Bayes framework. In the conventional theory, the
constant order term of the free energy has remained unknown, however, we
clarify its asymptotic behavior because it is necessary for constructing a
hypothesis test. Numerical experiments shows the validity of our theoretical
results.
"
"stat.TH","  This paper is concerned with the approximation of a function $u$ in a given
approximation space $V_m$ of dimension $m$ from evaluations of the function at
$n$ suitably chosen points. The aim is to construct an approximation of $u$ in
$V_m$ which yields an error close to the best approximation error in $V_m$ and
using as few evaluations as possible. Classical least-squares regression, which
defines a projection in $V_m$ from $n$ random points, usually requires a large
$n$ to guarantee a stable approximation and an error close to the best
approximation error. This is a major drawback for applications where $u$ is
expensive to evaluate. One remedy is to use a weighted least squares projection
using $n$ samples drawn from a properly selected distribution. In this paper,
we introduce a boosted weighted least-squares method which allows to ensure
almost surely the stability of the weighted least squares projection with a
sample size close to the interpolation regime $n=m$. It consists in sampling
according to a measure associated with the optimization of a stability
criterion over a collection of independent $n$-samples, and resampling
according to this measure until a stability condition is satisfied. A greedy
method is then proposed to remove points from the obtained sample.
Quasi-optimality properties are obtained for the weighted least-squares
projection, with or without the greedy procedure. The proposed method is
validated on numerical examples and compared to state-of-the-art interpolation
and weighted least squares methods.
"
"stat.TH","  This paper derives limit and estimation results for the spectral analysis of
time series with values in a separable Hilbert space. The convergence, in the
Hilbert-Schmidt operator norm, of the covariance operator of the functional
Discrete Fourier transform (fDFT) of the data to the spectral density operator
is proved, beyond the weak-dependent and linear cases. Under Short-Range
Dependence (SRD), a new Non-Central limit result for the periodogram operator
is obtained in the Gaussian case. This result also holds beyond the SRD
condition, under a suitable sieves approximation of the fDFT. Long-Range
Dependence (LRD) in functional sequences is characterized in the spectral
domain, applying spectral theory of self-adjoint operators on a separable
Hilbert space. This characterization covers the already analyzed
spatial-dependent (heterogeneous) LRD scenarios in functional time series. A
weak-consistent parameter estimator of the long--memory operator is derived,
based on the periodogram operator, and a weighted Kullback-Leibler divergence
operator.
"
"stat.TH","  Standard Bayesian inference is known to be sensitive to model
misspecification, leading to unreliable uncertainty quantification and poor
predictive performance. However, finding generally applicable and
computationally feasible methods for robust Bayesian inference under
misspecification has proven to be a difficult challenge. An intriguing,
easy-to-use, and widely applicable approach is to use bagging on the Bayesian
posterior (""BayesBag""); that is, to use the average of posterior distributions
conditioned on bootstrapped datasets. In this paper, we develop the asymptotic
theory of BayesBag, propose a model-data mismatch index for model criticism
using BayesBag, and empirically validate our theory and methodology on
synthetic and real-world data in linear regression, sparse logistic regression,
and a hierarchical mixed effects model. We find that in the presence of
significant misspecification, BayesBag yields more reproducible inferences and
has better predictive accuracy than the standard Bayesian posterior; on the
other hand, when the model is correctly specified, BayesBag produces superior
or equally good results. Overall, our results demonstrate that BayesBag
combines the attractive modeling features of standard Bayesian inference with
the distributional robustness properties of frequentist methods, providing
benefits over both Bayes alone and the bootstrap alone.
"
"stat.TH","  Based on deleting-item central limit theory, the classical Donsker's theorem
of partial-sum process of independent and identically distributed (i.i.d.)
random variables is extended to incomplete partial-sum process. The incomplete
partial-sum process Donsker's invariance principles are constructed and derived
for general partial-sum process of i.i.d random variables and empirical process
respectively, they are not only the extension of functional central limit
theory, but also the extension of deleting-item central limit theory. Our work
enriches the random elements structure of weak convergence.
"
"stat.TH","  In this expository note we describe a surprising phenomenon in
overparameterized linear regression, where the dimension exceeds the number of
samples: there is a regime where the test risk of the estimator found by
gradient descent increases with additional samples. In other words, more data
actually hurts the estimator. This behavior is implicit in a recent line of
theoretical works analyzing ""double-descent"" phenomenon in linear models. In
this note, we isolate and understand this behavior in an extremely simple
setting: linear regression with isotropic Gaussian covariates. In particular,
this occurs due to an unconventional type of bias-variance tradeoff in the
overparameterized regime: the bias decreases with more samples, but variance
increases.
"
"stat.TH","  We consider the problem of clustering datasets in the presence of arbitrary
outliers. Traditional clustering algorithms such as k-means and spectral
clustering are known to perform poorly for datasets contaminated with even a
small number of outliers. In this paper, we develop a provably robust spectral
clustering algorithm that applies a simple rounding scheme to denoise a
Gaussian kernel matrix built from the data points and uses vanilla spectral
clustering to recover the cluster labels of data points. We analyze the
performance of our algorithm under the assumption that the ""good"" data points
are generated from a mixture of sub-gaussians (we term these ""inliers""), while
the outlier points can come from any arbitrary probability distribution. For
this general class of models, we show that the asymptotic mis-classification
error decays at an exponential rate in the signal-to-noise ratio, provided the
number of outliers are a small fraction of the inlier points. Surprisingly,
this derived error bound matches with the best-known bound for semidefinite
programs (SDPs) under the same setting without outliers. We conduct extensive
experiments on a variety of simulated and real-world datasets to demonstrate
that our algorithm is less sensitive to outliers compared to other
state-of-the-art algorithms proposed in the literature.
"
"stat.TH","  There exist several endeavors proposing a new family of extended
distributions using the beta-generating technique. This is a well-known
mechanism in developing flexible distributions, by embedding the cumulative
distribution function (cdf) of a baseline distribution within the beta
distribution that acts as a generator. Univariate beta-generated distributions
offer many fruitful and tractable properties and have applications in
hydrology, biology and environmental sciences amongst other fields. In the
univariate cases, this extension works well, however, for multivariate cases,
the beta distribution generator delivers complex expressions. In this document,
the proposed extension from the univariate to the multivariate domain addresses
the need of flexible multivariate distributions that can model a wide range of
multivariate data. This new family of multivariate distributions, whose
marginals are beta-generated distributed, is constructed with the function
H(x_{1},...,x_{p})=F(G_{1}(x_{1}),G_{2}(x_{2}),...,G_{p}(x_{p})), where
$G_{i}(x_{i})$ are the cdfs of the gamma (baseline) distribution and F(.) as
the cdf of the Dirichlet distribution. Hence as the main example, a general
model having the support [0,1]^{p} (for p variates), using the Dirichlet as the
generator, is developed together with some distributional properties, such as
the moment generating function. The proposed Dirichlet-generated distributions
can be applied to compositional data. The parameters of the model are estimated
by using the maximum likelihood method. The effectiveness and prominence of the
proposed family are illustrated by analyzing simulated as well as two real
datasets. A new model testing technique is introduced to evaluate the
performance of the multivariate models.
"
"stat.TH","  We give a new example for a proper scoring rule motivated by the form of
Anderson--Darling distance of distribution functions and Example 5 in Brehmer
and Gneiting (2020).
"
"stat.TH","  We consider statistical models where functional data are artificially
contaminated by independent Wiener processes in order to satisfy privacy
constraints. We show that the corrupted observations have a Wiener density
which determines the distribution of the original functional random variables,
masked near the origin, uniquely, and we construct a nonparametric estimator of
that density. We derive an upper bound for its mean integrated squared error
which has a polynomial convergence rate, and we establish an asymptotic lower
bound on the minimax convergence rates which is close to the rate attained by
our estimator. Our estimator requires the choice of a basis and of two
smoothing parameters. We propose data-driven ways of choosing them and prove
that the asymptotic quality of our estimator is not significantly affected by
the empirical parameter selection. We examine the numerical performance of our
method via simulated examples.
"
"stat.TH","  A general jackknife estimator for the asymptotic covariance of moment
estimators is considered in the case when the sample is taken from a mixture
with varying concentrations of components. Consistency of the estimator is
demonstrated. A fast algorithm for its calculation is described. The estimator
is applied to construction of confidence sets for regression parameters in the
linear regression with errors in variables. An application to sociological data
analysis is considered.
"
"stat.TH","  In engineering systems, it is usually assumed that lifetimes of components
are independent and identically distributed (iid). But, the failure of a
component results in a higher load on the remaining components and hence causes
the distribution of the surviving components change. For modeling this kind of
systems, the theory of sequential order statistics (SOS) can be used. Assuming
Weibull distribution for lifetimes of components and conditionally proportional
hazard rates model as a special case of the SOS theory, the maximum likelihood
estimates of the unknown parameters are obtained in different cases. A new
model, denoted by PTCPHM, as a generalization of the iid case is proposed, and
then statistical inferential methods including point and interval estimation as
well as hypothesis tests under PTCPHM are then developed. Finally, a real data
on failure times of aircraft components, due to Mann and Fertig (1973), is
analyzed to illustrate the model and inferential methods developed here.
"
"stat.TH","  Probability density functions (PDFs) can be understood as continuous
compositions by the theory of Bayes spaces. The origin of a Bayes space is
determined by a given reference measure. This can be easily changed through the
well-known chain rule which has an impact on the geometry of the Bayes space.
This work provides a mathematical framework for setting a reference measure. It
is used to develop a weighting scheme on the bounded domain of distributional
data. The impact on statistical analysis is shown from the perspective of
simplicial functional principal component analysis. Moreover, a novel centered
log-ratio transformation is proposed to map a weighted Bayes spaces into an
unweighted $L^2$ space, enabling to use most tools developed in functional data
analysis (e.g. clustering, regression analysis, etc.) while accounting for the
weighting strategy. The potential of our proposal is shown through simulation
and on a real case study using Italian income data.
"
"stat.TH","  In this paper, we study non-asymptotic deviation bounds of the least squares
estimator in Gaussian AR($n$) processes. By relying on martingale concentration
inequalities and a tail-bound for $\chi^2$ distributed variables, we provide a
concentration bound for the sample covariance matrix of the process output.
With this, we present a problem-dependent finite-time bound on the deviation
probability of any fixed linear combination of the estimated parameters of the
AR$(n)$ process. We discuss extensions and limitations of our approach.
"
"stat.TH","  This article creates a link between two well-established fields in
mathematical statistics: empirical processes and inference based on
randomization via algebraic groups. To this end, a broadly applicable
conditional weak convergence theorem is developed for empirical processes that
are based on randomized observations. Random elements of an algebraic group are
applied to the data vectors from which the randomized version of a statistic is
derived. Combining a variant of the functional delta-method with a suitable
studentization of the statistic, asymptotically exact hypothesis tests can be
deduced, while the finite sample exactness property under group-invariant
sub-hypotheses is preserved. The methodology is exemplified with three
examples: the Pearson correlation coefficient, a Mann-Whitney effect based on
right-censored paired data, and a competing risks analysis. The practical
usefulness of the approaches is assessed through simulation studies and an
application to data from patients suffering from diabetic retinopathy.
"
"stat.TH","  Virtually any model we use in machine learning to make predictions does not
perfectly represent reality. So, most of the learning happens under model
misspecification. In this work, we present a novel analysis of the
generalization performance of Bayesian model averaging under model
misspecification and i.i.d. data using a new family of second-order PAC-Bayes
bounds. This analysis shows, in simple and intuitive terms, that Bayesian model
averaging provides suboptimal generalization performance when the model is
misspecified. In consequence, we provide strong theoretical arguments showing
that Bayesian methods are not optimal for learning predictive models, unless
the model class is perfectly specified. Using novel second-order PAC-Bayes
bounds, we derive a new family of Bayesian-like algorithms, which can be
implemented as variational and ensemble methods. The output of these algorithms
is a new posterior distribution, different from the Bayesian posterior, which
induces a posterior predictive distribution with better generalization
performance. Experiments with Bayesian neural networks illustrate these
findings.
"
"stat.TH","  A bivariate extension to Box and Jenkins (1963) feedback adjustment problem
is presented in this paper. The model balances the fixed cost of making an
adjustment, which is assumed independent of the magnitude of the adjustments,
with the cost of running the process off-target, which is assumed quadratic. It
is also assumed that two controllable factors are available to compensate for
the deviations from target of two responses in the presence of a bivariate
IMA(1,1) disturbance. The optimal policy has the form of a ""dead band"", in
which adjustments are justified only when the predicted process responses
exceed some boundary in $\mathbb{R}^2$. This boundary indicates when the
responses are predicted to be far enough from their targets that an additional
adjustment or intervention in the process is justified. Although originally
developed to control a machine tool, dead band control policies have
application in other areas. For example, they could be used to control a
disease through the application of a drug to a patient depending on the level
of a substance in the body (e.g., diabetes control). This paper presents
analytical formulae for the computation of the loss function that combines
off-target and adjustment costs per time unit. Expressions are derived for the
average adjustment interval and for the scaled mean square deviations from
target. The minimization of the loss function and the practical use of the
resulting dead band adjustment strategy is illustrated with an application to a
semiconductor manufacturing process.
"
"stat.TH","  We propose a general framework of sequential testing procedures based on
$U$-statistics which contains as an example a sequential CUSUM test based on
differences in mean but also includes a robust sequential Wilcoxon change point
procedure. Within this framework, we consider several monitoring schemes that
take different observations into account to make a decision at a given time
point. Unlike the originally proposed scheme that takes all observations of the
monitoring period into account, we also consider a modified moving-sum-version
as well as a version of a Page-monitoring scheme. The latter behave almost as
good for early changes while being advantageous for later changes. For all
proposed procedures we provide the limit distribution under the null hypothesis
which yields the threshold to control the asymptotic type-I-error. Furthermore,
we show that the proposed tests have asymptotic power one. In a simulation
study we compare the performance of the sequential procedures via their
empirical size, power and detection delay, which is further illustrated by
means of a temperature data set.
"
"stat.TH","  The extremal index $\theta$, a number in the interval $[0,1]$, is known to be
a measure of primal importance for analyzing the extremes of a stationary time
series. New rank-based estimators for $\theta$ are proposed which rely on the
construction of approximate samples from the exponential distribution with
parameter $\theta$ that is then to be fitted via the method of moments. The new
estimators are analyzed both theoretically as well as empirically through a
large-scale simulation study. In specific scenarios, in particular for time
series models with $\theta \approx 1$, they are found to be superior to recent
competitors from the literature.
"
"stat.TH","  In this paper, a new two-parameter model called generalized Ramos-Louzada
(GRL) distribution is proposed. The new model provides more flexibility in
modeling data with increasing, decreasing, j shaped and reversed-J shaped
hazard rate function. Several statistical and reliability properties of the GRL
model are also presented in this paper. The unknown parameters of the GRL
distribution are discussed using eight frequentist estimation approaches. These
approaches are important to develop a guideline to choose the best method of
estimation for the GRL parameters, that would be of great interest to
practitioners and applied statisticians. A detailed numerical simulation study
is carried out to examine the bias and the mean square error of the proposed
estimators. We illustrate the performance of the GRL distribution using two
real data sets from the fields of medicine and geology and both data sets show
that the new model is more appropriate as compared to the gamma, Marshall-Olkin
exponential, exponentiated exponential, beta exponential, generalized Lindley,
Poisson-Lomax, Lindley geometric and Lindley distributions, among others.
"
"stat.TH","  Adversarial attacks during the testing phase of neural networks pose a
challenge for the deployment of neural networks in security critical settings.
These attacks can be performed by adding noise that is imperceptible to humans
on top of the original data. By doing so, an attacker can create an adversarial
sample, which will cause neural networks to misclassify. In this paper, we seek
to understand the theoretical limits of what can be learned by neural networks
in the presence of an adversary. We first defined the hypothesis space of a
neural network, and showed the relationship between the growth number of the
entire neural network and the growth number of each neuron. Combine that with
the adversarial Vapnik-Chervonenkis(VC)-dimension of halfspace classifiers, we
concluded the adversarial VC-dimension of the neural networks with sign
activation functions.
"
"stat.TH","  Let $X_1,\dots, X_n$ be i.i.d. random variables sampled from a normal
distribution $N(\mu,\Sigma)$ in ${\mathbb R}^d$ with unknown parameter
$\theta=(\mu,\Sigma)\in \Theta:={\mathbb R}^d\times {\mathcal C}_+^d,$ where
${\mathcal C}_+^d$ is the cone of positively definite covariance operators in
${\mathbb R}^d.$ Given a smooth functional $f:\Theta \mapsto {\mathbb R}^1,$
the goal is to estimate $f(\theta)$ based on $X_1,\dots, X_n.$ Let $$
\Theta(a;d):={\mathbb R}^d\times \Bigl\{\Sigma\in {\mathcal C}_+^d:
\sigma(\Sigma)\subset [1/a, a]\Bigr\}, a\geq 1, $$ where $\sigma(\Sigma)$ is
the spectrum of covariance $\Sigma.$ Let $\hat \theta:=(\hat \mu, \hat
\Sigma),$ where $\hat \mu$ is the sample mean and $\hat \Sigma$ is the sample
covariance, based on the observations $X_1,\dots, X_n.$ For an arbitrary
functional $f\in C^s(\Theta),$ $s=k+1+\rho, k\geq 0, \rho\in (0,1],$ we define
a functional $f_k:\Theta \mapsto {\mathbb R}$ such that \begin{align*} &
\sup_{\theta\in \Theta(a;d)}\|f_k(\hat \theta)-f(\theta)\|_{L_2({\mathbb
P}_{\theta})} \lesssim_{s, \beta} \|f\|_{C^{s}(\Theta)}
\biggr[\biggl(\frac{a}{\sqrt{n}} \bigvee a^{\beta
s}\biggl(\sqrt{\frac{d}{n}}\biggr)^{s} \biggr)\wedge 1\biggr], \end{align*}
where $\beta =1$ for $k=0$ and $\beta>s-1$ is arbitrary for $k\geq 1.$ This
error rate is minimax optimal and similar bounds hold for more general loss
functions. If $d=d_n\leq n^{\alpha}$ for some $\alpha\in (0,1)$ and $s\geq
\frac{1}{1-\alpha},$ the rate becomes $O(n^{-1/2}).$ Moreover, for
$s>\frac{1}{1-\alpha},$ the estimators $f_k(\hat \theta)$ is shown to be
asymptotically efficient. The crucial part of the construction of estimator
$f_k(\hat \theta)$ is a bias reduction method studied in the paper for more
general statistical models than normal.
"
"stat.TH","  Spike-and-slab priors are popular Bayesian solutions for high-dimensional
linear regression problems. Previous theoretical studies on spike-and-slab
methods focus on specific prior formulations and use prior-dependent conditions
and analyses, and thus can not be generalized directly. In this paper, we
propose a class of generic spike-and-slab priors and develop a unified
framework to rigorously assess their theoretical properties. Technically, we
provide general conditions under which generic spike-and-slab priors can
achieve the nearly-optimal posterior contraction rate and the model selection
consistency. Our results include those of Narisetty and He (2014) and Castillo
et al. (2015) as special cases.
"
"stat.TH","  There has been considerable advance in understanding the properties of sparse
regularization procedures in high-dimensional models. In time series context,
it is mostly restricted to Gaussian autoregressions or mixing sequences. We
study oracle properties of LASSO estimation of weakly sparse
vector-autoregressive models with heavy tailed, weakly dependent innovations
with virtually no assumption on the conditional heteroskedasticity. In contrast
to current literature, our innovation process satisfy an $L^1$ mixingale type
condition on the centered conditional covariance matrices. This condition
covers $L^1$-NED sequences and strong ($\alpha$-) mixing sequences as
particular examples. From a modeling perspective, it covers several
multivariate-GARCH specifications, such as the BEKK model, and other factor
stochastic volatility specifications that were ruled out by assumption in
previous studies.
"
"stat.TH","  Population means and standard deviations are the most common estimands to
quantify effects in factorial layouts. In fact, most statistical procedures in
such designs are built towards inferring means or contrasts thereof. For more
robust analyses, we consider the population median, the interquartile range
(IQR) and more general quantile combinations as estimands in which we formulate
null hypotheses and calculate compatible confidence regions. Based upon
simultaneous multivariate central limit theorems and corresponding resampling
results, we derive asymptotically correct procedures in general, potentially
heteroscedastic, factorial designs with univariate endpoints. Special cases
cover robust tests for the population median or the IQR in arbitrary crossed
one-, two- and higher-way layouts with potentially heteroscedastic error
distributions. In extensive simulations we analyze their small sample
properties and also conduct an illustrating data analysis comparing children's
height and weight from different countries.
"
"stat.TH","  Meta-analysis, the statistical analysis of results from separate studies, is
a fundamental building block of science. But the assumptions of classical
meta-analysis models are not satisfied whenever publication bias is present,
which causes inconsistent parameter estimates. Hedges' selection function model
takes publication bias into account, but estimating and inferring with this
model is tough. Using a generalized Gleser--Hwang theorem, we show there is no
confidence set of guaranteed finite diameter for the parameters of Hedges'
selection model. This result provides a partial explanation for why inference
with Hedges' selection model is fraught with difficulties.
"
"stat.TH","  In this article we establish new central limit theorems for Ruppert-Polyak
averaged stochastic gradient descent schemes. Compared to previous work we do
not assume that convergence occurs to an isolated attractor but instead allow
convergence to a stable manifold. On the stable manifold the target function is
constant and the oscillations in the tangential direction may be significantly
larger than the ones in the normal direction. As we show, one still recovers a
central limit theorem with the same rates as in the case of isolated
attractors. Here we consider step-sizes $\gamma_n=n^{-\gamma}$ with
$\gamma\in(\frac34,1)$, typically.
"
"stat.TH","  Statistical system models provide the basis for the examination of various
sorts of distributions. Classification distributions are a very common and
versatile form of statistics in e.g. real economic, social, and IT systems. The
statistical distributions of classification features can be applied in
determining the a priori probabilities in Bayesian networks. We investigate a
statistical model of classification distributions based on finding the critical
point of a specialized form of entropy. A distribution function for
classification features is derived, with the two parameters $n_0$, minimal
class, and $\bar{N}$, average number of classes. Efficient algorithms for the
computation of the class probabilities and the approximation of real frequency
distributions are developed and applied to examples from different domains. The
method is compared to established distributions like Zipf's law. The majority
of examples can be approximated with a sufficient quality ($3-5\%$).
"
"stat.TH","  This work sets the matrix variate Birnbaum-Saunders theory in the context of
singular distributions and elliptical models. The so termed singular matrix
variate generalised Birnbaum-Saunders distribution is obtained with respect the
Hausdorff measure. Several basic properties and particular cases of this
distribution are also derived.
"
"stat.TH","  In multicenter research, individual-level data are often protected against
sharing across sites. To overcome the barrier of data sharing, many distributed
algorithms, which only require sharing aggregated information, have been
developed. The existing distributed algorithms usually assume the data are
homogeneously distributed across sites. This assumption ignores the important
fact that the data collected at different sites may come from various
sub-populations and environments, which can lead to heterogeneity in the
distribution of the data. Ignoring the heterogeneity may lead to erroneous
statistical inference. In this paper, we propose distributed algorithms which
account for the heterogeneous distributions by allowing site-specific nuisance
parameters. The proposed methods extend the surrogate likelihood approach to
the heterogeneous setting by applying a novel density ratio tilting method to
the efficient score function. The proposed algorithms maintain same
communication cost as the existing communication-efficient algorithms. We
establish the non-asymptotic risk bound of the proposed distributed estimator
and its limiting distribution in the two-index asymptotic setting. In addition,
we show that the asymptotic variance of the estimator attains the Cram\'er-Rao
lower bound. Finally, the simulation study shows the proposed algorithms reach
higher estimation accuracy compared to several existing methods.
"
"stat.TH","  This note presents a unified analysis of the identification of dynamical
systems with low-rank constraints under high-dimensional scaling. This
identification problem for dynamic systems are challenging due to the intrinsic
dependency of the data. To alleviate this problem, we first formulate this
identification problem into a multivariate linear regression problem with
row-sub-Gaussian measurement matrix using the more general input designs and
the independent repeated sampling schemes. We then propose a nuclear norm
heuristic method that estimates the parameter matrix of dynamic system from a
few input-state data samples. Based on this, we can extend the existing
results. In this paper, we consider two scenarios. (i) In the noiseless
scenario, nuclear-norm minimization is introduced for promoting low-rank. We
define the notion of weak restricted isometry property, which is weaker than
the ordinary restricted isometry property, and show it holds with high
probability for the row-sub-Gaussian measurement matrix. Thereby, the
rank-minimization matrix can be exactly recovered from finite number of data
samples. (ii) In the noisy scenario, a regularized framework involving nuclear
norm penalty is established. We give the notion of operator norm curvature
condition for the loss function, and show it holds for row-sub-Gaussian
measurement matrix with high probability. Consequently, when specifying the
suitable choice of the regularization parameter, the operator norm error of the
optimal solution of this program has a sharp bound given a finite amount of
data samples. This operator norm error bound is stronger than the ordinary
Frobenius norm error bound obtained in the existing work.
"
"stat.TH","  In this paper, we derive closed-form expressions for significant statistical
properties of the link signal-to-noise ratio (SNR) and the separation distance
in mobile ad hoc networks subject to Ornstein-Uhlenbeck (OU) mobility and
Rayleigh fading. In these systems, the SNR is a critical parameter as it
directly influences link performance. In the absence of signal fading, the
distribution of the link SNR depends exclusively on the squared distance
between nodes, which is governed by the mobility model. In our analysis, nodes
move randomly according to an Ornstein-Uhlenbeck process, using one tuning
parameter to control the temporal dependency in the mobility pattern. We derive
a complete statistical description of the squared distance and show that it
forms a stationary Markov process. Then, we compute closed-form expressions for
the probability density function (pdf), the cumulative distribution function
(cdf), the bivariate pdf, and the bivariate cdf of the link SNR. Next, we
introduce small-scale fading, modelled by a Rayleigh random variable, and
evaluate the pdf of the link SNR for rational path loss exponents. The validity
of our theoretical analysis is verified by extensive simulation studies. The
results presented in this work can be used to quantify link uncertainty and
evaluate stability in mobile ad hoc wireless systems.
"
"stat.TH","  We consider dynamics driven by interaction energies on graphs. We introduce
graph analogues of the continuum nonlocal-interaction equation and interpret
them as gradient flows with respect to a graph Wasserstein distance. The
particular Wasserstein distance we consider arises from the graph analogue of
the Benamou-Brenier formulation where the graph continuity equation uses an
upwind interpolation to define the density along the edges. While this approach
has both theoretical and computational advantages, the resulting distance is
only a quasi-metric. We investigate this quasi-metric both on graphs and on
more general structures where the set of ""vertices"" is an arbitrary positive
measure. We call the resulting gradient flow of the nonlocal-interaction energy
the nonlocal nonlocal-interaction equation (NL$^2$IE). We develop the existence
theory for the solutions of the NL$^2$IE as curves of maximal slope with
respect to the upwind Wasserstein quasi-metric. Furthermore, we show that the
solutions of the NL$^2$IE on graphs converge as the empirical measures of the
set of vertices converge weakly, which establishes a valuable
discrete-to-continuum convergence result.
"
"stat.TH","  In dealing with veracity of data analytics, fuzzy methods are more and more
relying on probabilistic and statistical techniques to underpin their
applicability. Conversely, standard statistical models usually disregard to
take into account the inherent fuzziness of choices and this issue is
particularly worthy of note in customers' satisfaction surveys, since there are
different shades of evaluations that classical statistical tools fail to catch.
Given these motivations, the paper introduces a model-based fuzzy analysis of
questionnaire with sound statistical foundation, driven by the design of a
hybrid method that sets in between fuzzy evaluation systems and statistical
modelling. The proposal is advanced on the basis of \cub mixture models to
account for uncertainty in ordinal data analysis and moves within the general
framework of Intuitionistic Fuzzy Set theory to allow membership,
non-membership, vagueness and accuracy assessments. Particular emphasis is
given to defuzzification procedures that enable uncertainty measures also at an
aggregated level. An application to a survey run at the University of Naples
Federico II about the evaluation of Orientation Services supports the efficacy
of the proposal.
"
"stat.TH","  Many problems in materials science and biology involve particles interacting
with strong, short-ranged bonds, that can break and form on experimental
timescales. Treating such bonds as constraints can significantly speed up
sampling their equilibrium distribution, and there are several methods to
sample probability distributions subject to fixed constraints. We introduce a
Monte Carlo method to handle the case when constraints can break and form. More
generally, the method samples a probability distribution on a stratification: a
collection of manifolds of different dimensions, where the lower-dimensional
manifolds lie on the boundaries of the higher-dimensional manifolds. We show
several applications of the method in polymer physics, self-assembly of
colloids, and volume calculation in high dimensions.
"
"stat.TH","  Popular network models such as the mixed membership and standard stochastic
block model are known to exhibit distinct geometric structure when embedded
into $\mathbb{R}^{d}$ using spectral methods. The resulting point cloud
concentrates around a simplex in the first model, whereas it separates into
clusters in the second. By adopting the formalism of generalised random
dot-product graphs, we demonstrate that both of these models, and different
mixing regimes in the case of mixed membership, may be distinguished by the
persistent homology of the underlying point distribution in the case of
adjacency spectral embedding. Moreover, despite non-identifiability issues, we
show that the persistent homology of the support of the distribution and its
super-level sets can be consistently estimated. As an application of our
consistency results, we provide a topological hypothesis test for
distinguishing the standard and mixed membership stochastic block models.
"
"stat.TH","  Topological statistical theory provides the foundation for a modern
mathematical reformulation of classical statistical theory: Structural
Statistics emphasizes the structural assumptions that accompany distribution
families and the set of structure preserving transformations between them,
given by their statistical morphisms. The resulting language is designed to
integrate complicated structured model spaces like deep-learning models and to
close the gap to topology and differential geometry. To preserve the
compatibility to classical statistics the language comprises corresponding
concepts for standard information criteria like sufficiency and completeness.
"
"stat.TH","  We leverage recent advances in high-dimensional statistics to derive new L2
estimation upper bounds for Lasso and Group Lasso in high-dimensions. For
Lasso, our bounds scale as $(k^*/n) \log(p/k^*)$---$n\times p$ is the size of
the design matrix and $k^*$ the dimension of the ground truth
$\boldsymbol{\beta}^*$---and match the optimal minimax rate. For Group Lasso,
our bounds scale as $(s^*/n) \log\left( G / s^* \right) + m^* / n$---$G$ is the
total number of groups and $m^*$ the number of coefficients in the $s^*$ groups
which contain $\boldsymbol{\beta}^*$---and improve over existing results. We
additionally show that when the signal is strongly group-sparse, Group Lasso is
superior to Lasso.
"
"stat.TH","  This paper deals with the Gaussian and bootstrap approximations to the
distribution of the max statistic in high dimensions. This statistic takes the
form of the maximum over components of the sum of independent random vectors
and its distribution plays a key role in many high-dimensional econometric
problems. Using a novel iterative randomized Lindeberg method, the paper
derives new bounds for the distributional approximation errors. These new
bounds substantially improve upon existing ones and simultaneously allow for a
larger class of bootstrap methods.
"
"stat.TH","  We provide sufficient conditions under which the center-outward distribution
and quantile functions introduced in Chernozhukov et al.~(2017) and
Hallin~(2017) are homeomorphisms, thereby extending a recent result by Figalli
\cite{Fi2}. Our approach relies on Cafarelli's classical regularity theory for
the solutions of the Monge-Amp\`ere equation, but has to deal with difficulties
related with the unboundedness at the origin of the density of the spherical
uniform reference measure. Our conditions are satisfied by probabillities on
Euclidean space with a general (bounded or unbounded) convex support which are
not covered in~\cite{Fi2}. We provide some additional results about
center-outward distribution and quantile functions, including the fact that
quantile sets exhibit some weak form of convexity.
"
"stat.TH","  The first part of this paper is devoted to the decision-theoretic analysis of
random-design linear prediction. It is known that, under boundedness
constraints on the response (and thus on regression coefficients), the minimax
excess risk scales, up to constants, as $\sigma^2 d / n$ in dimension $d$ with
$n$ samples and noise $\sigma^2$. Here, we study the expected excess risk with
respect to the full linear class. We show that the ordinary least squares
estimator is exactly minimax optimal in the well-specified case for every
distribution of covariates. Further, we express the minimax risk in terms of
the distribution of \emph{statistical leverage scores} of individual samples.
We deduce a precise minimax lower bound of $\sigma^2d/(n-d+1)$ for general
covariate distribution, which nearly matches the risk for Gaussian design. We
then obtain nonasymptotic upper bounds on the minimax risk for covariates that
satisfy a ""small ball""-type regularity condition, which scale as
$(1+o(1))\sigma^2d/n$ as $d=o(n)$, both in the well-specified and misspecified
cases.
  Our main technical contribution is the study of the lower tail of the
smallest singular value of empirical covariance matrices around $0$. We
establish a lower bound on this lower tail, valid for any distribution in
dimension $d \geq 2$, together with a matching upper bound under a necessary
regularity condition. Our proof relies on the PAC-Bayesian technique for
controlling empirical processes, and extends an analysis of Oliveira devoted to
a different part of the lower tail. Equivalently, our upper bound shows that
the operator norm of the inverse sample covariance matrix has bounded $L^q$
norm up to $q \asymp n$, and our lower bound implies that this exponent is
unimprovable. Finally, we show that the regularity condition naturally holds
for independent coordinates.
"
"stat.TH","  We introduce a procedure for predictive conditional density estimation under
logarithmic loss, which we call SMP (Sample Minmax Predictor). This estimator
minimizes a new general excess risk bound for supervised statistical learning.
On standard examples, this bound scales as $d/n$ with $d$ the model dimension
and $n$ the sample size, and critically remains valid under model
misspecification. Being an improper (out-of-model) procedure, SMP improves over
within-model estimators such as the maximum likelihood estimator, whose excess
risk degrades under misspecification. Compared to approaches reducing to the
sequential problem, our bounds remove suboptimal $\log n$ factors, addressing
an open problem from Gr\""unwald and Kotlowski for the considered models, and
can handle unbounded classes. For the Gaussian linear model, the predictions
and risk bound of SMP are governed by leverage scores of covariates, nearly
matching the optimal risk in the well-specified case without conditions on the
noise variance or approximation error of the linear model. For logistic
regression, SMP provides a non-Bayesian approach to calibration of
probabilistic predictions relying on virtual samples, and can be computed by
solving two logistic regressions. It achieves a non-asymptotic excess risk of
$O ( (d + B^2R^2)/n )$, where $R$ bounds the norm of features and $B$ that of
the comparison parameter; by contrast, no within-model estimator can achieve
better rate than $\min( {B R}/{\sqrt{n}}, {d e^{BR}}/{n} )$ in general. This
provides a computationally more efficient alternative to Bayesian approaches,
which require approximate posterior sampling, thereby partly answering a
question by Foster et al. (2018).
"
"stat.TH","  Chromy (1979) proposed a unequal probability sampling algorithm, which
enables to select a sample in one pass of the sampling frame only. This is the
default sequential method used in the SURVEYSELECT procedure of the SAS
software. In this article, we study the properties of Chromy sampling. We prove
that the Horvitz-Thompson is asymptotically normally distributed, and give an
explicit expression for the second-order inclusion probabilities. This makes it
possible to estimate the variance unbiasedly for the randomized version of the
method programmed in the SURVEYSELECT procedure.
"
"stat.TH","  Sobol sensitivity indices allow to quantify the respective effects of random
input variables and their combinations on the variance of mathematical model
output. We focus on the problem of Sobol indices estimation via a metamodeling
approach where we replace the true mathematical model with a sample-based
approximation to compute sensitivity indices. We propose a new method for
indices quality control and obtain asymptotic and non-asymptotic risk bounds
for Sobol indices estimates based on a general class of metamodels. Our
analysis is closely connected with the problem of nonparametric function
fitting using the orthogonal system of functions in the random design setting.
It considers the relation between the metamodel quality and the error of the
corresponding estimator for Sobol indices and shows the possibility of fast
convergence rates in the case of noiseless observations. The theoretical
results are complemented with numerical experiments for the approximations
based on multivariate Legendre and Trigonometric polynomials.
"
"stat.TH","  We study efficient algorithms for linear regression and covariance estimation
in the absence of Gaussian assumptions on the underlying distributions of
samples, making assumptions instead about only finitely-many moments. We focus
on how many samples are needed to do estimation and regression with high
accuracy and exponentially-good success probability.
  For covariance estimation, linear regression, and several other problems,
estimators have recently been constructed with sample complexities and rates of
error matching what is possible when the underlying distribution is Gaussian,
but algorithms for these estimators require exponential time. We narrow the gap
between the Gaussian and heavy-tailed settings for polynomial-time estimators
with:
  1. A polynomial-time estimator which takes $n$ samples from a random vector
$X \in R^d$ with covariance $\Sigma$ and produces $\hat{\Sigma}$ such that in
spectral norm $\|\hat{\Sigma} - \Sigma \|_2 \leq \tilde{O}(d^{3/4}/\sqrt{n})$
w.p. $1-2^{-d}$. The information-theoretically optimal error bound is
$\tilde{O}(\sqrt{d/n})$; previous approaches to polynomial-time algorithms were
stuck at $\tilde{O}(d/\sqrt{n})$.
  2. A polynomial-time algorithm which takes $n$ samples $(X_i,Y_i)$ where $Y_i
= \langle u,X_i \rangle + \varepsilon_i$ and produces $\hat{u}$ such that the
loss $\|u - \hat{u}\|^2 \leq O(d/n)$ w.p. $1-2^{-d}$ for any $n \geq d^{3/2}
\log(d)^{O(1)}$. This (information-theoretically optimal) error is achieved by
inefficient algorithms for any $n \gg d$; previous polynomial-time algorithms
suffer loss $\Omega(d^2/n)$ and require $n \gg d^2$.
  Our algorithms use degree-$8$ sum-of-squares semidefinite programs. We offer
preliminary evidence that improving these rates of error in polynomial time is
not possible in the median of means framework our algorithms employ.
"
"stat.TH","  We study the problem of estimating linear response statistics under external
perturbations using time series of unperturbed dynamics. A standard approach to
this estimation problem is to employ the Fluctuation-Dissipation Theory, which
requires the knowledge of the functional form of the underlying unperturbed
density that is not available in general. To overcome this issue, we consider a
nonparametric density estimator formulated by the kernel embedding of
distributions. To avoid the computational expense associated with using radial
type kernels, we consider the ""Mercer-type"" kernels constructed based on the
classical orthogonal bases defined on non-compact domains. While the resulting
representation is analogous to Polynomial Chaos Expansion(PCE), by studying in
the reproducing kernel Hilbert space(RKHS) setting, we establish the uniform
convergence of the estimator. More importantly, the RKHS formulation allows one
to systematically address a practical question of identifying the PCE basis for
a consistent estimation through the decay property of the target functions that
can be quantified using the available data. In terms of the linear response
estimation, our study provides practical conditions for the well-posedness of
not only the estimator but also the well-posedness of the underlying response
statistics. We provide a theoretical guarantee for the convergence of the
estimator to the underlying linear response statistics. Finally, we offer an
error bound for the density estimation that accounts for the Monte-Carlo
averaging over non-i.i.d time series and the biases due to truncation. This
error bound helps understand the feasibility as well as limitation of the
kernel embedding with Mercer-type kernels. Numerically, we verify the
effectiveness of the kernel embedding linear response estimator on two
stochastic dynamics with known, yet, non-trivial equilibrium densities.
"
"stat.TH","  We propose a general method for constructing hypothesis tests and confidence
sets that have finite sample guarantees without regularity conditions. We refer
to such procedures as ""universal."" The method is very simple and is based on a
modified version of the usual likelihood ratio statistic, that we call ""the
split likelihood ratio test"" (split LRT). The method is especially appealing
for irregular statistical models. Canonical examples include mixture models and
models that arise in shape-constrained inference. %mixture models and
shape-constrained models are just two examples. Constructing tests and
confidence sets for such models is notoriously difficult. Typical inference
methods, like the likelihood ratio test, are not useful in these cases because
they have intractable limiting distributions. In contrast, the method we
suggest works for any parametric model and also for some nonparametric models.
The split LRT can also be used with profile likelihoods to deal with nuisance
parameters, and it can also be run sequentially to yield anytime-valid
$p$-values and confidence sequences.
"
"stat.TH","  Determining the precise rank is an important problem in many large-scale
applications with matrix data exploiting low-rank plus noise models. In this
paper, we suggest a universal approach to rank inference via residual
subsampling (RIRS) for testing and estimating rank in a wide family of models,
including many popularly used network models such as the degree corrected mixed
membership model as a special case. Our procedure constructs a test statistic
via subsampling entries of the residual matrix after extracting the spiked
components. The test statistic converges in distribution to the standard normal
under the null hypothesis, and diverges to infinity with asymptotic probability
one under the alternative hypothesis. The effectiveness of RIRS procedure is
justified theoretically, utilizing the asymptotic expansions of eigenvectors
and eigenvalues for large random matrices recently developed in Fan et al.
(2019a) and Fan et al. (2019b). The advantages of the newly suggested procedure
are demonstrated through several simulation and real data examples.
"
"stat.TH","  We analyse correspondence of a text to a simple probabilistic model. The
model assumes that the words are selected independently from an infinite
dictionary. The probability distribution correspond to the Zipf---Mandelbrot
law. We count sequentially the numbers of different words in the text and get
the process of the numbers of different words. Then we estimate
Zipf---Mandelbrot law parameters using the same sequence and construct an
estimate of the expectation of the number of different words in the text. Then
we subtract the corresponding values of the estimate from the sequence and
normalize along the coordinate axes, obtaining a random process on a segment
from 0 to 1. We prove that this process (the empirical text bridge) converges
weakly in the uniform metric on $C (0,1)$ to a centered Gaussian process with
continuous a.s. paths. We develop and implement an algorithm for approximate
calculation of eigenvalues of the covariance function of the limit Gaussian
process, and then an algorithm for calculating the probability distribution of
the integral of the square of this process. We use the algorithm to analyze
uniformity of texts in English, French, Russian and Chinese.
"
"stat.TH","  Confounding matters in almost all observational studies that focus on
causality. In order to eliminate bias caused by connfounders, oftentimes a
substantial number of features need to be collected in the analysis. In this
case, large p small n problem can arise and dimensional reduction technique is
required. However, the traditional variable selection methods which focus on
prediction are problematic in this setting. Throughout this paper, we analyze
this issue in detail and assume the sparsity of confounders which is different
from the previous works. Under this assumption we propose several variable
selection methods based on support intersection to pick out the confounders.
Also we discussed the different approaches for estimation of causal effect and
unconfoundedness test. To aid in our description, finally we provide numerical
simulations to support our claims and compare to common heuristic methods, as
well as applications on real dataset.
"
"stat.TH","  We consider the problem of quickest change detection (QCD) in a signal where
its observations are obtained using a set of actions, and switching from one
action to another comes with a cost. The objective is to design a stopping rule
consisting of a sampling policy to determine the sequence of actions used to
observe the signal and a stopping time to quickly detect for the change,
subject to a constraint on the average observation-switching cost. We divide
the problem into open-loop sampling policies and casual sampling policies. For
open-loop sampling policies, we propose a sampling policy of finite window size
and a generalized likelihood ratio (GLR) Cumulative Sum (CuSum) stopping time
for the QCD problem. We show that the GLR CuSum stopping time is asymptotically
optimal with a properly designed sampling policy and formulate the design of
this sampling policy as a quadratic programming problem. We prove that it is
sufficient to consider policies of window size not more than one when designing
policies of finite window length and propose several algorithms that solve this
optimization problem with theoretical guarantees. For casual policies, we
proposed a 2-threshold stopping time and a casual sampling policy. We present a
method to design the casual sampling policy based on open-loop sampling
policies. Finally, we apply our approach to the problem of QCD of a partially
observed graph signal and empirically demonstrate the performance of our
proposed stopping times.
"
"stat.TH","  We study the asymptotic theory of misspecified models for diffusion processes
with noisy nonsynchronous observations. Unlike with correctly specified models,
the original maximum-likelihood-type estimator has an asymptotic bias under the
misspecified setting and fails to achieve an optimal rate of convergence. To
address this, we consider a new quasi-likelihood function that arrows
constructing a maximum-likelihood-type estimator that achieves the optimal rate
of convergence. Study of misspecified models enables us to apply
machine-learning techniques to the maximum-likelihood approach. With these
techniques, we can efficiently study the microstructure of a stock market by
using rich information of high-frequency data. Neural networks have
particularly good compatibility with the maximum-likelihood approach, so we
will consider an example of using a neural network for simulation studies and
empirical analysis of high-frequency data from the Tokyo Stock Exchange. We
demonstrate that the neural network outperforms polynomial models in volatility
predictions for major stocks in Tokyo Stock Exchange.
"
"stat.TH","  We conduct a theoretical and numerical study of the aliased spectral
densities and inverse operators of Mat\'ern covariance functions on a regular
grid of points. We apply our results to provide clarity on the properties of a
popular approximation based on stochastic partial differential equations; while
others have shown that it can approximate the covariance function well, we find
that it assigns too much power at high frequencies and does not provide
increasingly accurate approximations to the inverse as the grid spacing goes to
zero, except in the one-dimensional exponential covariance case. In a
simulation study, we compare the SPDE approximation toseveral other
approximations on the task of estimating Mat\'ern covariance parameters,
finding that the SPDE approximation overestimates short range spatial
dependence in the zero noise case, but the bias lessens when when noise is
added. In the zero noise case, an sparse approximation that minimizes
Kullback-Leibler divergence has better performance.
"
"stat.TH","  A new Central Limit Theorem (CLT) is developed for random variables of the
form $\xi=z^\top f(z) - \text{div} f(z)$ where $z\sim N(0,I_n)$. The normal
approximation is proved to hold when the squared norm of $f(z)$ dominates the
squared Frobenius norm of $\nabla f(z)$ in expectation.
  Applications of this CLT are given for the asymptotic normality of de-biased
estimators in linear regression with correlated design and convex penalty in
the regime $p/n\to \gamma\in (0,{\infty})$. For the estimation of linear
functions $\langle a_0,\beta\rangle$ of the unknown coefficient vector $\beta$,
this analysis leads to asymptotic normality of the de-biased estimate for most
normalized directions $a_0$, where ""most"" is quantified in a precise sense.
This asymptotic normality holds for any coercive convex penalty if $\gamma<1$
and for any strongly convex penalty if $\gamma\ge 1$. In particular the penalty
needs not be separable or permutation invariant. For the group Lasso, a simple
condition is given that grants asymptotic normality for a fixed direction
$a_0$. By allowing arbitrary regularizers, the results vastly broaden the scope
of applicability of de-biasing methodologies to obtain confidence intervals in
high-dimensions.
  In the absence of strong convexity for p > n, asymptotic normality of the
de-biased estimate is obtained under additional conditions that are naturally
satisfied by the Lasso and the group Lasso.
"
"stat.TH","  We address a fundamental issue in the nonparametric inference for systems of
interacting particles: the identifiability of the interaction functions. We
prove that the interaction functions are identifiable for a class of
first-order stochastic systems, including linear systems with general initial
laws and nonlinear systems with stationary distributions. We show that a
coercivity condition is sufficient for identifiability and becomes necessary
when the number of particles approaches infinity. The coercivity is equivalent
to the strict positivity of related integral operators, which we prove by
showing that their integral kernels are strictly positive definite by using
M\""untz type theorems.
"
"stat.TH","  Distance correlation has gained much recent attention in the data science
community: the sample statistic is straightforward to compute and
asymptotically equals zero if and only if independence, making it an ideal
choice to test any type of dependency structure given sufficient sample size.
One major bottleneck is the testing process: because the null distribution of
distance correlation depends on the underlying random variables and metric
choice, it typically requires a permutation test to estimate the null and
compute the p-value, which is very costly for large amount of data. To overcome
the difficulty, we propose a centered chi-square distribution, demonstrate it
well-approximates the limiting null distribution of unbiased distance
correlation, and prove upper tail dominance and distribution bound. The
resulting distance correlation chi-square test is a nonparametric test for
independence, is valid and universally consistent using any strong negative
type metric or characteristic kernel, enjoys a similar finite-sample testing
power as the standard permutation test, is provably most powerful among all
valid tests of distance correlation using known distributions, and is also
applicable to K-sample and partial testing.
"
"stat.TH","  Many objects of interest can be expressed as a linear, mean square continuous
functional of a least squares projection (regression). Often the regression may
be high dimensional, depending on many variables. This paper gives minimal
conditions for root-n consistent and efficient estimation of such objects when
the regression and the Riesz representer of the functional are approximately
sparse and the sum of the absolute value of the coefficients is bounded. The
approximately sparse functions we consider are those where an approximation by
some $t$ regressors has root mean square error less than or equal to
$Ct^{-\xi}$ for $C,$ $\xi>0.$ We show that a necessary condition for efficient
estimation is that the sparse approximation rate $\xi_{1}$ for the regression
and the rate $\xi_{2}$ for the Riesz representer satisfy $\max\{\xi_{1}
,\xi_{2}\}>1/2.$ This condition is stronger than the corresponding condition
$\xi_{1}+\xi_{2}>1/2$ for Holder classes of functions. We also show that Lasso
based, cross-fit, debiased machine learning estimators are asymptotically
efficient under these conditions. In addition we show efficiency of an
estimator without cross-fitting when the functional depends on the regressors
and the regression sparse approximation rate satisfies $\xi_{1}>1/2$.
"
"stat.TH","  We show that the problem of finding the measure supported on a compact subset
K of the complex plane such that the variance of the least squares predictor by
polynomials of degree at most n at a point exterior to K is a minimum, is
equivalent to the problem of finding the polynomial of degree at most n,
bounded by 1 on K with extremal growth at this external point. We use this to
find the polynomials of extremal growth for the interval [-1,1] at a purely
imaginary point. The related problem on the extremal growth of real polynomials
was studied by Erd\H{o}s in 1947.
"
"stat.TH","  Bayesian methods are actively used for parameter identification and
uncertainty quantification when solving nonlinear inverse problems with random
noise. However, there are only few theoretical results justifying the Bayesian
approach. Recent papers, see e.g. \cite{Nickl2017,lu2017bernsteinvon} and
references therein, illustrate the main difficulties and challenges in studying
the properties of the posterior distribution in the nonparametric setup. This
paper offers a new approach for study the frequentist properties of the
nonparametric Bayes procedures. The idea of the approach is to relax the
nonlinear structural equation by introducing an auxiliary functional parameter
and replacing the structural equation with a penalty and by imposing a prior on
the auxiliary parameter. For the such extended model, we state sharp bounds on
posterior concentration and on the accuracy of the penalized MLE and on
Gaussian approximation of the posterior, and a number of further results. All
the bounds are given in terms of effective dimension, and we show that the
proposed calming device does not significantly affect this value.
"
"stat.TH","  We consider the variance of a function of $n$ independent random variables
and provide new inequalities which, in particular, extend previous results
obtained for symmetric functions in the i.i.d.~setting. For instance, we obtain
various upper and lower variance bounds based on iterated jackknives statistics
that can be considered as generalizations of the Efron-Stein inequality.
"
"stat.TH","  Parameter inference of dynamical systems is a challenging task faced by many
researchers and practitioners across various fields. In many applications, it
is common that only limited variables are observable. In this paper, we propose
a method for parameter inference of a system of nonlinear coupled ODEs with
partial observations. Our method combines fast Gaussian process based gradient
matching (FGPGM) and deterministic optimization algorithms. By using initial
values obtained by Bayesian steps with low sampling numbers, our deterministic
optimization algorithm is both accurate and efficient.
"
"stat.TH","  It is known that describing or calculating the conditional probabilities of
multiple events is exponentially expensive. In this work, Bayesian tensor
network (BTN) is proposed to efficiently capture the conditional probabilities
of multiple sets of events with polynomial complexity. BTN is a directed
acyclic graphical model that forms a subset of TN. To testify its validity for
exponentially many events, BTN is implemented to the image recognition, where
the classification is mapped to capturing the conditional probabilities in an
exponentially large sample space. Competitive performance is achieved by the
BTN with simple tree network structures. Analogous to the tensor network
simulations of quantum systems, the validity of the simple-tree BTN implies an
``area law'' of fluctuations in image recognition problems.
"
"stat.TH","  Forecasting the evolution of complex systems is one of the grand challenges
of modern data science. The fundamental difficulty lies in understanding the
structure of the observed stochastic process. In this paper, we show that every
uniformly-positive-definite-in-covariance and sufficiently short-range
dependent non-stationary and nonlinear time series can be well approximated
globally by an auto-regressive process of slowly diverging order. When linear
prediction with ${\cal L}^2$ loss is concerned, the latter result facilitates a
unified globally-optimal short-term forecasting theory for a wide class of
locally stationary time series asymptotically. A nonparametric sieve method is
proposed to globally and adaptively estimate the optimal forecasting
coefficient functions and the associated mean squared error of forecast. An
adaptive stability test is proposed to check whether the optimal forecasting
coefficients are time-varying, a frequently-encountered question for
practitioners and researchers of time series. Furthermore, partial
auto-correlation functions (PACF) of general non-stationary time series are
studied and used as a visual tool to explore the linear dependence structure of
such series. We use extensive numerical simulations and two real data examples
to illustrate the usefulness of our results.
"
"stat.TH","  We consider the linear regression problem of estimating a $p$-dimensional
vector $\beta$ from $n$ observations $Y = X \beta + W$, where $\beta_j
\stackrel{\text{i.i.d.}}{\sim} \pi$ for a real-valued distribution $\pi$ with
zero mean and unit variance, $X_{ij} \stackrel{\text{i.i.d.}}{\sim}
\mathcal{N}(0,1)$, and $W_i\stackrel{\text{i.i.d.}}{\sim} \mathcal{N}(0,
\sigma^2)$. In the asymptotic regime where $n/p \to \delta$ and $ p/ \sigma^2
\to \mathsf{snr}$ for two fixed constants $\delta, \mathsf{snr}\in (0, \infty)$
as $p \to \infty$, the limiting (normalized) minimum mean-squared error (MMSE)
has been characterized by the MMSE of an associated single-letter (additive
Gaussian scalar) channel.
  In this paper, we show that if the MMSE function of the single-letter channel
converges to a step function, then the limiting MMSE of estimating $\beta$ in
the linear regression problem converges to a step function which jumps from $1$
to $0$ at a critical threshold. Moreover, we establish that the limiting
mean-squared error of the (MSE-optimal) approximate message passing algorithm
also converges to a step function with a larger threshold, providing evidence
for the presence of a computational-statistical gap between the two thresholds.
"
"stat.TH","  Due to the recent advancements in wearables and sensing technology, health
scientists are increasingly developing mobile health (mHealth) interventions.
In mHealth interventions, mobile devices are used to deliver treatment to
individuals as they go about their daily lives. These treatments are generally
designed to impact a near time, proximal outcome such as stress or physical
activity. The mHealth intervention policies, often called just-in-time adaptive
interventions, are decision rules that map an individual's current state (e.g.,
individual's past behaviors as well as current observations of time, location,
social activity, stress and urges to smoke) to a particular treatment at each
of many time points. The vast majority of current mHealth interventions deploy
expert-derived policies. In this paper, we provide an approach for conducting
inference about the performance of one or more such policies using historical
data collected under a possibly different policy. Our measure of performance is
the average of proximal outcomes over a long time period should the particular
mHealth policy be followed. We provide an estimator as well as confidence
intervals. This work is motivated by HeartSteps, an mHealth physical activity
intervention.
"
"stat.TH","  A model-free bootstrap procedure for a general class of stationary time
series is introduced. The theoretical framework is established, showing
asymptotic validity of bootstrap confidence intervals for many statistics of
interest. In addition, asymptotic validity of one-step ahead bootstrap
prediction intervals is also demonstrated. Finite-sample experiments are
conducted to empirically confirm the performance of the new method, and to
compare with
  popular methods such as the block bootstrap and the autoregressive (AR)-sieve
bootstrap.
"
"stat.TH","  We consider the issue of biases in scholarly research, specifically, in peer
review. There is a long standing debate on whether exposing author identities
to reviewers induces biases against certain groups, and our focus is on
designing tests to detect the presence of such biases. Our starting point is a
remarkable recent work by Tomkins, Zhang and Heavlin which conducted a
controlled, large-scale experiment to investigate existence of biases in the
peer reviewing of the WSDM conference. We present two sets of results in this
paper. The first set of results is negative, and pertains to the statistical
tests and the experimental setup used in the work of Tomkins et al. We show
that the test employed therein does not guarantee control over false alarm
probability and under correlations between relevant variables coupled with any
of the following conditions, with high probability, can declare a presence of
bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c)
reviewer calibration. Moreover, we show that the setup of their experiment may
itself inflate false alarm probability if (d) bidding is performed in non-blind
manner or (e) popular reviewer assignment procedure is employed. Our second set
of results is positive and is built around a novel approach to testing for
biases that we propose. We present a general framework for testing for biases
in (single vs. double blind) peer review. We then design hypothesis tests that
under minimal assumptions guarantee control over false alarm probability and
non-trivial power even under conditions (a)--(c) as well as propose an
alternative experimental setup which mitigates issues (d) and (e). Finally, we
show that no statistical test can improve over the non-parametric tests we
consider in terms of the assumptions required to control for the false alarm
probability.
"
"stat.TH","  A Poisson mixture is one of the practically important models in computer
science, biology, and sociology. However, the theoretical property has not been
studied because the posterior distribution can not be approximated by any
normal distribution. Such a model is called singular and it is known that Real
Log Canonical Threshold (RLCT) is equal to the coefficient of the
asymptotically main term of the Bayesian generalization error. In this paper,
we derive RLCT of a simplex Vandermonde matrix type singularity which is equal
to that of a Poisson mixture in general cases.
"
"stat.TH","  The topic of this paper is multiple hypothesis testing based on e-values,
which are Bayes factors stripped of their Bayesian content. Using e-values
instead of p-values, which are standard in this area, leads to simple and
efficient procedures that control the number of false discoveries under
arbitrary dependence of the base e-values. We prove an optimality result for
our main procedure and demonstrate advantages of our methods over standard
methods using simulated and real-world datasets.
"
"stat.TH","  This paper presents compact notations for concentration inequalities and
convenient results to streamline probabilistic analysis. The new expressions
describe the typical sizes and tails of random variables, allowing for simple
operations without heavy use of inessential constants. They bridge classical
asymptotic notations and modern non-asymptotic tail bounds together. Examples
of different kinds demonstrate their efficacy.
"
"stat.TH","  For sequentially observed functional data exhibiting multiple change points
in the mean function, we establish consistency results for the estimated number
and locations of the change points based on the norm of the functional CUSUM
process and standard binary segmentation. In addition to extending similar
results in Venkatraman (1992) and Fryzlewicz (2014) for scalar data to the
general Hilbert space setting, our main results are established without
assuming the Gaussianity of the data, and under general linear process
conditions on the model errors.
"
"stat.TH","  Kernel ridge regression is an important nonparametric method for estimating
smooth functions. We introduce a new set of conditions, under which the actual
rates of convergence of the kernel ridge regression estimator under both the
L_2 norm and the norm of the reproducing kernel Hilbert space exceed the
standard minimax rates. An application of this theory leads to a new
understanding of the Kennedy-O'Hagan approach for calibrating model parameters
of computer simulation. We prove that, under certain conditions, the
Kennedy-O'Hagan calibration estimator with a known covariance function
converges to the minimizer of the norm of the residual function in the
reproducing kernel Hilbert space.
"
"stat.TH","  Topological Data Analysis (TDA) has become a promising tool to uncover
low-dimensional features from high-dimensional data. Notwithstanding the
advantages afforded by TDA, its adoption in statistical methodology has been
limited by several reasons. In this paper we study the framework of topological
inference through the lens of classical parametric inference in statistics.
Suppose $\mathcal{P} = \{\mathbb{P}_\theta : \theta \in \Theta\}$ is a
parametric family of distributions indexed by a set $\Theta$, and $\mathbb{X}_n
= \{\mathbf{X}_1, \mathbf{X}_2, \dots, \mathbf{X}_n\}$ is observed iid at
random from a distribution $\mathbb{P}_\theta$. The asymptotic behaviour of the
Betti numbers associated with the \v{C}ech complex of $\mathbb{X}_n$ contain
both the topological and parametric information about the distribution of
points. We investigate necessary and sufficient conditions under which
topological inference is possible in this parametric setup.
"
"stat.TH","  The goal of this study was to test the equality of two covariance matrices by
using modified Pillai's trace statistics under a high-dimensional framework,
i.e., the dimension and sample sizes go to infinity proportionally. In this
paper, we introduce two modified Pillai's trace statistics and obtain their
asymptotic distributions under the null hypothesis. The benefits of the
proposed statistics include the following: (1) the sample size can be smaller
than the dimensions; (2) the limiting distributions of the proposed statistics
are universal; and (3) we do not restrict the structure of the population
covariance matrices. The theoretical results are established under mild and
practical assumptions, and their properties are demonstrated numerically by
simulations and a real data analysis.
"
"stat.TH","  We develop an estimator for the high-dimensional covariance matrix of a
locally stationary process with a smoothly varying trend and use this statistic
to derive consistent predictors in non-stationary time series. In contrast to
the currently available methods for this problem the predictor developed here
does not rely on fitting an autoregressive model and does not require a
vanishing trend. The finite sample properties of the new methodology are
illustrated by means of a simulation study and a financial indices study.
"
"stat.TH","  We present some new results on the joint distribution of an arbitrary subset
of the ordered eigenvalues of complex Wishart, double Wishart, and Gaussian
hermitian random matrices of finite dimensions, using a tensor
pseudo-determinant operator. Specifically, we derive compact expressions for
the joint probability distribution function of the eigenvalues and the
expectation of functions of the eigenvalues, including joint moments, for the
case of both ordered and unordered eigenvalues.
"
"stat.TH","  While spectral embedding is a widely applied dimension reduction technique in
various fields, so far it is still challenging to make it scalable and robust
to handle ""big data"". Motivated by the need of handling such data, we propose a
novel spectral embedding algorithm, which we coined Robust and Scalable
Embedding via Landmark Diffusion (ROSELAND). In short, we measure the affinity
between two points via a set of landmarks, which is composed of a small number
of points, and ""diffuse"" on the dataset via the landmark set to achieve a
spectral embedding. The embedding is not only scalable and robust, but also
preserves the geometric properties under the manifold setup. The Roseland can
be viewed as a generalization of the commonly applied spectral embedding
algorithm, the diffusion map (DM), in the sense that it shares various
properties of the DM. In addition to providing a theoretical justification of
the Roseland under the manifold setup, including handling the U-statistics-like
quantities, providing a $L^\infty$ spectral convergence with a rate, and
offering a high dimensional noise analysis, we show various numerical
simulations and compare the Roseland with other existing algorithms.
"
"stat.TH","  The divergence minimization problem plays an important role in various
fields. In this note, we focus on differentiable and strictly convex
divergences. For some minimization problems, we show the minimizer conditions
and the uniqueness of the minimizer without assuming a specific form of
divergences. Furthermore, we show geometric properties related to the
minimization problems.
"
"stat.TH","  The permutation test is a versatile type of exact nonparametric significance
test that requires drastically fewer assumptions than similar parametric tests
by considering the distribution of a test statistic over a discrete group of
distributionally invariant transformations. The main downfall of the
permutation test is the high computational cost of running such a test making
this approach laborious for complex data and experimental designs and
completely infeasible in any application requiring speedy results. We rectify
this problem through application of Kahane--Khintchine-type inequalities under
a weak dependence condition and thus propose a computation free permutation
test---i.e. a permutation-less permutation test. This general framework is
studied within both commutative and non-commutative Banach spaces. We further
improve these Kahane-Khintchine-type bounds via a transformation based on the
incomplete beta function and Talagrand's concentration inequality. For
$k$-sample testing, we extend the theory presented for Rademacher sums to
weakly dependent Rademacher chaoses making use of modified decoupling
inequalities. We test this methodology on classic functional data sets
including the Berkeley growth curves and the phoneme dataset. We also consider
hypothesis testing on speech samples under two experimental designs: the Latin
square and the complete randomized block design.
"
"stat.TH","  Although it is well-known that some exponential family random graph model
(ERGM) families exhibit phase transitions (in which small parameter changes
lead to qualitative changes in graph structure), the behavior of other models
is still poorly understood. Recently, Krivitsky and Morris have reported a
previously unobserved phase transition in the edge/concurrent vertex family (a
simple starting point for models of sexual contact networks). Here, we examine
this phase transition, showing it to be a first order transition with respect
to an order parameter associated with the fraction of concurrent vertices. This
transition stems from weak cooperativity in the recruitment of vertices to the
concurrent phase, which may not be a desirable property in some applications.
"
"stat.TH","  As high-dimensional and high-frequency data are being collected on a large
scale, the development of new statistical models is being pushed forward.
Functional data analysis provides the required statistical methods to deal with
large-scale and complex data by assuming that data are continuous functions,
e.g., a realization of a continuous process (curves) or continuous random
fields (surfaces), and that each curve or surface is considered as a single
observation. Here, we provide an overview of functional data analysis when data
are complex and spatially correlated. We provide definitions and estimators of
the first and second moments of the corresponding functional random variable.
We present two main approaches: The first assumes that data are realizations of
a functional random field, i.e., each observation is a curve with a spatial
component. We call them 'spatial functional data'. The second approach assumes
that data are continuous deterministic fields observed over time. In this case,
one observation is a surface or manifold, and we call them 'surface time
series'. For the two approaches, we describe software available for the
statistical analysis. We also present a data illustration, using a
high-resolution wind speed simulated dataset, as an example of the two
approaches. The functional data approach offers a new paradigm of data
analysis, where the continuous processes or random fields are considered as a
single entity. We consider this approach to be very valuable in the context of
big data.
"
"stat.TH","  We determine the information-theoretic cutoff value on separation of cluster
centers for exact recovery of cluster labels in a $K$-component Gaussian
mixture model with equal cluster sizes. Moreover, we show that a semidefinite
programming (SDP) relaxation of the $K$-means clustering method achieves such
sharp threshold for exact recovery without assuming the symmetry of cluster
centers.
"
"stat.TH","  This paper develops a Hoeffding inequality for the partial sums $\sum_{k=1}^n
f (X_k)$, where $\{X_k\}_{k \in \mathbb{Z}_{> 0}}$ is an irreducible Markov
chain on a finite state space $S$, and $f : S \to [a, b]$ is a real-valued
function. Our bound is simple, general, since it only assumes irreducibility
and finiteness of the state space, and powerful. In order to demonstrate its
usefulness we provide two applications in multi-armed bandit problems. The
first is about identifying an approximately best Markovian arm, while the
second is concerned with regret minimization in the context of Markovian
bandits.
"
"stat.TH","  We establish exponential inequalities for a class of V-statistics under
strong mixing conditions. Our theory is developed via a novel kernel expansion
based on random Fourier features and the use of a probabilistic method. This
type of expansion is new and useful for handling many notorious classes of
kernels.
"
"stat.TH","  Measurements are inseparable from inference, where the estimation of signals
of interest from other observations is called an indirect measurement. While a
variety of measurement limits have been defined by the physical constraint on
each setup, the fundamental limit of an indirect measurement is essentially the
limit of inference. Here, we propose the concept of statistical limits on
indirect measurement: the bounds of distinction between signals and noise and
between a signal and another signal. By developing the asymptotic theory of
Bayesian regression, we investigate the phenomenology of a typical indirect
measurement and demonstrate the existence of these limits. Based on the
connection between inference and statistical physics, we also provide a unified
interpretation in which these limits emerge from phase transitions of
inference. Our results could pave the way for novel experimental design,
enabling assess to the required quality of observations according to the
assumed ground truth before the concerned indirect measurement is actually
performed.
"
"stat.TH","  Stochastic differential equations and stochastic dynamics are good models to
describe stochastic phenomena in real world. In this paper, we study N
independent stochastic processes Xi(t) with real entries and the processes are
determined by the stochastic differential equations with drift term relying on
some random effects. We obtain the Girsanov-type formula of the stochastic
differential equation driven by Fractional Brownian Motion through kernel
transformation. Under some assumptions of the random effect, we estimate the
parameter estimators by the maximum likelihood estimation and give some
numerical simulations for the discrete observations. Results show that for the
different H, the parameter estimator is closer to the true value as the amount
of data increases.
"
"stat.TH","  We study first order methods to compute the barycenter of a probability
distribution $P$ over the space of probability measures with finite second
moment. We develop a framework to derive global rates of convergence for both
gradient descent and stochastic gradient descent despite the fact that the
barycenter functional is not geodesically convex. Our analysis overcomes this
technical hurdle by employing a Polyak-Lojasiewicz (PL) inequality and relies
on tools from optimal transport and metric geometry. In turn, we establish a PL
inequality when $P$ is supported on the Bures-Wasserstein manifold of Gaussian
probability measures. It leads to the first global rates of convergence for
first order methods in this context.
"
"stat.TH","  This paper considers the estimation and inference of the low-rank components
in high-dimensional matrix-variate factor models, where each dimension of the
matrix-variates ($p \times q$) is comparable to or greater than the number of
observations ($T$). We propose an estimation method called $\alpha$-PCA that
preserves the matrix structure and aggregates mean and contemporary covariance
through a hyper-parameter $\alpha$. We develop an inferential theory,
establishing consistency, the rate of convergence, and the limiting
distributions, under general conditions that allow for correlations across
time, rows, or columns of the noise. We show both theoretical and empirical
methods of choosing the best $\alpha$, depending on the use-case criteria.
Simulation results demonstrate the adequacy of the asymptotic results in
approximating the finite sample properties. The $\alpha$-PCA compares favorably
with the existing ones. Finally, we illustrate its applications with a real
numeric data set and two real image data sets. In all applications, the
proposed estimation procedure outperforms previous methods in the power of
variance explanation using out-of-sample 10-fold cross-validation.
"
"stat.TH","  In this paper we explore the behaviour of dependent test statistics for
testing of multiple hypothesis . To keep simplicity, we have considered a
mixture normal model with equicorrelated correlation set up. With a simple
linear transformation,the test statistics were decomposed into independent
components, which, when conditioned appropriately generated independent
variables. These were used to construct conditional tests , which were shown to
be asymptotically optimal with power as large as that obtained using N.P.Lemma.
We have pursued extensive simulation to support the claim.
"
"stat.TH","  Classical geostatistical methods face serious computational challenges if
they are confronted with large sets of spatially distributed data. We present a
simplified stochastic local interaction (SLI) model for computationally
efficient spatial prediction that can handle large data. The SLI method
constructs a spatial interaction matrix (precision matrix) that accounts for
the data values, their locations, and the sampling density variations without
user input. We show that this precision matrix is strictly positive definite.
The SLI approach does not require matrix inversion for parameter estimation,
spatial prediction, and uncertainty estimation, leading to computational
procedures that are significantly less intensive computationally than kriging.
The precision matrix involves compact kernel functions (spherical, quadratic,
etc.) which enable the application of sparse matrix methods, thus improving
computational time and memory requirements. We investigate the proposed SLI
method with a data set that includes approximately 11500 drill-hole data of
coal thickness from Campbell County (Wyoming, USA). We also compare SLI with
ordinary kriging (OK) in terms of estimation performance, using cross
validation analysis, and computational time. According to the validation
measures used, SLI performs slightly better in estimating seam thickness than
OK in terms of cross-validation measures. In terms of computation time, SLI
prediction is 3 to 25 times (depending on the size of the kriging neighborhood)
faster than OK for the same grid size.
"
"stat.TH","  The paper is concerned with non-linear Gaussian filtering and smoothing in
continuous-discrete state-space models, where the dynamic model is formulated
as an It\^{o} stochastic differential equation (SDE), and the measurements are
obtained at discrete time instants. We propose novel Taylor moment expansion
(TME) Gaussian filter and smoother which approximate the moments of the SDE
with a temporal Taylor expansion. Differently from classical linearisation or
It\^{o}--Taylor approaches, the Taylor expansion is formed for the moment
functions directly and in time variable, not by using a Taylor expansion on the
non-linear functions in the model. We analyse the theoretical properties,
including the positive definiteness of the covariance estimate and stability of
the TME Gaussian filter and smoother. By numerical experiments, we demonstrate
that the proposed TME Gaussian filter and smoother significantly outperform the
state-of-the-art methods in terms of estimation accuracy and numerical
stability.
"
"stat.TH","  The aim of this paper is the development of consistent tests for the
comparison of the distributions of two possibly dependent portfolios. The tests
can be used to check whether the two portfolios are risk equivalent. The
related testing problem can be endowed into a more general paired data
framework by testing marginal homogeneity of bivariate functional data, or even
paired random variables taking values in a general Hilbert space. To address
this problem, we apply a Cram\'er-von-Mises type test statistic and suggest a
bootstrap as well as permutation procedure to obtain critical values. The
usually desired properties of a bootstrap and permutation test can be derived,
that are asymptotic exactness under the null hypothesis and consistency under
alternatives. Simulations demonstrate the quality of the tests in the finite
sample case and confirm the theoretical findings. Finally, we illustrate the
application of the approach by comparing real financial time series.
"
"stat.TH","  Discrete time trawl processes constitute a large class of time series
parameterized by a trawl sequence (a j) j$\in$N and defined though a sequence
of independent and identically distributed (i.i.d.) copies of a continuous time
process ($\gamma$(t)) t$\in$R called the seed process. They provide a general
framework for modeling linear or non-linear long range dependent time series.
We investigate the spectral estimation, either pointwise or broadband, of long
range dependent discrete-time trawl processes. The difficulty arising from the
variety of seed processes and of trawl sequences is twofold. First, the
spectral density may take different forms, often including smooth additive
correction terms. Second, trawl processes with similar spectral densities may
exhibit very different statistical behaviors. We prove the consistency of our
estimators under very general conditions and we show that a wide class of trawl
processes satisfy them. This is done in particular by introducing a weighted
weak dependence index that can be of independent interest. The broadband
spectral estimator includes an estimator of the long memory parameter. We
complete this work with numerical experiments to evaluate the finite sample
size performance of this estimator for various integer valued discrete time
trawl processes.
"
"stat.TH","  When there is interference, a subject's outcome depends on the treatment of
others and treatment effects may take on several different forms. This
situation arises often, particularly in vaccine evaluation. In settings where
interference is likely, two-stage cluster randomized trials have been suggested
as a means of estimating some of the causal contrast of interest. Working in
the finite population setting to investigate rare and unplanned subgroup
analyses using some of the estimators that have been suggested in the
literature, include Horvitz-Thompson, Hajek, and what might be called the
natural extension of the marginal estimators suggested in Hudgens and Halloran
2008. I define the estimands of interest conditional on individual, group and
both individual and group baseline variables, giving unbiased Horvitz-Thompson
style estimates for each. I also provide variance estimators for several
estimators. I show that the Horvitz-Thompson (HT) type estimators are always
unbiased provided at least one subject within the group or population, whatever
the level of interest for the estimator, is in the subgroup of interest. This
is not true of the ""natural"" or the Hajek style estimators, which will often be
undefined for rare subgroups.
"
"stat.TH","  We consider the problem of conditional independence testing of $X$ and $Y$
given $Z$ where $X,Y$ and $Z$ are three real random variables and $Z$ is
continuous. We focus on two main cases - when $X$ and $Y$ are both discrete,
and when $X$ and $Y$ are both continuous. In view of recent results on
conditional independence testing (Shah and Peters, 2018), one cannot hope to
design non-trivial tests, which control the type I error for all absolutely
continuous conditionally independent distributions, while still ensuring power
against interesting alternatives. Consequently, we identify various, natural
smoothness assumptions on the conditional distributions of $X,Y|Z=z$ as $z$
varies in the support of $Z$, and study the hardness of conditional
independence testing under these smoothness assumptions. We derive matching
lower and upper bounds on the critical radius of separation between the null
and alternative hypotheses in the total variation metric. The tests we consider
are easily implementable and rely on binning the support of the continuous
variable $Z$. To complement these results, we provide a new proof of the
hardness result of Shah and Peters.
"
"stat.TH","  Generalizing an idea of Davie and Gaines (2001), we present a method for the
simulation of fully discrete samples of the solution to the stochastic heat
equation on an interval. We provide a condition for the validity of the
approximation, which holds particularly when the number of temporal and spatial
observations tends to infinity. Hereby, the quality of the approximation is
measured in total variation distance. In a simulation study we calculate
temporal and spatial quadratic variations from sample paths generated both via
our method and via naive truncation of the Fourier series representation of the
process. Hereby, the results provided by our method are more accurate at a
considerably lower computational cost.
"
"stat.TH","  A number of discrete time, finite population size models in genetics
describing the dynamics of allele frequencies are known to converge (subject to
suitable scaling) to a diffusion process in the infinite population limit,
termed the Wright-Fisher diffusion. In this article we show that the diffusion
is ergodic uniformly in the selection and mutation parameters, and that the
measures induced by the solution to the stochastic differential equation are
uniformly locally asymptotically normal. Subsequently these two results are
used to analyse the statistical properties of the Maximum Likelihood and
Bayesian estimators for the selection parameter, when both selection and
mutation are acting on the population. In particular, it is shown that these
estimators are uniformly over compact sets consistent, display uniform in the
selection parameter asymptotic normality and convergence of moments over
compact sets, and are asymptotically efficient for a suitable class of loss
functions.
"
"stat.TH","  We study the problem of predicting the properties of a probabilistic model
and the next outcome of the model sequentially in the infinite horizon, so that
the perdition will make finitely many errors with probability 1. We introduce a
general framework that models such predication problems. We prove some general
properties of the framework, and show some concrete examples with the
application of such properties.
"
"stat.TH","  We develop a novel, general framework for the asymptotic reduction of the
bias of $M$-estimators from unbiased estimating functions. The framework relies
on additive, empirical adjustments to the estimating functions that depend only
on the first two derivatives of the contributions to the estimating functions.
The new estimation method has markedly broader applicability than previous
bias-reduction methods by applying to models that are either
partially-specified or that have a likelihood that is intractable or expensive
to compute, and a surrogate objective is employed. The method also offers
itself to easy, general implementations for arbitrary models by using automatic
differentiation. This is in contrast to other popular bias-reduction methods
that require either resampling or evaluation of expectations of products of
log-likelihood derivatives. If $M$-estimation is by the maximization of an
objective function, then, reduced-bias $M$-estimation can be achieved by
maximizing an appropriately penalized objective. That penalized objective
relates closely to information criteria based on the Kullback-Leibler
divergence, establishing, for the first time, a strong link between reduction
of estimation bias and model selection. The reduced-bias $M$-estimators are
found to have the same asymptotic distribution, and, hence, the same asymptotic
efficiency properties as the original $M$-estimators, and we discuss inference
and model selection with reduced-bias $M$-estimates. The properties of
reduced-bias $M$-estimation are illustrated in well-used, important modelling
settings of varying complexity.
"
"stat.TH","  Support size estimation and the related problem of unseen species estimation
have wide applications in ecology and database analysis. Perhaps the most used
support size estimator is the Chao estimator. Despite its wide spread use,
little is known about its theoretical properties. We analyze the Chao estimator
and show that its worst case mean squared error (MSE) is smaller than the MSE
of the plug-in estimator by a factor of $\mathcal{O} ((k/n)^4)$, where $k$ is
the maximum support size and $n$ is the number of samples. Our main technical
contribution is a new method to analyze rational estimators for discrete
distribution properties, which may be of independent interest.
"
"stat.TH","  Tree ensemble methods such as random forests [Breiman, 2001] are very popular
to handle high-dimensional tabular data sets, notably because of their good
predictive accuracy. However, when machine learning is used for decision-making
problems, settling for the best predictive procedures may not be reasonable
since enlightened decisions require an in-depth comprehension of the algorithm
prediction process. Unfortunately, random forests are not intrinsically
interpretable since their prediction results from averaging several hundreds of
decision trees. A classic approach to gain knowledge on this so-called
black-box algorithm is to compute variable importances, that are employed to
assess the predictive impact of each input variable. Variable importances are
then used to rank or select variables and thus play a great role in data
analysis. Nevertheless, there is no justification to use random forest variable
importances in such way: we do not even know what these quantities estimate. In
this paper, we analyze one of the two well-known random forest variable
importances, the Mean Decrease Impurity (MDI). We prove that if input variables
are independent and in absence of interactions, MDI provides a variance
decomposition of the output, where the contribution of each variable is clearly
identified. We also study models exhibiting dependence between input variables
or interaction, for which the variable importance is intrinsically ill-defined.
Our analysis shows that there may exist some benefits to use a forest compared
to a single tree.
"
"stat.TH","  We consider the classical sequential binary hypothesis testing problem in
which there are two hypotheses governed respectively by distributions $P_0$ and
$P_1$ and we would like to decide which hypothesis is true using a sequential
test. It is known from the work of Wald and Wolfowitz that as the expectation
of the length of the test grows, the optimal type-I and type-II error exponents
approach the relative entropies $D(P_1\|P_0)$ and $D(P_0\|P_1)$. We refine this
result by considering the optimal backoff---or second-order asymptotics---from
the corner point of the achievable exponent region $(D(P_1\|P_0),D(P_0\|P_1))$
under two different constraints on the length of the test (or the sample size).
First, we consider a probabilistic constraint in which the probability that the
length of test exceeds a prescribed integer $n$ is less than a certain
threshold $0<\varepsilon <1$. Second, the expectation of the sample size is
bounded by $n$. In both cases, and under mild conditions, the second-order
asymptotics is characterized exactly. Numerical examples are provided to
illustrate our results.
"
"stat.TH","  This paper aims to address two fundamental challenges arising in eigenvector
estimation and inference for a low-rank matrix from noisy observations: (1) how
to estimate an unknown eigenvector when the eigen-gap (i.e. the spacing between
the associated eigenvalue and the rest of the spectrum) is particularly small;
(2) how to perform estimation and inference on linear functionals of an
eigenvector -- a sort of ""fine-grained"" statistical reasoning that goes far
beyond the usual $\ell_2$ analysis. We investigate how to address these
challenges in a setting where the unknown $n\times n$ matrix is symmetric and
the additive noise matrix contains independent (and non-symmetric) entries.
Based on eigen-decomposition of the asymmetric data matrix, we propose
estimation and uncertainty quantification procedures for an unknown
eigenvector, which further allow us to reason about linear functionals of an
unknown eigenvector. The proposed procedures and the accompanying theory enjoy
several important features: (1) distribution-free (i.e. prior knowledge about
the noise distributions is not needed); (2) adaptive to heteroscedastic noise;
(3) minimax optimal under Gaussian noise. Along the way, we establish optimal
procedures to construct confidence intervals for the unknown eigenvalues. All
this is guaranteed even in the presence of a small eigen-gap (up to
$O(\sqrt{n/\mathrm{poly}\log (n)})$ times smaller than the requirement in prior
theory), which goes significantly beyond what generic matrix perturbation
theory has to offer.
"
"stat.TH","  We study the geometry of probability distributions with respect to a
generalized family of Csisz\'ar $f$-divergences. A member of this family is the
relative $\alpha$-entropy which is also a R\'enyi analog of relative entropy in
information theory and known as logarithmic or projective power divergence in
statistics. We apply Eguchi's theory to derive the Fisher information metric
and the dual affine connections arising from these generalized divergence
functions. This enables us to arrive at a more widely applicable version of the
Cram\'{e}r-Rao inequality, which provides a lower bound for the variance of an
estimator for an escort of the underlying parametric probability distribution.
We then extend the Amari-Nagaoka's dually flat structure of the exponential and
mixer models to other distributions with respect to the aforementioned
generalized metric. We show that these formulations lead us to find unbiased
and efficient estimators for the escort model. Finally, we compare our work
with prior results on generalized Cram\'er-Rao inequalities that were derived
from non-information-geometric frameworks.
"
"stat.TH","  We focus on the problem of manifold estimation: given a set of observations
sampled close to some unknown submanifold $M$, one wants to recover information
about the geometry of $M$. Minimax estimators which have been proposed so far
all depend crucially on the a priori knowledge of some parameters quantifying
the regularity of $M$ (such as its reach), whereas those quantities will be
unknown in practice. Our contribution to the matter is twofold: first, we
introduce a one-parameter family of manifold estimators $(\hat{M}_t)_{t\geq
0}$, and show that for some choice of $t$ (depending on the regularity
parameters), the corresponding estimator is minimax on the class of models of
$C^2$ manifolds introduced in [Genovese et al., Manifold estimation and
singular deconvolution under Hausdorff loss]. Second, we propose a completely
data-driven selection procedure for the parameter $t$, leading to a minimax
adaptive manifold estimator on this class of models. This selection procedure
actually allows to recover the sample rate of the set of observations, and can
therefore be used as an hyperparameter in other settings, such as tangent space
estimation.
"
"stat.TH","  We consider a $p$-dimensional time series where the dimension $p$ increases
with the sample size $n$. The resulting data matrix $X$ follows a stochastic
volatility model: each entry consists of a positive random volatility term
multiplied by an independent noise term. The volatility multipliers introduce
dependence in each row and across the rows. We study the asymptotic behavior of
the eigenvalues and eigenvectors of the sample covariance matrix $XX'$ under a
regular variation assumption on the noise. In particular, we prove Poisson
convergence for the point process of the centered and normalized eigenvalues
and derive limit theory for functionals acting on them, such as the trace. We
prove related results for stochastic volatility models with additional linear
dependence structure and for stochastic volatility models where the
time-varying volatility terms are extinguished with high probability when $n$
increases. We provide explicit approximations of the eigenvectors which are of
a strikingly simple structure. The main tools for proving these results are
large deviation theorems for heavy-tailed time series, advocating a unified
approach to the study of the eigenstructure of heavy-tailed random matrices.
"
"stat.TH","  We provide asymptotic theory for certain functions of the sample
autocovariance matrices of a high-dimensional time series with infinite fourth
moment. The time series exhibits linear dependence across the coordinates and
through time. Assuming that the dimension increases with the sample size, we
provide theory for the eigenvectors of the sample autocovariance matrices and
find explicit approximations of a simple structure, whose finite sample quality
is illustrated for simulated data. We also obtain the limits of the normalized
eigenvalues of functions of the sample autocovariance matrices in terms of
cluster Poisson point processes. In turn, we derive the distributional limits
of the largest eigenvalues and functionals acting on them. In our proofs, we
use large deviation techniques for heavy-tailed processes, point process
techniques motivated by extreme value theory, and related continuous mapping
arguments.
"
"stat.TH","  Spiking activity in cortical networks is nonlinear in nature. The
linear-nonlinear cascade model, some versions of which are also known as
point-process generalized linear model, can efficiently capture the nonlinear
dynamics exhibited by such networks. Of particular interest in such models are
theoretical predictions of spike train statistics. However, due to the
moment-closure problem, approximations are inevitable. We suggest here a series
expansion that explains how higher-order moments couple to lower-order ones.
Our approach makes predictions in terms of certain integrals, the so-called
loop integrals. In previous studies these integrals have been evaluated
numerically, but numerical instabilities are sometimes encountered rendering
the results unreliable. Analytic solutions are presented here to overcome this
problem, and to arrive at more robust evaluations. We were able to deduce these
analytic solutions by switching to Fourier space and making use of complex
analysis, specifically Cauchy's residue theorem. We formalized the loop
integrals and explicitly solved them for specific response functions. To
quantify the importance of these corrections for spike train cumulants, we
numerically simulated spiking networks and compared their sample statistics to
our theoretical predictions. Our results demonstrate that the magnitude of the
nonlinear corrections depends on the working point of the nonlinear network
dynamics, and that it is related to the eigenvalues of the mean-field stability
matrix. For our example, the corrections for the firing rates are in the range
between 4 % and 21 % on average. Precise and robust predictions of spike train
statistics accounting for nonlinear effects are, for example, highly relevant
for theories involving spike-timing dependent plasticity (STDP).
"
"stat.TH","  The $K$ sample problem for high-dimensional vector time series is studied,
especially focusing on sensor data streams, in order to analyze the second
moment structure and detect changes across samples and/or across variables
cumulated sum (CUSUM) statistics of bilinear forms of the sample covariance
matrix. In this model $K$ independent vector time series
$\mathbf{Y}_{T,1},\dots,\mathbf{Y}_{T,K}$ are observed over a time span $ [0,T]
$, which may correspond to $K$ sensors (locations) yielding $d$-dimensional
data as well as $K$ locations where $d$ sensors emit univariate data. Unequal
sample sizes are considered as arising when the sampling rate of the sensors
differs. We provide large sample approximations and two related change-point
statistics, a sums of squares and a pooled variance statistic. The resulting
procedures are investigated by simulations and illustrated by analyzing a real
data set.
"
"stat.TH","  We consider least squares estimators of the finite dimensional regression
parameter $\alpha$ in the single index regression model
$Y=\psi(\alpha^TX)+\epsilon$, where $X$ is a $d$-dimensional random vector,
$E(Y|X)=\psi(\alpha^TX)$, and where $\psi$ is monotone. It has been suggested
to estimate $\alpha$ by a profile least squares estimator, minimizing
$\sum_{i=1}^n(Y_i-\psi(\alpha^TX_i))^2$ over monotone $\psi$ and $\alpha$ on
the boundary $S_{d-1}$of the unit ball. Although this suggestion has been
around for a long time, it is still unknown whether the estimate is $\sqrt{n}$
convergent. We show that a profile least squares estimator, using the same
pointwise least squares estimator for fixed $\alpha$, but using a different
global sum of squares, is $\sqrt{n}$-convergent and asymptotically normal. We
show that a profile least squares estimator, using the same pointwise least
squares estimator for fixed $\bma$, but using a different global sum of
squares, is $\sqrt{n}$-convergent and asymptotically normal. The difference
between the corresponding loss functions is studied and also a comparison with
other methods is given.
"
"stat.TH","  This paper delivers improved theoretical guarantees for the convex
programming approach in low-rank matrix estimation, in the presence of (1)
random noise, (2) gross sparse outliers, and (3) missing data. This problem,
often dubbed as robust principal component analysis (robust PCA), finds
applications in various domains. Despite the wide applicability of convex
relaxation, the available statistical support (particularly the stability
analysis vis-a-vis random noise) remains highly suboptimal, which we strengthen
in this paper. When the unknown matrix is well-conditioned, incoherent, and of
constant rank, we demonstrate that a principled convex program achieves
near-optimal statistical accuracy, in terms of both the Euclidean loss and the
$\ell_{\infty}$ loss. All of this happens even when nearly a constant fraction
of observations are corrupted by outliers with arbitrary magnitudes. The key
analysis idea lies in bridging the convex program in use and an auxiliary
nonconvex optimization algorithm, and hence the title of this paper.
"
"stat.TH","  We study the problem of independence testing given independent and
identically distributed pairs taking values in a $\sigma$-finite, separable
measure space. Defining a natural measure of dependence $D(f)$ as the squared
$L^2$-distance between a joint density $f$ and the product of its marginals, we
first show that there is no valid test of independence that is uniformly
consistent against alternatives of the form $\{f: D(f) \geq \rho^2 \}$. We
therefore restrict attention to alternatives that impose additional
Sobolev-type smoothness constraints, and define a permutation test based on a
basis expansion and a $U$-statistic estimator of $D(f)$ that we prove is
minimax optimal in terms of its separation rates in many instances. Finally,
for the case of a Fourier basis on $[0,1]^2$, we provide an approximation to
the power function that offers several additional insights. Our methodology is
implemented in the R package USP.
"
"stat.TH","  We study the statistical decision process of detecting the presence of signal
from a 'signal+noise' type matrix model with an additive Wigner noise. We
derive the error of the likelihood ratio test, which minimizes the sum of the
Type-I and Type-II errors, under the Gaussian noise for the signal matrix with
arbitrary finite rank. We propose a hypothesis test based on the linear
spectral statistics of the data matrix, which is optimal and does not depend on
the distribution of the signal or the noise. We also introduce a test for rank
estimation that does not require the prior information on the rank of the
signal.
"
"stat.TH","  Tests for proportional hazards assumption concerning specified covariates or
groups of covariates are proposed. The class of alternatives is wide:
log-hazard rates under different values of covariates may cross, approach, go
away. The data may be right censored. The limit distribution of the test
statistic is derived. Power of the test against approaching alternatives is
given. Real data examples are considered.
"
"stat.TH","  We introduce the use of the Zig-Zag sampler to the problem of sampling
conditional diffusion processes (diffusion bridges). The Zig-Zag sampler is a
rejection-free sampling scheme based on a non-reversible continuous piecewise
deterministic Markov process. Similar to the L\'evy-Ciesielski construction of
a Brownian motion, we expand the diffusion path in a truncated Faber-Schauder
basis. The coefficients within the basis are sampled using a Zig-Zag sampler. A
key innovation is the use of the fully local Algorithm for the Zig-Zag sampler
that allows to exploit the sparsity structure implied by the dependency graph
of the coefficients and by the subsampling technique to reduce the complexity
of the algorithm. We illustrate the performance of the proposed methods in a
number of examples.
"
"stat.TH","  Understanding the pathways whereby an intervention has an effect on an
outcome is a common scientific goal. A rich body of literature provides various
decompositions of the total intervention effect into pathway specific effects.
Interventional direct and indirect effects provide one such decomposition.
Existing estimators of these effects are based on parametric models with
confidence interval estimation facilitated via the nonparametric bootstrap. We
provide theory that allows for more flexible, possibly machine learning-based,
estimation techniques to be considered. In particular, we establish weak
convergence results that facilitate the construction of closed-form confidence
intervals and hypothesis tests. Finally, we demonstrate multiple robustness
properties of the proposed estimators. Simulations show that inference based on
large-sample theory has adequate small-sample performance. Our work thus
provides a means of leveraging modern statistical learning techniques in
estimation of interventional mediation effects.
"
"stat.TH","  There is a large body of work on convergence rates either in passive or
active learning. Here we first outline some of the main results that have been
obtained, more specifically in a nonparametric setting under assumptions about
the smoothness of the regression function (or the boundary between classes) and
the margin noise. We discuss the relative merits of these underlying
assumptions by putting active learning in perspective with recent work on
passive learning. We design an active learning algorithm with a rate of
convergence better than in passive learning, using a particular smoothness
assumption customized for k-nearest neighbors. Unlike previous active learning
algorithms, we use a smoothness assumption that provides a dependence on the
marginal distribution of the instance space. Additionally, our algorithm avoids
the strong density assumption that supposes the existence of the density
function of the marginal distribution of the instance space and is therefore
more generally applicable.
"
"stat.TH","  Linear thresholding models postulate that the conditional distribution of a
response variable in terms of covariates differs on the two sides of a
(typically unknown) hyperplane in the covariate space. A key goal in such
models is to learn about this separating hyperplane. Exact likelihood or least
square methods to estimate the thresholding parameter involve an indicator
function which make them difficult to optimize and are, therefore, often
tackled by using a surrogate loss that uses a smooth approximation to the
indicator. In this note, we demonstrate that the resulting estimator is
asymptotically normal with a near optimal rate of convergence: $n^{-1}$ up to a
log factor, in a classification thresholding model. This is substantially
faster than the currently established convergence rates of smoothed estimators
for similar models in the statistics and econometrics literatures.
"
"stat.TH","  In time series analysis there is an apparent dichotomy between time and
frequency domain methods. The aim of this paper is to draw connections between
frequency and time domain methods. Our focus will be on reconciling the
Gaussian likelihood and the Whittle likelihood. We derive an exact,
interpretable, bound between the Gaussian and Whittle likelihood of a second
order stationary time series. The derivation is based on obtaining the
transformation which is biorthogonal to the discrete Fourier transform of the
time series. Such a transformation yields a new decomposition for the inverse
of a Toeplitz matrix and enables the representation of the Gaussian likelihood
within the frequency domain. We show that the difference between the Gaussian
and Whittle likelihood is due to the omission of the best linear predictions
outside the domain of observation in the periodogram associated with the
Whittle likelihood. Based on this result, we obtain an approximation for the
difference between the Gaussian and Whittle likelihoods in terms of the best
fitting, finite order autoregressive parameters. These approximations are used
to define two new frequency domain quasi-likelihoods criteria. We show that
these new criteria can yield a better approximation of the spectral divergence
criterion, as compared to both the Gaussian and Whittle likelihoods. In
simulations, we show that the proposed estimators have satisfactory finite
sample properties.
"
"stat.TH","  The Multinomial Logit (MNL) model and the axiom it satisfies, the
Independence of Irrelevant Alternatives (IIA), are together the most widely
used tools of discrete choice. The MNL model serves as the workhorse model for
a variety of fields, but is also widely criticized, with a large body of
experimental literature claiming to document real-world settings where IIA
fails to hold. Statistical tests of IIA as a modelling assumption have been the
subject of many practical tests focusing on specific deviations from IIA over
the past several decades, but the formal size properties of hypothesis testing
IIA are still not well understood. In this work we replace some of the
ambiguity in this literature with rigorous pessimism, demonstrating that any
general test for IIA with low worst-case error would require a number of
samples exponential in the number of alternatives of the choice problem. A
major benefit of our analysis over previous work is that it lies entirely in
the finite-sample domain, a feature crucial to understanding the behavior of
tests in the common data-poor settings of discrete choice. Our lower bounds are
structure-dependent, and as a potential cause for optimism, we find that if one
restricts the test of IIA to violations that can occur in a specific collection
of choice sets (e.g., pairs), one obtains structure-dependent lower bounds that
are much less pessimistic. Our analysis of this testing problem is unorthodox
in being highly combinatorial, counting Eulerian orientations of cycle
decompositions of a particular bipartite graph constructed from a data set of
choices. By identifying fundamental relationships between the comparison
structure of a given testing problem and its sample efficiency, we hope these
relationships will help lay the groundwork for a rigorous rethinking of the IIA
testing problem as well as other testing problems in discrete choice.
"
"stat.TH","  We consider the problem of constructing pointwise confidence intervals in the
multiple isotonic regression model. Recently, [HZ19] obtained a pointwise limit
distribution theory for the so-called block max-min and min-max estimators
[FLN17] in this model, but inference remains a difficult problem due to the
nuisance parameter in the limit distribution that involves multiple unknown
partial derivatives of the true regression function.
  In this paper, we show that this difficult nuisance parameter can be
effectively eliminated by taking advantage of information beyond point
estimates in the block max-min and min-max estimators. Formally, let
$\hat{u}(x_0)$ (resp. $\hat{v}(x_0)$) be the maximizing lower-left (resp.
minimizing upper-right) vertex in the block max-min (resp. min-max) estimator,
and $\hat{f}_n$ be the average of the block max-min and min-max estimators. If
all (first-order) partial derivatives of $f_0$ are non-vanishing at $x_0$, then
the following pivotal limit distribution theory holds: $$
\sqrt{n_{\hat{u},\hat{v}}(x_0)}\big(\hat{f}_n(x_0)-f_0(x_0)\big)\rightsquigarrow
\sigma\cdot \mathbb{L}_{1_d}. $$ Here $n_{\hat{u},\hat{v}}(x_0)$ is the number
of design points in the block $[\hat{u}(x_0),\hat{v}(x_0)]$, $\sigma$ is the
standard deviation of the errors, and $\mathbb{L}_{1_d}$ is a universal limit
distribution free of nuisance parameters. This immediately yields confidence
intervals for $f_0(x_0)$ with asymptotically exact confidence level and oracle
length. Notably, the construction of the confidence intervals, even new in the
univariate setting, requires no more efforts than performing an isotonic
regression for once using the block max-min and min-max estimators, and can be
easily adapted to other common monotone models. Extensive simulations are
carried out to support our theory.
"
"stat.TH","  The $\ell_0$-constrained empirical risk minimization ($\ell_0$-ERM) is a
promising tool for high-dimensional statistical estimation. The existing
analysis of $\ell_0$-ERM estimator is mostly on parameter estimation and
support recovery consistency. From the perspective of statistical learning,
another fundamental question is how well the $\ell_0$-ERM estimator would
perform on unseen samples. The answer to this question is important for
understanding the learnability of such a non-convex (and also NP-hard)
M-estimator but still relatively under explored.
  In this paper, we investigate this problem and develop a generalization
theory for $\ell_0$-ERM. We establish, in both white-box and black-box
statistical regimes, a set of generalization gap and excess risk bounds for
$\ell_0$-ERM to characterize its sparse prediction and optimization capability.
Our theory mainly reveals three findings: 1) tighter generalization bounds can
be attained by $\ell_0$-ERM than those of $\ell_2$-ERM if the risk function is
(with high probability) restricted strongly convex; 2) tighter uniform
generalization bounds can be established for $\ell_0$-ERM than the conventional
dense ERM; and 3) sparsity level invariant bounds can be established by
imposing additional strong-signal conditions to ensure the stability of
$\ell_0$-ERM. In light of these results, we further provide generalization
guarantees for the Iterative Hard Thresholding (IHT) algorithm which serves as
one of the most popular greedy pursuit methods for approximately solving
$\ell_0$-ERM. Numerical evidence is provided to confirm our theoretical
predictions when implied to sparsity-constrained linear regression and logistic
regression models.
"
"stat.TH","  In this paper we propose a class of weighted rank correlation coefficients
extending the Spearman's rho. The proposed class constructed by giving suitable
weights to the distance between two sets of ranks to place more emphasis on
items having low rankings than those have high rankings or vice versa. The
asymptotic distribution of the proposed measures and properties of the
parameters estimated by them are studied through the associated copula. A
simulation study is performed to compare the performance of the proposed
statistics for testing independence using asymptotic relative efficiency
calculations.
"
"stat.TH","  In this paper, we show that a suitably chosen covariance function of a
continuous time, second order stationary stochastic process can be viewed as a
symmetric higher order kernel. This leads to the construction of a higher order
kernel by choosing an appropriate covariance function. An optimal choice of the
constructed higher order kernel that partially minimizes the mean integrated
square error of the kernel density estimator is also discussed.
"
"stat.TH","  The problem of maximizing (or minimizing) the agreement between clusterings,
subject to given marginals, can be formally posed under a common framework for
several agreement measures. Until now, it was possible to find its solution
only through numerical algorithms. Here, an explicit solution is shown for the
case where the two clusterings have two clusters each.
"
"stat.TH","  We consider the solution X = (Xt) t$\ge$0 of a multivariate stochastic
differential equation with Levy-type jumps and with unique invariant
probability measure with density $\mu$. We assume that a continuous record of
observations X T = (Xt) 0$\le$t$\le$T is available. In the case without jumps,
Reiss and Dalalyan (2007) and Strauch (2018) have found convergence rates of
invariant density estimators, under respectively isotropic and anisotropic
H{\""o}lder smoothness constraints, which are considerably faster than those
known from standard multivariate density estimation. We extend the previous
works by obtaining, in presence of jumps, some estimators which have the same
convergence rates they had in the case without jumps for d $\ge$ 2 and a rate
which depends on the degree of the jumps in the one-dimensional setting. We
propose moreover a data driven bandwidth selection procedure based on the
Goldensh-luger and Lepski (2011) method which leads us to an adaptive
non-parametric kernel estimator of the stationary density $\mu$ of the jump
diffusion X. Adaptive bandwidth selection, anisotropic density estimation,
ergodic diffusion with jumps, L{\'e}vy driven SDE
"
"stat.TH","  The Bures-Wasserstein distance is a Riemannian distance on the space of
positive definite Hermitian matrices and is given by: $d(\Sigma,T) =
\left[\text{tr}(\Sigma) + \text{tr}(T) - 2 \text{tr}
\left(\Sigma^{1/2}T\Sigma^{1/2}\right)^{1/2}\right]^{1/2}$. This distance
function appears in the fields of optimal transport, quantum information, and
optimisation theory. In this paper, the geometrical properties of this distance
are studied using Riemannian submersions and quotient manifolds. The Riemannian
metric and geodesics are derived on both the whole space and the subspace of
trace-one matrices. In the first part of the paper a general framework is
provided, including different representations of the tangent bundle for the SLD
Fisher metric. The last part of the paper unifies up till now independent
arguments and results from quantum information theory and optimal transport.
The Bures-Wasserstein geometry is related to the Fubini-Study metric and the
Wigner-Yanase information.
"
"stat.TH","  We analyze the performance of the Tukey median estimator under total
variation (TV) distance corruptions. Previous results show that under Huber's
additive corruption model, the breakdown point is 1/3 for high-dimensional
halfspace-symmetric distributions. We show that under TV corruptions, the
breakdown point reduces to 1/4 for the same set of distributions. We also show
that a certain projection algorithm can attain the optimal breakdown point of
1/2. Both the Tukey median estimator and the projection algorithm achieve
sample complexity linear in dimension.
"
"stat.TH","  We consider the problem of assessing the importance of multiple variables or
factors from a dataset when side information is available. In principle, using
side information can allow the statistician to pay attention to variables with
a greater potential, which in turn, may lead to more discoveries. We introduce
an adaptive knockoff filter, which generalizes the knockoff procedure (Barber
and Cand\`es, 2015; Cand\`es et al., 2018) in that it uses both the data at
hand and side information to adaptively order the variables under study and
focus on those that are most promising. Adaptive knockoffs controls the
finite-sample false discovery rate (FDR) and we demonstrate its power by
comparing it with other structured multiple testing methods. We also apply our
methodology to real genetic data in order to find associations between genetic
variants and various phenotypes such as Crohn's disease and lipid levels. Here,
adaptive knockoffs makes more discoveries than reported in previous studies on
the same datasets.
"
"stat.TH","  Many functions of interest are in a high-dimensional space but exhibit
low-dimensional structures. This paper studies regression of a $s$-H\""{o}lder
function $f$ in $\mathbb{R}^D$ which varies along an active subspace of
dimension $d$ while $d\ll D$. A direct approximation of $f$ in $\mathbb{R}^D$
with an $\varepsilon$ accuracy requires the number of samples $n$ in the order
of $\varepsilon^{-(2s+D)/s}$. In this paper, we modify the Generalized Contour
Regression (GCR) algorithm to estimate the active subspace and use piecewise
polynomials for function approximation. GCR is among the best estimators for
the active subspace, but its sample complexity is an open question. Our
modified GCR improves the efficiency over the original GCR and leads to an mean
squared estimation error of $O(n^{-1})$ for the active subspace, when $n$ is
sufficiently large. The mean squared regression error of $f$ is proved to be in
the order of $\left(n/\log n\right)^{-\frac{2s}{2s+d}}$ where the exponent
depends on the dimension of the active subspace $d$ instead of the ambient
space $D$. This result demonstrates that GCR is effective in learning
low-dimensional active subspaces. The convergence rate is validated through
several numerical experiments.
"
"stat.TH","  We develop a new non-parametric test for testing normal distribution using
Stein's characterization. We study asymptotic properties of the test statistic.
We also develop jackknife empirical likelihood ratio test for testing
normality. Using Monte Carlo simulation study, we evaluate the finite sample
performance of the proposed JEL based test. Finally, we illustrate our test
procedure using two real data.
"
"stat.TH","  The reach of a submanifold is a crucial regularity parameter for manifold
learning and geometric inference from point clouds. This paper relates the
reach of a submanifold to its convexity defect function. Using the stability
properties of convexity defect functions, along with some new bounds and the
recent submanifold estimator of Aamari and Levrard [Ann. Statist. 47 177-204
(2019)], an estimator for the reach is given. A uniform expected loss bound
over a C^k model is found. Lower bounds for the minimax rate for estimating the
reach over these models are also provided. The estimator almost achieves these
rates in the C^3 and C^4 cases, with a gap given by a logarithmic factor.
"
"stat.TH","  Low-rank tensor approximations have shown great potential for uncertainty
quantification in high dimensions, for example, to build surrogate models that
can be used to speed up large-scale inference problems (Eigel et al., Inverse
Problems 34, 2018; Dolgov et al., Statistics & Computing 30, 2020). The
feasibility and efficiency of such approaches depends critically on the rank
that is necessary to represent or approximate the underlying distribution. In
this paper, a-priori rank bounds for approximations in the functional
tensor-train representation for the case of Gaussian models are developed. It
is shown that under suitable conditions on the precision matrix, the Gaussian
density can be approximated to high accuracy without suffering from an
exponential growth of complexity as the dimension increases. These results
provide a rigorous justification of the suitability and the limitations of
low-rank tensor methods in a simple but important model case. Numerical
experiments confirm that the rank bounds capture the qualitative behavior of
the rank structure when varying the parameters of the precision matrix and the
accuracy of the approximation. Finally, the practical relevance of the
theoretical results is demonstrated in the context of a Bayesian filtering
problem.
"
"stat.TH","  We develop new higher-order asymptotic techniques for the Gaussian maximum
likelihood estimator in a spatial panel data model, with fixed effects,
time-varying covariates, and spatially correlated errors. Our saddlepoint
density and tail area approximation feature relative error of order $O(m^{-1})$
for $m=n(T-1)$ with $n$ being the cross-sectional dimension and $T$ the
time-series dimension. The main theoretical tool is the tilted-Edgeworth
technique in a non-identically distributed setting. The density approximation
is always non-negative, does not need resampling, and is accurate in the tails.
We provide an algorithm and Monte Carlo experiments illustrating its good
performance over first-order asymptotics and Edgeworth expansions, while
preserving analytical tractability. An empirical application on the
investment-saving relationship in OECD countries shows disagreement between
testing results based on first-order asymptotics and saddlepoint techniques.
"
"stat.TH","  The discrepant posterior phenomenon (DPP) is a counterintuitive phenomenon
that occurs in the Bayesian analysis of multivariate parameters. It refers to
when an estimate of a marginal parameter obtained from the posterior is more
extreme than both of those obtained using either the prior or the likelihood
alone. Inferential claims that exhibit DPP defy intuition, and the phenomenon
can be surprisingly ubiquitous in well-behaved Bayesian models. Using point
estimation as an example, we derive conditions under which the DPP occurs in
Bayesian models with exponential quadratic likelihoods, including Gaussian
models and those with local asymptotic normality property, with conjugate
multivariate Gaussian priors. We also examine the DPP for the Binomial model,
in which the posterior mean is not a linear combination of that of the prior
and the likelihood. We provide an intuitive geometric interpretation of the
phenomenon and show that there exists a non-trivial space of marginal
directions such that the DPP occurs. We further relate the phenomenon to the
Simpson's paradox and discover their deep-rooted connection that is associated
with marginalization. We also draw connections with Bayesian computational
algorithms when difficult geometry exists. Theoretical results are complemented
by numerical illustrations. Scenarios covered in this study have implications
for parameterization, sensitivity analysis, and prior choice for Bayesian
modeling.
"
"stat.TH","  The Wald test remains ubiquitous in statistical practice despite shortcomings
such as its inaccuracy in small samples and lack of invariance under
reparameterization. This paper develops on another but lesser-known shortcoming
called the Hauck--Donner effect (HDE) whereby a Wald test statistic is not
monotonely increasing as a function of increasing distance between the
parameter estimate and the null value. Resulting in an upward biased $p$-value
and loss of power, the aberration can lead to very damaging consequences such
as in variable selection. The HDE afflicts many types of regression models and
corresponds to estimates near the boundary of the parameter space. This article
presents several new results, and its main contributions are to (i) propose a
very general test for detecting the HDE, regardless of its underlying cause;
(ii) fundamentally characterize the HDE by pairwise ratios of Wald and Rao
score and likelihood ratio test statistics for 1-parameter distributions; (iii)
show that the parameter space may be partitioned into an interior encased by 5
HDE severity measures (faint, weak, moderate, strong, extreme); (iv) prove that
a necessary condition for the HDE in a 2 by 2 table is a log odds ratio of at
least 2; (v) give some practical guidelines about HDE-free hypothesis testing.
Overall, practical post-fit tests can now be conducted potentially to any model
estimated by iteratively reweighted least squares, such as the generalized
linear model (GLM) and Vector GLM (VGLM) classes, the latter which encompasses
many popular regression models.
"
"stat.TH","  In Siotani & Fujikoshi (1984), a precise local limit theorem for the
multinomial distribution is derived by inverting the Fourier transform, where
the error terms are explicit up to order $N^{-1}$. In this paper, we give an
alternative (conceptually simpler) proof based on Stirling's formula and a
careful handling of Taylor expansions, and we show how the result can be used
to approximate multinomial probabilities on most subsets of $\mathbb{R}^d$.
Furthermore, we discuss a recent application of the result to obtain asymptotic
properties of Bernstein estimators on the simplex, we improve the main result
in Carter (2002) on the Le Cam distance bound between multinomial and
multivariate normal experiments while simultaneously simplifying the proof, and
we mention another potential application related to finely tuned continuity
corrections.
"
"stat.TH","  A classical result for the simple symmetric random walk with $2n$ steps is
that the number of steps above the origin, the time of the last visit to the
origin, and the time of the maximum height all have exactly the same
distribution and converge when scaled to the arcsine law. Motivated by
applications in genomics, we study the distributions of these statistics for
the non-Markovian random walk generated from the ascents and descents of a
uniform random permutation and a Mallows($q$) permutation and show that they
have the same asymptotic distributions as for the simple random walk. We also
give an unexpected conjecture, along with numerical evidence and a partial
proof in special cases, for the result that the number of steps above the
origin by step $2n$ for the uniform permutation generated walk has exactly the
same discrete arcsine distribution as for the simple random walk, even though
the other statistics for these walks have very different laws. We also give
explicit error bounds to the limit theorems using Stein's method for the
arcsine distribution, as well as functional central limit theorems and a strong
embedding of the Mallows$(q)$ permutation which is of independent interest.
"
"stat.TH","  We study distributed estimation of a Gaussian mean under communication
constraints in a decision theoretical framework. Minimax rates of convergence,
which characterize the tradeoff between the communication costs and statistical
accuracy, are established in both the univariate and multivariate settings.
Communication-efficient and statistically optimal procedures are developed. In
the univariate case, the optimal rate depends only on the total communication
budget, so long as each local machine has at least one bit. However, in the
multivariate case, the minimax rate depends on the specific allocations of the
communication budgets among the local machines.
  Although optimal estimation of a Gaussian mean is relatively simple in the
conventional setting, it is quite involved under the communication constraints,
both in terms of the optimal procedure design and lower bound argument. The
techniques developed in this paper can be of independent interest. An essential
step is the decomposition of the minimax estimation problem into two stages,
localization and refinement. This critical decomposition provides a framework
for both the lower bound analysis and optimal procedure design.
"
"stat.TH","  In this paper we derive locally D-optimal designs for discrete choice
experiments based on multinomial probit models. These models include several
discrete explanatory variables as well as a quantitative one. The commonly used
multinomial logit model assumes independent utilities for different choice
options. Thus, D-optimal optimal designs for such multinomial logit models may
comprise choice sets, e.g., consisting of alternatives which are identical in
all discrete attributes but different in the quantitative variable. Obviously
such designs are not appropriate for many empirical choice experiments. It will
be shown that locally D-optimal designs for multinomial probit models supposing
independent utilities consist of counterintuitive choice sets as well. However,
locally D-optimal designs for multinomial probit models allowing for dependent
utilities turn out to be reasonable for analyzing decisions using discrete
choice studies.
"
"stat.TH","  Our objective is to estimate the unknown compositional input from its output
response through an unknown system after estimating the inverse of the original
system with a training set. The proposed methods using artificial neural
networks (ANNs) can compete with the optimal bounds for linear systems, where
convex optimization theory applies, and demonstrate promising results for
nonlinear system inversions. We performed extensive experiments by designing
numerous different types of nonlinear systems.
"
"stat.TH","  We study high-dimensional regression with missing entries in the covariates.
A common strategy in practice is to \emph{impute} the missing entries with an
appropriate substitute and then implement a standard statistical procedure
acting as if the covariates were fully observed. Recent literature on this
subject proposes instead to design a specific, often complicated or non-convex,
algorithm tailored to the case of missing covariates. We investigate a simpler
approach where we fill-in the missing entries with their conditional mean given
the observed covariates. We show that this imputation scheme coupled with
standard off-the-shelf procedures such as the LASSO and square-root LASSO
retains the minimax estimation rate in the random-design setting where the
covariates are i.i.d.\ sub-Gaussian. We further show that the square-root LASSO
remains \emph{pivotal} in this setting.
  It is often the case that the conditional expectation cannot be computed
exactly and must be approximated from data. We study two cases where the
covariates either follow an autoregressive (AR) process, or are jointly
Gaussian with sparse precision matrix. We propose tractable estimators for the
conditional expectation and then perform linear regression via LASSO, and show
similar estimation rates in both cases. We complement our theoretical results
with simulations on synthetic and semi-synthetic examples, illustrating not
only the sharpness of our bounds, but also the broader utility of this strategy
beyond our theoretical assumptions.
"
"stat.TH","  Optimal transport (OT), and in particular the Wasserstein distance, has seen
a surge of interest and applications in machine learning. However, empirical
approximation under Wasserstein distances suffers from a severe curse of
dimensionality, rendering them impractical in high dimensions. As a result,
entropically regularized OT has become a popular workaround. However, while it
enjoys fast algorithms and better statistical properties, it looses the metric
structure that Wasserstein distances enjoy. This work proposes a novel
Gaussian-smoothed OT (GOT) framework, that achieves the best of both worlds:
preserving the 1-Wasserstein metric structure while alleviating the empirical
approximation curse of dimensionality. Furthermore, as the Gaussian-smoothing
parameter shrinks to zero, GOT $\Gamma$-converges towards classic OT (with
convergence of optimizers), thus serving as a natural extension. An empirical
study that supports the theoretical results is provided, promoting
Gaussian-smoothed OT as a powerful alternative to entropic OT.
"
"stat.TH","  Valid prediction of future observations is an important and challenging
problem. The two general approaches for quantifying uncertainty about the
future value employ prediction regions and predictive distribution,
respectively, with the latter usually considered to be more informative because
it performs other prediction-related tasks. Standard notions of validity focus
on the former, i.e., coverage probability bounds for prediction regions, but a
notion of validity relevant to the other prediction-related tasks performed by
the latter is lacking. In this paper, we present a new notion---strong
prediction validity---relevant to these more general prediction tasks. We show
that strong validity is connected to more familiar notions of coherence, and
argue that imprecise probability considerations are required in order to
achieve it. We go on to show that strong prediction validity can be achieved by
interpreting the conformal prediction output as the contour function of a
consonant plausibility function. We also offer an alternative characterization,
based on a new nonparametric inferential model construction, wherein the
appearance of consonance is more natural, and prove strong prediction validity.
"
"stat.TH","  Stein importance sampling is a widely applicable technique based on
kernelized Stein discrepancy, which corrects the output of approximate sampling
algorithms by reweighting the empirical distribution of the samples. A general
analysis of this technique is conducted for the previously unconsidered setting
where samples are obtained via the simulation of a Markov chain, and applies to
an arbitrary underlying Polish space. We prove that Stein importance sampling
yields consistent estimators for quantities related to a target distribution of
interest by using samples obtained from a geometrically ergodic Markov chain
with a possibly unknown invariant measure that differs from the desired target.
The approach is shown to be valid under conditions that are satisfied for a
large number of unadjusted samplers, and is capable of retaining consistency
when data subsampling is used. Along the way, a universal theory of reproducing
Stein kernels is established, which enables the construction of kernelized
Stein discrepancy on general Polish spaces, and provides sufficient conditions
for kernels to be convergence-determining on such spaces. These results are of
independent interest for the development of future methodology based on
kernelized Stein discrepancies.
"
"stat.TH","  We study the distribution of the maximum likelihood estimate (MLE) in
high-dimensional logistic models, extending the recent results from Sur (2019)
to the case where the Gaussian covariates may have an arbitrary covariance
structure. We prove that in the limit of large problems holding the ratio
between the number $p$ of covariates and the sample size $n$ constant, every
finite list of MLE coordinates follows a multivariate normal distribution.
Concretely, the $j$th coordinate $\hat {\beta}_j$ of the MLE is asymptotically
normally distributed with mean $\alpha_\star \beta_j$ and standard deviation
$\sigma_\star/\tau_j$; here, $\beta_j$ is the value of the true regression
coefficient, and $\tau_j$ the standard deviation of the $j$th predictor
conditional on all the others. The numerical parameters $\alpha_\star > 1$ and
$\sigma_\star$ only depend upon the problem dimensionality $p/n$ and the
overall signal strength, and can be accurately estimated. Our results imply
that the MLE's magnitude is biased upwards and that the MLE's standard
deviation is greater than that predicted by classical theory. We present a
series of experiments on simulated and real data showing excellent agreement
with the theory.
"
"stat.TH","  We show that lower-dimensional marginal densities of dependent zero-mean
normal distributions truncated to the positive orthant exhibit a mass-shifting
phenomenon. Despite the truncated multivariate normal density having a mode at
the origin, the marginal density assigns increasingly small mass near the
origin as the dimension increases. The phenomenon accentuates with stronger
correlation between the random variables. A precise quantification
characterizing the role of the dimension as well as the dependence is provided.
This surprising behavior has serious implications towards Bayesian constrained
estimation and inference, where the prior, in addition to having a full
support, is required to assign a substantial probability near the origin to
capture at parts of the true function of interest. Without further
modification, we show that truncated normal priors are not suitable for
modeling at regions and propose a novel alternative strategy based on shrinking
the coordinates using a multiplicative scale parameter. The proposed shrinkage
prior is empirically shown to guard against the mass shifting phenomenon while
retaining computational efficiency.
"
"stat.TH","  In a regression model, we write the Nadaraya-Watson estimator of the
regression function as the quotient of two kernel estimators, and propose a
bandwidth selection method for both the numerator and the denominator. We prove
risk bounds for both data driven estimators and for the resulting ratio. The
simulation study confirms that both estimators have good performances, compared
to the ones obtained by cross-validation selection of the bandwidth. However,
unexpectedly, the single-bandwidth cross-validation estimator is found to be
much better while choosing very small bandwidths. It performs even better than
the ratio of the two best estimators of the numerator and the denominator of
the collection, for which larger bandwidth are to be chosen.
"
"stat.TH","  The negative multinomial distribution is a multivariate generalization of the
negative binomial distribution. In this paper, we consider the problem of
estimating an unknown matrix of probabilities on the basis of observations of
negative multinomial variables under the standardized squared error loss.
First, a general sufficient condition for a shrinkage estimator to dominate the
UMVU estimator is derived and an empirical Bayes estimator satisfying the
condition is constructed. Next, a hierarchical shrinkage prior is introduced,
an associated Bayes estimator is shown to dominate the UMVU estimator under
some conditions, and some remarks about posterior computation are presented.
Finally, shrinkage estimators and the UMVU estimator are compared by
simulation.
"
"stat.TH","  We study the Wasserstein distance $W_2$ for Gaussian samples. We establish
the exact rate of convergence $\sqrt{\log\log n/n}$ of the expected value of
the $W_2$ distance between the empirical and true $c.d.f.$'s for the normal
distribution. We also show that the rate of weak convergence is unexpectedly
$1/\sqrt{n}$ in the case of two correlated Gaussian samples.
"
"stat.TH","  We consider predictive inference using a class of temporally dependent
Dirichlet processes driven by Fleming--Viot diffusions, which have a natural
bearing in Bayesian nonparametrics and lend the resulting family of random
probability measures to analytical posterior analysis. Formulating the implied
statistical model as a hidden Markov model, we fully describe the predictive
distribution induced by these Fleming--Viot-driven dependent Dirichlet
processes, for a sequence of observations collected at a certain time given
another set of draws collected at several previous times. This is identified as
a mixture of P\'olya urns, whereby the observations can be values from the
baseline distribution or copies of previous draws collected at the same time as
in the usual P\`olya urn, or can be sampled from a random subset of the data
collected at previous times. We characterise the time-dependent weights of the
mixture which select such subsets and discuss the asymptotic regimes. We
describe the induced partition by means of a Chinese restaurant process
metaphor with a conveyor belt, whereby new customers who do not sit at an
occupied table open a new table by picking a dish either from the baseline
distribution or from a time-varying offer available on the conveyor belt. We
lay out explicit algorithms for exact and approximate posterior sampling of
both observations and partitions, and illustrate our results on predictive
problems with synthetic and real data.
"
"stat.TH","  This short note reviews so-called Natural Gradient Descent (NGD) for
multivariate Gaussians. The Fisher Information Matrix (FIM) is derived for
several different parameterizations of Gaussians. Careful attention is paid to
the symmetric nature of the covariance matrix when calculating derivatives. We
show that there are some advantages to choosing a parameterization comprising
the mean and inverse covariance matrix and provide a simple NGD update that
accounts for the symmetric (and sparse) nature of the inverse covariance
matrix.
"
"stat.TH","  In this paper, we study the problem of non-adaptive group testing, in which
one seeks to identify which items are defective given a set of
suitably-designed tests whose outcomes indicate whether or not at least one
defective item was included in the test. The most widespread recovery criterion
seeks to exactly recover the entire defective set, and relaxed criteria such as
approximate recovery and list decoding have also been considered. In this
paper, we study the fundamental limits of group testing under the significantly
relaxed {\em weak recovery} criterion, which only seeks to identify a small
fraction (e.g., $0.01$) of the defective items. Given the near-optimality of
i.i.d.~Bernoulli testing for exact recovery in sufficiently sparse scaling
regimes, it is natural to ask whether this design additionally succeeds with
much fewer tests under weak recovery. Our main negative result shows that this
is not the case, and in fact, under i.i.d.~Bernoulli random testing in the
sufficiently sparse regime, an {\em all-or-nothing} phenomenon occurs: When the
number of tests is slightly below a threshold, weak recovery is impossible,
whereas when the number of tests is slightly above the same threshold,
high-probability exact recovery is possible. In establishing this result, we
additionally prove similar negative results under Bernoulli designs for the
weak detection problem (distinguishing between the group testing model
vs.~completely random outcomes) and the problem of identifying a single item
that is definitely defective. On the positive side, we show that all three
relaxed recovery criteria can be attained using considerably fewer tests under
suitably-chosen non-Bernoulli designs.
"
"stat.TH","  We obtain an optimal bound for a Gaussian approximation of a large class of
vector-valued random processes. Our results provide a substantial
generalization of earlier results that assume independence and/or stationarity.
Based on the decay rate of the functional dependence measure, we quantify the
error bound of the Gaussian approximation using the sample size $n$ and the
moment condition. Under the assumption of $p$th finite moment, with $p>2$, this
can range from a worst case rate of $n^{1/2}$ to the best case rate of
$n^{1/p}$.
"
"stat.TH","  This paper is concerned by the analysis of observations organized in a matrix
form whose elements are count data assumed to follow a Poisson or a multinomial
distribution. We focus on the estimation of either the intensity matrix
(Poisson case) or the compositional matrix (multinomial case) that is assumed
to have a low rank structure. We propose to construct an estimator minimizing
the regularized negative log-likelihood by a nuclear norm penalty. Our approach
easily yields a low-rank matrix-valued estimator with positive entries which
belongs to the set of row-stochastic matrices in the multinomial case. Then,
our main contribution is to propose a data-driven way to select the
regularization parameter in the construction of such estimators by minimizing
(approximately) unbiased estimates of the Kullback-Leibler (KL) risk in such
models, which generalize Stein's unbiased risk estimation originally proposed
for Gaussian data. The evaluation of these quantities is a delicate problem,
and we introduce novel methods to obtain accurate numerical approximation of
such unbiased estimates. Simulated data are used to validate this way of
selecting regularizing parameters for low-rank matrix estimation from count
data. For data following a multinomial distribution, we also compare its
performances to K-fold cross-validation. Examples from a survey study and
metagenomics also illustrate the benefits of our approach for real data
analysis.
"
"stat.TH","  We study the problem of the non-parametric estimation for the density $\pi$
of the stationary distribution of a stochastic two-dimensional damping
Hamiltonian system $(Z_t)_{t\in[0,T]}=(X_t,Y_t)_{t \in [0,T]}$. From the
continuous observation of the sampling path on $[0,T]$, we study the rate of
estimation for $\pi(x_0,y_0)$ as $T \to \infty$. We show that kernel based
estimators can achieve the rate $T^{-v}$ for some explicit exponent $v \in
(0,1/2)$. One finding is that the rate of estimation depends on the smoothness
of $\pi$ and is completely different with the rate appearing in the standard
i.i.d.\ setting or in the case of two-dimensional non degenerate diffusion
processes. Especially, this rate depends also on $y_0$. Moreover, we obtain a
minimax lower bound on the $L^2$-risk for pointwise estimation, with the same
rate $T^{-v}$, up to $\log(T)$ terms.
"
"stat.TH","  Building on the theory of causal discovery from observational data, we study
interactions between multiple (sets of) random variables in a linear structural
equation model with non-Gaussian error terms. We give a correspondence between
structure in the higher order cumulants and combinatorial structure in the
causal graph. It has previously been shown that low rank of the covariance
matrix corresponds to trek separation in the graph. Generalizing this criterion
to multiple sets of vertices, we characterize when determinants of subtensors
of the higher order cumulant tensors vanish. This criterion applies when hidden
variables are present as well. For instance, it allows us to identify the
presence of a hidden common cause of k of the observed variables.
"
"stat.TH","  This article gives a survey of the e-value, a statistical significance
measure a.k.a. the evidence rendered by observational data, X, in support of a
statistical hypothesis, H, or, the other way around, the epistemic value of H
given X. The $e$-value and the accompanying FBST, the Full Bayesian
Significance Test, constitute the core of a research program that was started
at IME-USP, is being developed by over 20 researchers worldwide, and has, so
far, been referenced by over 200 publications.
  The e-value and the FBST comply with the best principles of Bayesian
inference, including the likelihood principle, complete invariance, asymptotic
consistency, etc. Furthermore, they exhibit powerful logic or algebraic
properties in situations where one needs to compare or compose distinct
hypotheses that can be formulated either in the same or in different
statistical models. Moreover, they effortlessly accommodate the case of sharp
or precise hypotheses, a situation where alternative methods often require ad
hoc and convoluted procedures. Finally, the FBST has outstanding robustness and
reliability characteristics, outperforming traditional tests of hypotheses in
many practical applications of statistical modeling and operations research.
"
"stat.TH","  In the setting of sequential prediction of individual $\{0, 1\}$-sequences
with expert advice, we show that by allowing the learner to abstain from the
prediction by paying a cost marginally smaller than $\frac 12$ (say, $0.49$),
it is possible to achieve expected regret bounds that are independent of the
time horizon $T$. We exactly characterize the dependence on the abstention cost
$c$ and the number of experts $N$ by providing matching upper and lower bounds
of order $\frac{\log N}{1-2c}$, which is to be contrasted with the best
possible rate of $\sqrt{T\log N}$ that is available without the option to
abstain. We also discuss various extensions of our model, including a setting
where the sequence of abstention costs can change arbitrarily over time, where
we show regret bounds interpolating between the slow and the fast rates
mentioned above, under some natural assumptions on the sequence of abstention
costs.
"
"stat.TH","  Random linear mappings are widely used in modern signal processing,
compressed sensing and machine learning. These mappings may be used to embed
the data into a significantly lower dimension while at the same time preserving
useful information. This is done by approximately preserving the distances
between data points, which are assumed to belong to $\mathbb{R}^n$. Thus, the
performance of these mappings is usually captured by how close they are to an
isometry on the data. Random Gaussian linear mappings have been the object of
much study, while the sub-Gaussian settings is not yet fully understood. In the
latter case, the performance depends on the sub-Gaussian norm of the rows. In
many applications, e.g., compressed sensing, this norm may be large, or even
growing with dimension, and thus it is important to characterize this
dependence.
  We study when a sub-Gaussian matrix can become a near isometry on a set, show
that previous best known dependence on the sub-Gaussian norm was sub-optimal,
and present the optimal dependence. Our result not only answers a remaining
question posed by Liaw, Mehrabian, Plan and Vershynin in 2017, but also
generalizes their work. We also develop a new Bernstein type inequality for
sub-exponential random variables, and a new Hanson-Wright inequality for
quadratic forms of sub-Gaussian random variables, in both cases improving the
bounds in the sub-Gaussian regime under moment constraints. Finally, we
illustrate popular applications such as Johnson-Lindenstrauss embeddings,
randomized sketches and blind demodulation, whose theoretical guarantees can be
improved by our results in the sub-Gaussian case.
"
"stat.TH","  Machine learning models have been shown to be vulnerable to membership
inference attacks, i.e., inferring whether individuals' data have been used for
training models. The lack of understanding about factors contributing success
of these attacks motivates the need for modelling membership information
leakage using information theory and for investigating properties of machine
learning models and training algorithms that can reduce membership information
leakage. We use conditional mutual information leakage to measure the amount of
information leakage from the trained machine learning model about the presence
of an individual in the training dataset. We devise an upper bound for this
measure of information leakage using Kullback--Leibler divergence that is more
amenable to numerical computation. We prove a direct relationship between the
Kullback--Leibler membership information leakage and the probability of success
for a hypothesis-testing adversary examining whether a particular data record
belongs to the training dataset of a machine learning model. We show that the
mutual information leakage is a decreasing function of the training dataset
size and the regularization weight. We also prove that, if the sensitivity of
the machine learning model (defined in terms of the derivatives of the fitness
with respect to model parameters) is high, more membership information is
potentially leaked. This illustrates that complex models, such as deep neural
networks, are more susceptible to membership inference attacks in comparison to
simpler models with fewer degrees of freedom. We show that the amount of the
membership information leakage is reduced by
$\mathcal{O}(\log^{1/2}(\delta^{-1})\epsilon^{-1})$ when using Gaussian
$(\epsilon,\delta)$-differentially-private additive noises.
"
"stat.TH","  We use distributionally-robust optimization for machine learning to mitigate
the effect of data poisoning attacks. We provide performance guarantees for the
trained model on the original data (not including the poison records) by
training the model for the worst-case distribution on a neighbourhood around
the empirical distribution (extracted from the training dataset corrupted by a
poisoning attack) defined using the Wasserstein distance. We relax the
distributionally-robust machine learning problem by finding an upper bound for
the worst-case fitness based on the empirical sampled-averaged fitness and the
Lipschitz-constant of the fitness function (on the data for given model
parameters) as regularizer. For regression models, we prove that this
regularizer is equal to the dual norm of the model parameters. We use the Wine
Quality dataset, the Boston Housing Market dataset, and the Adult dataset for
demonstrating the results of this paper.
"
"stat.TH","  Optimal asset allocation is a key topic in modern finance theory. To realize
the optimal asset allocation on investor's risk aversion, various portfolio
construction methods have been proposed. Recently, the applications of machine
learning are rapidly growing in the area of finance. In this article, we
propose the Student's $t$-process latent variable model (TPLVM) to describe
non-Gaussian fluctuations of financial timeseries by lower dimensional latent
variables. Subsequently, we apply the TPLVM to minimum-variance portfolio as an
alternative of existing nonlinear factor models. To test the performance of the
proposed portfolio, we construct minimum-variance portfolios of global stock
market indices based on the TPLVM or Gaussian process latent variable model. By
comparing these portfolios, we confirm the proposed portfolio outperforms that
of the existing Gaussian process latent variable model.
"
"stat.TH","  We distinguish two questions (i) how much information does the prior contain?
and (ii) what is the effect of the prior? Several measures have been proposed
for quantifying effective prior sample size, for example Clarke [1996] and
Morita et al. [2008]. However, these measures typically ignore the likelihood
for the inference currently at hand, and therefore address (i) rather than
(ii). Since in practice (ii) is of great concern, Reimherr et al. [2014]
introduced a new class of effective prior sample size measures based on
prior-likelihood discordance. We take this idea further towards its natural
Bayesian conclusion by proposing measures of effective prior sample size that
not only incorporate the general mathematical form of the likelihood but also
the specific data at hand. Thus, our measures do not average across datasets
from the working model, but condition on the current observed data.
Consequently, our measures can be highly variable, but we demonstrate that this
is because the impact of a prior can be highly variable. Our measures are Bayes
estimates of meaningful quantities and well communicate the extent to which
inference is determined by the prior, or framed differently, the amount of
effort saved due to having prior information. We illustrate our ideas through a
number of examples including a Gaussian conjugate model (continuous
observations), a Beta-Binomial model (discrete observations), and a linear
regression model (two unknown parameters). Future work on further developments
of the methodology and an application to astronomy are discussed at the end.
"
"stat.TH","  We consider adaptive estimation and statistical inference for
high-dimensional graph-based linear models. In our model, the coordinates of
regression coefficients correspond to an underlying undirected graph.
Furthermore, the given graph governs the piecewise polynomial structure of the
regression vector. In the adaptive estimation part, we apply graph-based
regularization techniques and propose a family of locally adaptive estimators
called the Graph-Piecewise-Polynomial-Lasso. We further study a one-step update
of the Graph-Piecewise-Polynomial-Lasso for the problem of statistical
inference. We develop the corresponding theory, which includes the fixed design
and the sub-Gaussian random design. Finally, we illustrate the superior
performance of our approaches by extensive simulation studies and conclude with
an application to an Arabidopsis thaliana microarray dataset.
"
"stat.TH","  Gaussian processes are ubiquitous in machine learning, statistics, and
applied mathematics. They provide a flexible modelling framework for
approximating functions, whilst simultaneously quantifying uncertainty.
However, this is only true when the model is well-specified, which is often not
the case in practice. In this paper, we study the properties of Gaussian
process means when the smoothness of the model and the likelihood function are
misspecified. In this setting, an important theoretical question of practial
relevance is how accurate the Gaussian process approximations will be given the
difficulty of the problem, our model and the extent of the misspecification.
The answer to this problem is particularly useful since it can inform our
choice of model and experimental design. In particular, we describe how the
experimental design and choice of kernel and kernel hyperparameters can be
adapted to alleviate model misspecification.
"
"stat.TH","  ""Spatial"" or ""geometric"" quantiles are the only multivariate quantiles coping
with both high-dimensional data and functional data, also in the framework of
multiple-output quantile regression. This work studies spatial quantiles in the
finite-dimensional case, where the spatial quantile $\mu_{\alpha,u}(P)$ of the
distribution $P$ taking values in $\mathbb{R}^d $ is a point in $\mathbb{R}^d$
indexed by an order $\alpha\in[0,1)$ and a direction $u$ in the unit sphere
$\mathcal{S}^{d-1}$ of $\mathbb{R}^d$ --- or equivalently by a vector $\alpha
u$ in the open unit ball of $\mathbb{R}^d$. Recently, Girard and Stupfler
(2017) proved that (i) the extreme quantiles $\mu_{\alpha,u}(P)$ obtained as
$\alpha\to 1$ exit all compact sets of $\mathbb{R}^d$ and that (ii) they do so
in a direction converging to $u$. These results help understanding the nature
of these quantiles: the first result is particularly striking as it holds even
if $P$ has a bounded support, whereas the second one clarifies the delicate
dependence of spatial quantiles on $u$. However, they were established under
assumptions imposing that $P$ is non-atomic, so that it is unclear whether they
hold for empirical probability measures. We improve on this by proving these
results under much milder conditions, allowing for the sample case. This
prevents using gradient condition arguments, which makes the proofs very
challenging. We also weaken the well-known sufficient condition for uniqueness
of finite-dimensional spatial quantiles.
"
"stat.TH","  We study non-parametric estimation of an unknown density with support in R
(respectively R+). The proposed estimation procedure is based on the projection
on finite dimensional subspaces spanned by the Hermite (respectively the
Laguerre) functions. The focus of this paper is to introduce a data-driven
aggregation approach in order to deal with the upcoming bias-variance
trade-off. Our novel procedure integrates the usual model selection method as a
limit case. We show the oracle- and the minimax-optimality of the data-driven
aggregated density estimator and hence its adaptivity. We present results of a
simulation study which allow to compare the finite sample performance of the
data-driven estimators using model selection compared to the new aggregation.
"
"stat.TH","  We obtain explicit error bounds for the $d$-dimensional normal approximation
on hyperrectangles for a random vector that has a Stein kernel, or admits an
exchangeable pair coupling, or is a non-linear statistic of independent random
variables or a sum of $n$ locally dependent random vectors. We assume the
approximating normal distribution has a non-singular covariance matrix. The
error bounds vanish even when the dimension $d$ is much larger than the sample
size $n$. We prove our main results using the approach of G\""otze (1991) in
Stein's method, together with modifications of an estimate of Anderson, Hall
and Titterington (1998) and a smoothing inequality of Bhattacharya and Rao
(1976). For sums of $n$ independent and identically distributed isotropic
random vectors having a log-concave density, we obtain an error bound that is
optimal up to a $\log n$ factor. We also discuss an application to multiple
Wiener-It\^{o} integrals.
"
"stat.TH","  Despite the ubiquity of the Gaussian process regression model, few
theoretical results are available that account for the fact that parameters of
the covariance kernel typically need to be estimated from the dataset. This
article provides one of the first theoretical analyses in the context of
Gaussian process regression with a noiseless dataset. Specifically, we consider
the scenario where the scale parameter of a Sobolev kernel (such as a
Mat\'{e}rn kernel) is estimated by maximum likelihood. We show that the maximum
likelihood estimation of the scale parameter alone provides significant
adaptation against misspecification of the Gaussian process model in the sense
that the model can become ""slowly"" overconfident at worst, regardless of the
difference between the smoothness of the data-generating function and that
expected by the model. The analysis is based on a combination of techniques
from nonparametric regression and scattered data interpolation. Empirical
results are provided in support of the theoretical findings.
"
"stat.TH","  We discuss probability text models and its modifications. We have proved a
theorem on the convergence of a multidimensional process of the number of urns
containing a fixed number of balls in the Simon model to a multidimensional
Gaussian process. We have introduced and investigated three two-parameter urn
schemes that guarantee the power tail asymptotics of the number of occupied
urns. These models do not follow the restriction on the limitation of the ratio
of the number of urns with exactly one ball to the number of occupied urns that
appears in the classical Bahadur-Karlin model.
"
"stat.TH","  A general class of Bayesian lower bounds when the underlying loss function is
a Bregman divergence is demonstrated. This class can be considered as an
extension of the Weinstein--Weiss family of bounds for the mean squared error
and relies on finding a variational characterization of Bayesian risk. The
approach allows for the derivation of a version of the Cram\'er--Rao bound that
is specific to a given Bregman divergence. The new generalization of the
Cram\'er--Rao bound reduces to the classical one when the loss function is
taken to be the Euclidean norm. The effectiveness of the new bound is evaluated
in the Poisson noise setting and the Binomial noise setting.
"
"stat.TH","  We consider a multi-armed bandit problem with covariates. Given a realization
of the covariate vector, instead of targeting the treatment with highest
conditional expectation, the decision maker targets the treatment which
maximizes a general functional of the conditional potential outcome
distribution, e.g., a conditional quantile, trimmed mean, or a socio-economic
functional such as an inequality, welfare or poverty measure. We develop
expected regret lower bounds for this problem, and construct a near minimax
optimal assignment policy.
"
"stat.TH","  We bound the variance and other moments of a random vector based on the range
of its realizations, thus generalizing inequalities of Popoviciu (1935) and
Bhatia and Davis (2000) concerning measures on the line to several dimensions.
This is done using convex duality and (infinite-dimensional) linear
programming.
  The following consequence of our bounds exhibits symmetry breaking, provides
a new proof of Jung's theorem (1901), and turns out to have applications to the
aggregation dynamics modelling attractive-repulsive interactions: among
probability measures on ${\mathbf R}^n$ whose support has diameter at most
$\sqrt{2}$, we show that the variance around the mean is maximized precisely by
those measures which assign mass $1/(n+1)$ to each vertex of a standard
simplex.
  For $1 \le p <\infty$, the $p$-th moment --- optimally centered --- is
maximized by the same measures among those satisfying the diameter constraint.
"
"stat.TH","  Cross validation is a central tool in evaluating the performance of machine
learning and statistical models. However, despite its ubiquitous role, its
theoretical properties are still not well understood. We study the asymptotic
properties of the cross validated-risk for a large class of models. Under
stability conditions, we establish a central limit theorem and Berry-Esseen
bounds, which enable us to compute asymptotically accurate confidence
intervals. Using our results, we paint a big picture for the statistical
speed-up of cross validation compared to a train-test split procedure. A
corollary of our results is that parametric M-estimators (or empirical risk
minimizers) benefit from the ""full"" speed-up when performing cross-validation
under the training loss. In other common cases, such as when the training is
done using a surrogate loss or a regularizer, we show that the behavior of the
cross-validated risk is complex with a variance reduction which may be smaller
or larger than the ""full"" speed-up, depending on the model and the underlying
distribution. We allow the number of folds to grow with the number of
observations at any rate.
"
"stat.TH","  Modal linear regression (MLR) is a method for obtaining a conditional mode
predictor as a linear model. We study kernel selection for MLR from two
perspectives: ""which kernel achieves smaller error?"" and ""which kernel is
computationally efficient?"". First, we show that a Biweight kernel is optimal
in the sense of minimizing an asymptotic mean squared error of a resulting MLR
parameter. This result is derived from our refined analysis of an asymptotic
statistical behavior of MLR. Secondly, we provide a kernel class for which
iteratively reweighted least-squares algorithm (IRLS) is guaranteed to
converge, and especially prove that IRLS with an Epanechnikov kernel terminates
in a finite number of iterations. Simulation studies empirically verified that
using a Biweight kernel provides good estimation accuracy and that using an
Epanechnikov kernel is computationally efficient. Our results improve MLR of
which existing studies often stick to a Gaussian kernel and modal EM algorithm
specialized for it, by providing guidelines of kernel selection.
"
"stat.TH","  We study an extension of the classic stochastic multi-armed bandit problem
which involves multiple plays and Markovian rewards in the rested bandits
setting. In order to tackle this problem we consider an adaptive allocation
rule which at each stage combines the information from the sample means of all
the arms, with the Kullback-Leibler upper confidence bound of a single arm
which is selected in round-robin way. For rewards generated from a
one-parameter exponential family of Markov chains, we provide a finite-time
upper bound for the regret incurred from this adaptive allocation rule, which
reveals the logarithmic dependence of the regret on the time horizon, and which
is asymptotically optimal. For our analysis we devise several concentration
results for Markov chains, including a maximal inequality for Markov chains,
that may be of interest in their own right. As a byproduct of our analysis we
also establish asymptotically optimal, finite-time guarantees for the case of
multiple plays, and i.i.d. rewards drawn from a one-parameter exponential
family of probability densities. Additionally, we provide simulation results
that illustrate that calculating Kullback-Leibler upper confidence bounds in a
round-robin way, is significantly more efficient than calculating them for
every arm at each round, and that the expected regrets of those two approaches
behave similarly.
"
"stat.TH","  In this work, we develop and study an empirical projection operator scheme
for solving nonparametric regression problems. This scheme is based on an
approximate projection of the regression function over a suitable reproducing
kernel Hilbert space (RKHS). The RKHS considered in this paper are generated by
the Mercer kernels given by the Legendre Christoffel-Darboux and convolution
Sinc kernels. We provide error and convergence analysis of the proposed scheme
under the assumption that the regression function belongs to some suitable
functional spaces. We also consider the popular RKHS regularized least square
minimization for nonparametric regression. In particular, we check the
numerical stability of this second scheme and we provide its convergence rate
in the special case of the Sinc kernel. Finally, we illustrate the proposed
methods by various numerical simulation.
"
"stat.TH","  The problem of constructing effective statistical tests for random number
generators (RNG) is considered. Currently, there are hundreds of RNG
statistical tests that are often combined into so-called batteries, each
containing from a dozen to more than one hundred tests.
  When a battery test is used, it is applied to a sequence generated by the
RNG, and the calculation time is determined by the length of the sequence and
the number of tests. Generally speaking, the longer the sequence, the smaller
deviations from randomness can be found by a specific test. So, when a battery
is applied, on the one hand, the ""better"" tests are in the battery, the more
chances to reject a ""bad"" RNG. On the other hand, the larger the battery, the
less time can be spent on each test and, therefore, the shorter the test
sequence. In turn, this reduces the ability to find small deviations from
randomness. To reduce this trade-off, we propose an adaptive way to use
batteries (and other sets) of tests, which requires less time but, in a certain
sense, preserves the power of the original battery. We call this method
time-adaptive battery of tests.
"
"stat.TH","  For multivariate distributions in the domain of attraction of a max-stable
distribution, the tail copula and the stable tail dependence function are
equivalent ways to capture the dependence in the upper tail. The empirical
versions of these functions are rank-based estimators whose inflated estimation
errors are known to converge weakly to a Gaussian process that is similar in
structure to the weak limit of the empirical copula process. We extend this
multivariate result to continuous functional data by establishing the
asymptotic normality of the estimators of the tail copula, uniformly over all
finite subsets of at most $D$ points ($D$ fixed). An application for testing
tail copula stationarity is presented. The main tool for deriving the result is
the uniform asymptotic normality of all the $D$-variate tail empirical
processes. The proof of the main result is non-standard.
"
"stat.TH","  We develop a mathematically rigorous framework for multilayer neural networks
in the mean field regime. As the network's width increases, the network's
learning trajectory is shown to be well captured by a meaningful and
dynamically nonlinear limit (the \textit{mean field} limit), which is
characterized by a system of ODEs. Our framework applies to a broad range of
network architectures, learning dynamics and network initializations. Central
to the framework is the new idea of a \textit{neuronal embedding}, which
comprises of a non-evolving probability space that allows to embed neural
networks of arbitrary widths.
  We demonstrate two applications of our framework. Firstly the framework gives
a principled way to study the simplifying effects that independent and
identically distributed initializations have on the mean field limit. Secondly
we prove a global convergence guarantee for two-layer and three-layer networks.
Unlike previous works that rely on convexity, our result requires a certain
universal approximation property, which is a distinctive feature of
infinite-width neural networks. To the best of our knowledge, this is the first
time global convergence is established for neural networks of more than two
layers in the mean field regime.
"
"stat.TH","  In this paper, we show that the largest and smallest eigenvalues of a sample
correlation matrix stemming from $n$ independent observations of a
$p$-dimensional time series with iid components converge almost surely to
$(1+\sqrt{\gamma})^2$ and $(1-\sqrt{\gamma})^2$, respectively, as $n \to
\infty$, if $p/n\to \gamma \in (0,1]$ and the truncated variance of the entry
distribution is 'almost slowly varying', a condition we describe via moment
properties of self-normalized sums. Moreover, the empirical spectral
distributions of these sample correlation matrices converge weakly, with
probability 1, to the Marchenko-Pastur law, which extends a result in Bai and
Zhou (2008). We compare the behavior of the eigenvalues of the sample
covariance and sample correlation matrices and argue that the latter seems more
robust, in particular in the case of infinite fourth moment. We briefly address
some practical issues for the estimation of extreme eigenvalues in a simulation
study.
  In our proofs we use the method of moments combined with a Path-Shortening
Algorithm, which efficiently uses the structure of sample correlation matrices,
to calculate precise bounds for matrix norms. We believe that this new approach
could be of further use in random matrix theory.
"
"stat.TH","  Gaussian process (GP) priors are non-parametric generative models with
appealing modelling properties for Bayesian inference: they can model
non-linear relationships through noisy observations, have closed-form
expressions for training and inference, and are governed by interpretable
hyperparameters. However, GP models rely on Gaussianity, an assumption that
does not hold in several real-world scenarios, e.g., when observations are
bounded or have extreme-value dependencies, a natural phenomenon in physics,
finance and social sciences. Although beyond-Gaussian stochastic processes have
caught the attention of the GP community, a principled definition and rigorous
treatment is still lacking. In this regard, we propose a methodology to
construct stochastic processes, which include GPs, warped GPs, Student-t
processes and several others under a single unified approach. We also provide
formulas and algorithms for training and inference of the proposed models in
the regression problem. Our approach is inspired by layers-based models, where
each proposed layer changes a specific property over the generated stochastic
process. That, in turn, allows us to push-forward a standard Gaussian white
noise prior towards other more expressive stochastic processes, for which
marginals and copulas need not be Gaussian, while retaining the appealing
properties of GPs. We validate the proposed model through experiments with
real-world data.
"
"stat.TH","  We develop a quasi-likelihood analysis procedure for a general class of
multivariate marked point processes. As a by-product of the general method, we
establish under stability and ergodicity conditions the local asymptotic
normality of the quasi-log likelihood, along with the convergence of moments of
quasi-likelihood and quasi-Bayesian estimators. To illustrate the general
approach, we then turn our attention to a class of multivariate marked Hawkes
processes with generalized exponential kernels, comprising among others the
so-called Erlang kernels. We provide explicit conditions on the kernel
functions and the mark dynamics under which a certain transformation of the
original process is Markovian and $V$-geometrically ergodic. We finally prove
that the latter result, which is of interest in its own right, constitutes the
key ingredient to show that the generalized exponential Hawkes process falls
under the scope of application of the quasi-likelihood analysis.
"
"stat.TH","  An original graph clustering approach to efficient localization of error
covariances is proposed within an ensemble-variational data assimilation
framework. Here the localization term is very generic and refers to the idea of
breaking up a global assimilation into subproblems. This unsupervised
localization technique based on a linearizedstate-observation measure is
general and does not rely on any prior information such as relevant spatial
scales, empirical cut-off radius or homogeneity assumptions. It automatically
segregates the state and observation variables in an optimal number of clusters
(otherwise named as subspaces or communities), more amenable to scalable data
assimilation.The application of this method does not require underlying
block-diagonal structures of prior covariance matrices. In order to deal with
inter-cluster connectivity, two alternative data adaptations are proposed. Once
the localization is completed, an adaptive covariance diagnosis and tuning is
performed within each cluster. Numerical tests show that this approach is less
costly and more flexible than a global covariance tuning, and most often
results in more accurate background and observations error covariances.
"
"stat.TH","  In this paper we revisit the integral functional of geometric Brownian motion
$I_t= \int_0^t e^{-(\mu s +\sigma W_s)}ds$, where $\mu\in\mathbb{R}$, $\sigma >
0$, and $(W_s )_s>0$ is a standard Brownian motion. Specifically, we calculate
the Laplace transform in $t$ of the cumulative distribution function and of the
probability density function of this functional.
"
"stat.TH","  The singular value decomposition (SVD) and the principal component analysis
are fundamental tools and probably the most popular methods for data dimension
reduction. The rapid growth in the size of data matrices has lead to a need for
developing efficient large-scale SVD algorithms. Randomized SVD was proposed,
and its potential was demonstrated for computing a low-rank SVD (Rokhlin et
al., 2009). In this article, we provide a consistency theorem for the
randomized SVD algorithm and a numerical example to show how the random
projections to low dimension affect the consistency.
"
"stat.TH","  We study the problem of estimating the source of a network cascade. The
cascade starts from a single vertex at time 0 and spreads over time, but only a
noisy version of the propagation is observable. The goal is then to design a
stopping time and estimator that will estimate the source well while ensuring
the cost of the cascade to the system is not too large. We rigorously formulate
a Bayesian approach to the problem. If vertices can be labelled by vectors in
Euclidean space (which is natural in geo-spatial networks), the optimal
estimator is the conditional mean estimator, and we derive an explicit form for
the optimal stopping time under minimal assumptions on the cascade dynamics. We
study the performance of the optimal stopping time on lattices, and show that a
computationally efficient but suboptimal stopping time which compares the
posterior variance to a threshold has near-optimal performance.
"
"stat.TH","  Estimating the shape of an elliptical distribution is a fundamental problem
in statistics. One estimator for the shape matrix, Tyler's M-estimator, has
been shown to have many appealing asymptotic properties. It performs well in
numerical experiments and can be quickly computed in practice by a simple
iterative procedure. Despite the many years the estimator has been studied in
the statistics community, there was neither a non-asymptotic bound on the rate
of the estimator nor a proof that the iterative procedure converges in
polynomially many steps.
  Here we observe a surprising connection between Tyler's M-estimator and
operator scaling, which has been intensively studied in recent years in part
because of its connections to the Brascamp-Lieb inequality in analysis. We use
this connection, together with novel results on quantum expanders, to show that
Tyler's M-estimator has the optimal rate up to factors logarithmic in the
dimension, and that in the generative model the iterative procedure has a
linear convergence rate even without regularization.
"
"stat.TH","  In machine learning, it is common to optimize the parameters of a
probabilistic model, modulated by a somewhat ad hoc regularization term that
penalizes some values of the parameters. Regularization terms appear naturally
in Variational Inference (VI), a tractable way to approximate Bayesian
posteriors: the loss to optimize contains a Kullback--Leibler divergence term
between the approximate posterior and a Bayesian prior. We fully characterize
which regularizers can arise this way, and provide a systematic way to compute
the corresponding prior. This viewpoint also provides a prediction for useful
values of the regularization factor in neural networks. We apply this framework
to regularizers such as L1 or group-Lasso.
"
"stat.TH","  In this paper, we consider the nonlinear ill-posed inverse problem with noisy
data in the statistical learning setting. The Tikhonov regularization scheme in
Hilbert scales is considered to reconstruct the estimator from the random noisy
data. In this statistical learning setting, we derive the rates of convergence
for the regularized solution under certain assumptions on the nonlinear forward
operator and the prior assumptions. We discuss estimates of the reconstruction
error using the approach of reproducing kernel Hilbert spaces.
"
"stat.TH","  We develop a corrective mechanism for neural network approximation: the total
available non-linear units are divided into multiple groups and the first group
approximates the function under consideration, the second group approximates
the error in approximation produced by the first group and corrects it, the
third group approximates the error produced by the first and second groups
together and so on. This technique yields several new representation and
learning results for neural networks. First, we show that two-layer neural
networks in the random features regime (RF) can memorize arbitrary labels for
arbitrary points under under Euclidean distance separation condition using
$\tilde{O}(n)$ ReLUs which is optimal in $n$ up to logarithmic factors. Next,
we give a powerful representation result for two-layer neural networks with
ReLUs and smoothed ReLUs which can achieve a squared error of at most
$\epsilon$ with $O(C(a,d)\epsilon^{-1/(a+1)})$ for $a \in \mathbb{N}\cup\{0\}$
when the function is smooth enough (roughly when it has $\Theta(ad)$ bounded
derivatives). In certain cases $d$ can be replaced with effective dimension $q
\ll d$. Previous results of this type implement Taylor series approximation
using deep architectures. We also consider three-layer neural networks and show
that the corrective mechanism yields faster representation rates for smooth
radial functions. Lastly, we obtain the first $O(\mathrm{subpoly}(1/\epsilon))$
upper bound on the number of neurons required for a two layer network to learn
low degree polynomials up to squared error $\epsilon$ via gradient descent.
Even though deep networks can express these polynomials with
$O(\mathrm{polylog}(1/\epsilon))$ neurons, the best learning bounds on this
problem require $\mathrm{poly}(1/\epsilon)$ neurons.
"
"stat.TH","  We consider the problem of sampling from a strongly log-concave density in
$\mathbb{R}^d$, and prove an information theoretic lower bound on the number of
stochastic gradient queries of the log density needed. Several popular sampling
algorithms (including many Markov chain Monte Carlo methods) operate by using
stochastic gradients of the log density to generate a sample; our results
establish an information theoretic limit for all these algorithms.
  We show that for every algorithm, there exists a well-conditioned strongly
log-concave target density for which the distribution of points generated by
the algorithm would be at least $\epsilon$ away from the target in total
variation distance if the number of gradient queries is less than
$\Omega(\sigma^2 d/\epsilon^2)$, where $\sigma^2 d$ is the variance of the
stochastic gradient. Our lower bound follows by combining the ideas of Le Cam
deficiency routinely used in the comparison of statistical experiments along
with standard information theoretic tools used in lower bounding Bayes risk
functions. To the best of our knowledge our results provide the first
nontrivial dimension-dependent lower bound for this problem.
"
"stat.TH","  We consider the problem of spherical Gaussian Mixture models with $k \geq 3$
components when the components are well separated. A fundamental previous
result established that separation of $\Omega(\sqrt{\log k})$ is necessary and
sufficient for identifiability of the parameters with polynomial sample
complexity (Regev and Vijayaraghavan, 2017). In the same context, we show that
$\tilde{O} (kd/\epsilon^2)$ samples suffice for any $\epsilon \lesssim 1/k$,
closing the gap from polynomial to linear, and thus giving the first optimal
sample upper bound for the parameter estimation of well-separated Gaussian
mixtures. We accomplish this by proving a new result for the
Expectation-Maximization (EM) algorithm: we show that EM converges locally,
under separation $\Omega(\sqrt{\log k})$. The previous best-known guarantee
required $\Omega(\sqrt{k})$ separation (Yan, et al., 2017). Unlike prior work,
our results do not assume or use prior knowledge of the (potentially different)
mixing weights or variances of the Gaussian components. Furthermore, our
results show that the finite-sample error of EM does not depend on
non-universal quantities such as pairwise distances between means of Gaussian
components.
"
"stat.TH","  The paper deals with disorders detection in the multivariate stochastic
process. We consider the multidimensional Poisson process or the multivariate
renewal process. This class of processes can be used as a description of the
distributed detection system. The multivariate renewal process can be seen as
the sequence of random vectors, where parts of its coordinates are holding
times, others are the size of jumps and the index of stream, at which the new
event appears. It is assumed that at each stream two kinds of changes are
possible: in the holding time or in the size of jumps distribution. The various
specific mutual relations between the change points are possible. The aim of
the research is to derive the detectors which realize the optimal value of the
specified criterion. The change point moment estimates have been obtained in
some cases. The difficulties have appeared for the dependent streams with
unspecified order of change points. The presented results suggest further
research on the construction of detectors in the general model.
"
"stat.TH","  Recently, there have been significant interests in studying the so-called
""double-descent"" of the generalization error of linear regression models under
the overparameterized and overfitting regime, with the hope that such analysis
may provide the first step towards understanding why overparameterized deep
neural networks (DNN) still generalize well. However, to date most of these
studies focused on the min $\ell_2$-norm solution that overfits the data. In
contrast, in this paper we study the overfitting solution that minimizes the
$\ell_1$-norm, which is known as Basis Pursuit (BP) in the compressed sensing
literature. Under a sparse true linear regression model with $p$ i.i.d.
Gaussian features, we show that for a large range of $p$ up to a limit that
grows exponentially with the number of samples $n$, with high probability the
model error of BP is upper bounded by a value that decreases with $p$. To the
best of our knowledge, this is the first analytical result in the literature
establishing the double-descent of overfitting BP for finite $n$ and $p$.
Further, our results reveal significant differences between the double-descent
of BP and min $\ell_2$-norm solutions. Specifically, the double-descent
upper-bound of BP is independent of the signal strength, and for high SNR and
sparse models the descent-floor of BP can be much lower and wider than that of
min $\ell_2$-norm solutions.
"
"stat.TH","  This paper addresses the problem of prediction with expert advice for
outcomes in a geodesic space with non-positive curvature in the sense of
Alexandrov. Via geometric considerations, and in particular the notion of
barycenters, we extend to this setting the definition and analysis of the
classical exponentially weighted average forecaster. We also adapt the
principle of online to batch conversion to this setting. We shortly discuss the
application of these results in the context of aggregation and for the problem
of barycenter estimation.
"
"stat.TH","  Minimum distance estimation (MDE) gained recent attention as a formulation of
(implicit) generative modeling. It considers minimizing, over model parameters,
a statistical distance between the empirical data distribution and the model.
This formulation lends itself well to theoretical analysis, but typical results
are hindered by the curse of dimensionality. To overcome this and devise a
scalable finite-sample statistical MDE theory, we adopt the framework of smooth
1-Wasserstein distance (SWD) $\mathsf{W}_1^{(\sigma)}$. The SWD was recently
shown to preserve the metric and topological structure of classic Wasserstein
distances, while enjoying dimension-free empirical convergence rates. In this
work, we conduct a thorough statistical study of the minimum smooth Wasserstein
estimators (MSWEs), first proving the estimator's measurability and asymptotic
consistency. We then characterize the limit distribution of the optimal model
parameters and their associated minimal SWD. These results imply an
$O(n^{-1/2})$ generalization bound for generative modeling based on MSWE, which
holds in arbitrary dimension. Our main technical tool is a novel
high-dimensional limit distribution result for empirical
$\mathsf{W}_1^{(\sigma)}$. The characterization of a nondegenerate limit stands
in sharp contrast with the classic empirical 1-Wasserstein distance, for which
a similar result is known only in the one-dimensional case. The validity of our
theory is supported by empirical results, posing the SWD as a potent tool for
learning and inference in high dimensions.
"
"stat.TH","  Statistical divergences are ubiquitous in machine learning as tools for
measuring discrepancy between probability distributions. As these applications
inherently rely on approximating distributions from samples, we consider
empirical approximation under two popular $f$-divergences: the total variation
(TV) distance and the $\chi^2$-divergence. To circumvent the sensitivity of
these divergences to support mismatch, the framework of Gaussian smoothing is
adopted. We study the limit distributions of
$\sqrt{n}\delta_{\mathsf{TV}}(P_n\ast\mathcal{N},P\ast\mathcal{N})$ and
$n\chi^2(P_n\ast\mathcal{N}\|P\ast\mathcal{N})$, where $P_n$ is the empirical
measure based on $n$ independently and identically distributed (i.i.d.)
observations from $P$,
$\mathcal{N}_\sigma:=\mathcal{N}(0,\sigma^2\mathrm{I}_d)$, and $\ast$ stands
for convolution. In arbitrary dimension, the limit distributions are
characterized in terms of Gaussian process on $\mathbb{R}^d$ with covariance
operator that depends on $P$ and the isotropic Gaussian density of parameter
$\sigma$. This, in turn, implies optimality of the $n^{-1/2}$ expected value
convergence rates recently derived for
$\delta_{\mathsf{TV}}(P_n\ast\mathcal{N},P\ast\mathcal{N})$ and
$\chi^2(P_n\ast\mathcal{N}\|P\ast\mathcal{N})$. These strong statistical
guarantees promote empirical approximation under Gaussian smoothing as a potent
framework for learning and inference based on high-dimensional data.
"
"stat.TH","  In longitudinal studies, repeated measures are collected over time and hence
they tend to be serially correlated. In this paper we consider an extension of
skew-normal/independent linear mixed models introduced by Lachos et al. (2010),
where the error term has a dependence structure, such as damped exponential
correlation or autoregressive correlation of order p. The proposed model
provides flexibility in capturing the effects of skewness and heavy tails
simultaneously when continuous repeated measures are serially correlated. For
this robust model, we present an efficient EM-type algorithm for computation of
maximum likelihood estimation of parameters and the observed information matrix
is derived analytically to account for standard errors. The methodology is
illustrated through an application to schizophrenia data and several simulation
studies. The proposed algorithm and methods are implemented in the new R
package skewlmm.
"
"stat.TH","  Construction of tight confidence regions and intervals is central to
statistical inference and decision-making. Consider an empirical distribution
$\widehat{\boldsymbol{p}}$ generated from $n$ iid realizations of a random
variable that takes one of $k$ possible values according to an unknown
distribution $\boldsymbol{p}$. This is analogous with a single draw from a
multinomial distribution. A confidence region is a subset of the probability
simplex that depends on $\widehat{\boldsymbol{p}}$ and contains the unknown
$\boldsymbol{p}$ with a specified confidence. This paper shows how one can
construct minimum average volume confidence regions, answering a long standing
question. We also show the optimality of the regions directly translates to
optimal confidence intervals of functionals, such as the mean, variance and
median.
"
"stat.TH","  Bayesian and other likelihood-based methods require specification of a
statistical model and may not be fully satisfactory for inference on
quantities, such as quantiles, that are not naturally defined as model
parameters. In this paper, we construct a direct and model-free Gibbs posterior
distribution for multivariate quantiles. Being model-free means that inferences
drawn from the Gibbs posterior are not subject to model misspecification bias,
and being direct means that no priors for or marginalization over nuisance
parameters are required. We show here that the Gibbs posterior enjoys a
root-$n$ convergence rate and a Bernstein--von Mises property, i.e., for large
n, the Gibbs posterior distribution can be approximated by a Gaussian.
Moreover, we present numerical results showing the validity and efficiency of
credible sets derived from a suitably scaled Gibbs posterior.
"
"stat.TH","  We study learning problems in which the underlying class is a bounded subset
of $L_p$ and the target $Y$ belongs to $L_p$. Previously, minimax sample
complexity estimates were known under such boundedness assumptions only when
$p=\infty$. We present a sharp sample complexity estimate that holds for any $p
> 4$. It is based on a learning procedure that is suited for heavy-tailed
problems.
"
"stat.TH","  This paper addresses the case where data come as point sets, or more
generally as discrete measures. Our motivation is twofold: first we intend to
approximate with a compactly supported measure the mean of the measure
generating process, that coincides with the intensity measure in the point
process framework, or with the expected persistence diagram in the framework of
persistence-based topological data analysis. To this aim we provide two
algorithms that we prove almost minimax optimal. Second we build from the
estimator of the mean measure a vectorization map, that sends every measure
into a finite-dimensional Euclidean space, and investigate its properties
through a clustering-oriented lens. In a nutshell, we show that in a mixture of
measure generating process, our technique yields a representation in
$\mathbb{R}^k$, for $k \in \mathbb{N}^*$ that guarantees a good clustering of
the data points with high probability. Interestingly, our results apply in the
framework of persistence-based shape classification via the ATOL procedure
described in \cite{Royer19}.
"
"stat.TH","  The Gaussian process modeling is a standard tool for building emulators for
computer experiments, which is usually a deterministic function, for example,
solution to a partial differential equations system. In this work, we
investigate applying Gaussian process models to a deterministic function from
prediction and uncertainty quantification perspectives. While the upper bounds
and optimal convergence rates of prediction in Gaussian process modeling have
been extensively studied in the literature, a thorough exploration of the
convergence rate and theoretical study of uncertainty quantification is
lacking. We prove that, if one uses maximum likelihood estimation to estimate
the variance, under different choices of nugget parameters, the predictor is
not optimal and/or the confidence interval is not reliable. In particular,
lower bounds of the predictor under different choices of nugget parameters are
obtained. The results suggest that, if one applies Gaussian process models to a
deterministic function, the reliability of the confidence interval and the
optimality of predictors cannot be achieved at the same time.
"
"stat.TH","  The goal of this paper is to show that a single robust estimator of the mean
of a multivariate Gaussian distribution can enjoy five desirable properties.
First, it is computationally tractable in the sense that it can be computed in
a time which is at most polynomial in dimension, sample size and the logarithm
of the inverse of the contamination rate. Second, it is equivariant by
translations and orthogonal transformations. Third, it has a high breakdown
point equal to $0.5$, and a nearly-minimax-rate-breakdown point approximately
equal to $0.28$. Fourth, it is minimax rate optimal when data consist of
independent observations corrupted by adversarially chosen outliers. Fifth, it
is asymptotically optimal when the rate of contamination tends to zero. The
estimator is obtained by an iterative reweighting approach. Each sample point
is assigned a weight that is iteratively updated using a convex optimization
problem. We also establish a dimension-free non-asymptotic risk bound for the
expected error of the proposed estimator. It is the first of this kind results
in the literature and involves only the effective rank of the covariance
matrix.
"
"stat.TH","  In this paper, we investigate Gaussian process regression with input location
error, where the inputs are corrupted by noise. Here, we consider the best
linear unbiased predictor for two cases, according to whether there is noise at
the target untried location or not. We show that the mean squared prediction
error does not converge to zero in either case. We investigate the use of
stochastic Kriging in the prediction of Gaussian processes with input location
error, and show that stochastic Kriging is a good approximation when the sample
size is large. Several numeric examples are given to illustrate the results,
and a case study on the assembly of composite parts is presented. Technical
proofs are provided in the Appendix.
"
"stat.TH","  Bayesian optimization is a class of global optimization techniques. It
regards the underlying objective function as a realization of a Gaussian
process. Although the outputs of Bayesian optimization are random according to
the Gaussian process assumption, quantification of this uncertainty is rarely
studied in the literature. In this work, we propose a novel approach to assess
the output uncertainty of Bayesian optimization algorithms, in terms of
constructing confidence regions of the maximum point or value of the objective
function. These regions can be computed efficiently, and their confidence
levels are guaranteed by newly developed uniform error bounds for sequential
Gaussian process regression. Our theory provides a unified uncertainty
quantification framework for all existing sequential sampling policies and
stopping criteria.
"
"stat.TH","  This paper establishes a precise high-dimensional asymptotic theory for
boosting on separable data, taking statistical and computational perspectives.
We consider the setting where the number of features (weak learners) $p$ scales
with the sample size $n$, in an over-parametrized regime. Under a broad class
of statistical models, we provide an exact analysis of the generalization error
of boosting, when the algorithm interpolates the training data and maximizes
the empirical $\ell_1$-margin. The relation between the boosting test error and
the optimal Bayes error is pinned down explicitly. In turn, these precise
characterizations resolve several open questions raised in
\cite{breiman1999prediction, schapire1998boosting} surrounding boosting. On the
computational front, we provide a sharp analysis of the stopping time when
boosting approximately maximizes the empirical $\ell_1$ margin. Furthermore, we
discover that the larger the overparametrization ratio $p/n$, the smaller the
proportion of active features (with zero initialization), and the faster the
optimization reaches interpolation. At the heart of our theory lies an in-depth
study of the maximum $\ell_1$-margin, which can be accurately described by a
new system of non-linear equations; we analyze this margin and the properties
of this system, using Gaussian comparison techniques and a novel uniform
deviation argument. Variants of AdaBoost corresponding to general $\ell_q$
geometry, for $q > 1$, are also presented, together with an exact analysis of
the high-dimensional generalization and optimization behavior of a class of
these algorithms.
"
"stat.TH","  We revisit the model of heteroscedastic extremes initially introduced by
Einmahl et al. (JRSSB, 2016) to describe the evolution of a non stationary
sequence whose extremes evolve over time and adapt it into a general extreme
quantile regression framework. We provide estimates for the extreme value index
and the integrated skedasis function and prove their asymptotic normality. Our
results are quite similar to those developed for heteroscedastic extremes but
with a different proof approach emphasizing coupling arguments. We also propose
a pointwise estimator of the skedasis function and a Weissman estimator of the
conditional extreme quantile and prove the asymptotic normality of both
estimators.
"
"stat.TH","  In this paper, we analyze maximum Sharpe ratio when the number of assets in a
portfolio is larger than its time span. One obstacle in this large dimensional
setup is the singularity of the sample covariance matrix of the excess asset
returns. To solve this issue, we benefit from a technique called nodewise
regression, which was developed by Meinshausen and Buhlmann (2006). It provides
a sparse/weakly sparse and consistent estimate of the precision matrix, using
the Lasso method. We analyze three issues. One of the key results in our paper
is that mean-variance efficiency for the portfolios in large dimensions is
established. Then tied to that result, we also show that the maximum
out-of-sample Sharpe ratio can be consistently estimated in this large
portfolio of assets. Furthermore, we provide convergence rates and see that the
number of assets slow down the convergence up to a logarithmic factor. Then, we
provide consistency of maximum Sharpe Ratio when the portfolio weights add up
to one, and also provide a new formula and an estimate for constrained maximum
Sharpe ratio. Finally, we provide consistent estimates of the Sharpe ratios of
global minimum variance portfolio and Markowitz's (1952) mean variance
portfolio. In terms of assumptions, we allow for time series data. Simulation
and out-of-sample forecasting exercise shows that our new method performs well
compared to factor and shrinkage based techniques.
"
"stat.TH","  Except for certain parameter values, a closed form formula for the mode of
the generalized hyperbolic (GH) distribution is not available. In this paper,
we exploit results from the literature on modified Bessel functions and their
ratios to obtain simple but tight two-sided inequalities for the mode of the GH
distribution for general parameter values. As a special case, we deduce tight
two-sided inequalities for the mode of the variance-gamma (VG) distribution,
and through a similar approach we also obtain tight two-sided inequalities for
the mode of the McKay Type I distribution. The analogous problem for the median
is more challenging, but we conjecture some monotonicity results for the median
of the VG and McKay Type I distributions, from we which we conjecture some
tight two-sided inequalities for their medians. Numerical experiments support
these conjectures and also lead us to a conjectured tight lower bound for the
median of the GH distribution.
"
"stat.TH","  Dynamical systems describe the changes in processes that arise naturally from
their underlying physical principles, such as the laws of motion or the
conservation of mass, energy or momentum. These models facilitate a causal
explanation for the drivers and impediments of the processes. But do they
describe the behaviour of the observed data? And how can we quantify the
models' parameters that cannot be measured directly? This paper addresses these
two questions by providing a methodology for estimating the solution; and the
parameters of linear dynamical systems from incomplete and noisy observations
of the processes.
  The proposed procedure builds on the parameter cascading approach, where a
linear combination of basis functions approximates the implicitly defined
solution of the dynamical system. The systems' parameters are then estimated so
that this approximating solution adheres to the data. By taking advantage of
the linearity of the system, we have simplified the parameter cascading
estimation procedure, and by developing a new iterative scheme, we achieve fast
and stable computation.
  We illustrate our approach by obtaining a linear differential equation that
represents real data from biomechanics. Comparing our approach with popular
methods for estimating the parameters of linear dynamical systems, namely, the
non-linear least-squares approach, simulated annealing, parameter cascading and
smooth functional tempering reveals a considerable reduction in computation and
an improved bias and sampling variance.
"
"stat.TH","  Any traditional classification problem in general involves modelling
individual classes and in turn classification by evaluating the similarity of
the test set with the modelled classes. In this paper, we introduce another
approach that would find the differential information between two classes
rather than modelling individual classes separately. The classes are viewed on
a common frame of reference in which one class would have a constant variance,
unlike the other class which would have unequal variance along its basis
vectors which would capture the differential information of one class over the
other.This, when mathematically formulated, leads to the solution of Matrix
Pencil equation.The theory of binary classification was extended to a
multi-class scenario.This is borne out by illustrative examples on the
classification of the MNIST database.
"
"stat.TH","  We consider the robust algorithms for the $k$-means clustering problem where
a quantizer is constructed based on $N$ independent observations. Our main
results are median of means based non-asymptotic excess distortion bounds that
hold under the two bounded moments assumption in a general separable Hilbert
space. In particular, our results extend the renowned asymptotic result of
Pollard, 1981 who showed that the existence of two moments is sufficient for
strong consistency of an empirically optimal quantizer in $\mathbb{R}^d$. In a
special case of clustering in $\mathbb{R}^d$, under two bounded moments, we
prove matching (up to constant factors) non-asymptotic upper and lower bounds
on the excess distortion, which depend on the probability mass of the lightest
cluster of an optimal quantizer. Our bounds have the sub-Gaussian form, and the
proofs are based on the versions of uniform bounds for robust mean estimators.
"
"stat.TH","  In the last decade, after Pareto distribution has been validated for X-band
high-resolution maritime clutter returns, new detection schemes were designed,
and heuristics for constant false alarm rate (CFAR) processors appeared in the
literature. These schemes used the same form of adaptive thresholding that was
originally derived for detecting Swerling-I target in exponentially distributed
clutter. Such an approach to get a CFAR would affect the detection performance
when applied to different target and clutter models. Very recently, it has also
been reported that Generalized Pareto distribution fits best for the measured
Radar-cross-section (RCS) data of a SAAB aircraft. Therefore in the context of
Pareto Clutter, we pose a Pareto distributed target-fluctuating-model or
Pareto-Target (PT) aircraft detection problem as a two-sample, Pareto vs.
Pareto composite hypothesis testing problem. We solve this problem
systematically from the first principles of Neyman Pearson (NP)- lemma first to
simple vs. composite, and then for a more realistic composite vs. composite
while considering no knowledge of both scale and shape parameters of Pareto
distributed clutter. For the composite case, we derive the generalized
likelihood ratio test (GLRT) statistic and show that the GLRT test statistic is
a constant false alarm rate (CFAR) detector. We provide extensive simulation
results to demonstrate the performance of the proposed detectors.
"
"stat.TH","  This work studies finite-sample properties of the risk of the minimum-norm
interpolating predictor in high-dimensional regression models. If the effective
rank of the covariance matrix $\Sigma$ of the $p$ regression features is much
larger than the sample size $n$, we show that the min-norm interpolating
predictor is not desirable, as its risk approaches the risk of trivially
predicting the response by $0$. However, our detailed finite sample analysis
reveals, surprisingly, that this behavior is not present when the regression
response and the features are jointly low-dimensional, and follow a widely used
factor regression model. Within this popular model class, and when the
effective rank of $\Sigma$ is smaller than $n$, while still allowing for $p \gg
n$, both the bias and the variance terms of the excess risk can be controlled,
and the risk of the minimum-norm interpolating predictor approaches optimal
benchmarks. Moreover, through a detailed analysis of the bias term, we exhibit
model classes under which our upper bound on the excess risk approaches zero,
while the corresponding upper bound in the recent work arXiv:1906.11300v3
diverges. Furthermore, we show that minimum-norm interpolating predictors
analyzed under factor regression models, despite being model-agnostic, can have
similar risk to model-assisted predictors based on principal components
regression, in the high-dimensional regime.
"
"stat.TH","  Large-scale randomized experiments, sometimes called A/B tests, are
increasingly prevalent in many industries. Though such experiments are often
analyzed via frequentist $t$-tests, arguably such analyses are deficient:
$p$-values are hard to interpret and not easily incorporated into
decision-making. As an alternative, we propose an empirical Bayes approach,
which assumes that the treatment effects are realized from a ""true prior"". This
requires inferring the prior from previous experiments. Following Robbins, we
estimate a family of marginal densities of empirical effects, indexed by the
noise scale. We show that this family is characterized by the heat equation. We
develop a spectral maximum likelihood estimate based on a Fourier series
representation, which can be efficiently computed via convex optimization. In
order to select hyperparameters and compare models, we describe two model
selection criteria. We demonstrate our method on simulated and real data, and
compare posterior inference to that under a Gaussian mixture model of the
prior.
"
"stat.TH","  This paper concerns error bounds for recursive equations subject to Markovian
disturbances. Motivating examples abound within the fields of Markov chain
Monte Carlo (MCMC) and Reinforcement Learning (RL), and many of these
algorithms can be interpreted as special cases of stochastic approximation
(SA). It is argued that it is not possible in general to obtain a Hoeffding
bound on the error sequence, even when the underlying Markov chain is
reversible and geometrically ergodic, such as the M/M/1 queue. This is
motivation for the focus on mean square error bounds for parameter estimates.
It is shown that mean square error achieves the optimal rate of $O(1/n)$,
subject to conditions on the step-size sequence. Moreover, the exact constants
in the rate are obtained, which is of great value in algorithm design.
"
"stat.TH","  We introduce a general framework for defining equivalence and measuring
distances between time series, and a first concrete method for doing so. We
prove the existence of equivalence relations on the space of time series, such
that the quotient spaces can be equipped with a metrizable topology. We
illustrate algorithmically how to calculate such distances among a collection
of time series, and perform clustering analysis based on these distances. We
apply these insights to analyse the recent bushfires in NSW, Australia. There,
we introduce a new method to analyse time series in a cross-contextual setting.
"
"stat.TH","  Recently a distribution free approach for testing parametric hypotheses based
on unitary transformations has been suggested in \cite{Khm13, Khm16, Khm17} and
further studied in \cite{Ngu17} and \cite{Rob19}. In this note we show that the
transformation takes extremely simple form in distribution free testing of
linear regression. Then we extend it to general parametric regression with
vector-valued covariates.
"
"stat.TH","  There exist various types of network block models such as the Stochastic
Block Model (SBM), the Degree Corrected Block Model (DCBM), and the Popularity
Adjusted Block Model (PABM). While this leads to a variety of choices, the
block models do not have a nested structure. In addition, there is a
substantial jump in the number of parameters from the DCBM to the PABM. The
objective of this paper is formulation of a hierarchy of block model which does
not rely on arbitrary identifiability conditions, treats the SBM, the DCBM and
the PABM as its particular cases with specific parameter values and, in
addition, allows a multitude of versions that are more complicated than DCBM
but have fewer unknown parameters than the PABM. The latter allows one to carry
out clustering and estimation without preliminary testing to see which block
model is really true.
"
"stat.TH","  We develop a timescale synthesis-based probabilistic approach for the
modeling of locally stationary signals. Inspired by our previous work, the
model involves zero-mean, complex Gaussian wavelet coefficients, whose
distribution varies as a function of time by time dependent translations on the
scale axis. In a maximum a posteriori approach, we propose an estimator for the
model parameters, namely the time-varying scale translation and an underlying
power spectrum. The proposed approach is illustrated on a denoising example. It
is also shown that the model can handle locally stationary signals with fast
frequency variations, and provide in this case very sharp timescale
representations more concentrated than synchrosqueezed or reassigned wavelet
transform.
"
"stat.TH","  We propose a new unsupervised and non-parametric method to detect change
points in intricate quasi-periodic signals. The detection relies on optimal
transport theory combined with topological analysis and the bootstrap
procedure. The algorithm is designed to detect changes in virtually any
harmonic or a partially harmonic signal and is verified on three different
sources of physiological data streams. We successfully find abnormal or
irregular cardiac cycles in the waveforms for the six of the most frequent
types of clinical arrhythmias using a single algorithm. The validation and the
efficiency of the method are shown both on synthetic and on real time series.
Our unsupervised approach reaches the level of performance of the supervised
state-of-the-art techniques. We provide conceptual justification for the
efficiency of the method and prove the convergence of the bootstrap procedure
theoretically.
"
"stat.TH","  The estimation of information measures of continuous distributions based on
samples is a fundamental problem in statistics and machine learning. In this
paper, we analyze estimates of differential entropy in $K$-dimensional
Euclidean space, computed from a finite number of samples, when the probability
density function belongs to a predetermined convex family $\mathcal{P}$. First,
estimating differential entropy to any accuracy is shown to be infeasible if
the differential entropy of densities in $\mathcal{P}$ is unbounded, clearly
showing the necessity of additional assumptions. Subsequently, we investigate
sufficient conditions that enable confidence bounds for the estimation of
differential entropy. In particular, we provide confidence bounds for simple
histogram based estimation of differential entropy from a fixed number of
samples, assuming that the probability density function is Lipschitz continuous
with known Lipschitz constant and known, bounded support. Our focus is on
differential entropy, but we provide examples that show that similar results
hold for mutual information and relative entropy as well.
"
"stat.TH","  In this paper, we analyse classical variants of the Spectral Clustering (SC)
algorithm in the Dynamic Stochastic Block Model (DSBM). Existing results show
that, in the relatively sparse case where the expected degree grows
logarithmically with the number of nodes, guarantees in the static case can be
extended to the dynamic case and yield improved error bounds when the DSBM is
sufficiently smooth in time, that is, the communities do not change too much
between two time steps. We improve over these results by drawing a new link
between the sparsity and the smoothness of the DSBM: the more regular the DSBM
is, the more sparse it can be, while still guaranteeing consistent recovery. In
particular, a mild condition on the smoothness allows to treat the sparse case
with bounded degree. We also extend these guarantees to the normalized
Laplacian, and as a by-product of our analysis, we obtain to our knowledge the
best spectral concentration bound available for the normalized Laplacian of
matrices with independent Bernoulli entries.
"
"stat.TH","  We investigate the problem of algorithmic fairness in the case where
sensitive and non-sensitive features are available and one aims to generate
new, `oblivious', features that closely approximate the non-sensitive features,
and are only minimally dependent on the sensitive ones. We study this question
in the context of kernel methods. We analyze a relaxed version of the Maximum
Mean Discrepancy criterion which does not guarantee full independence but makes
the optimization problem tractable. We derive a closed-form solution for this
relaxed optimization problem and complement the result with a study of the
dependencies between the newly generated features and the sensitive ones. Our
key ingredient for generating such oblivious features is a Hilbert-space-valued
conditional expectation, which needs to be estimated from data. We propose a
plug-in approach and demonstrate how the estimation errors can be controlled.
While our techniques help reduce the bias, we would like to point out that no
post-processing of any dataset could possibly serve as an alternative to
well-designed experiments.
"
"stat.TH","  Single-cell RNA-seq data are challenging because of the sparseness of the
read counts, the tiny expression of many relevant genes, and the variability in
the efficiency of RNA extraction for different cells. We consider a simple
probabilistic model for read counts, based on a negative binomial distribution
for each gene, modified by a cell-dependent coefficient interpreted as an
extraction efficiency. We provide two alternative fast methods to estimate the
model parameters, together with the probability that a cell results in zero
read counts for a gene. This allows to measure genes co-expression and
differential expression in a novel way.
"
"stat.TH","  Learning from data in the presence of outliers is a fundamental problem in
statistics. In this work, we study robust statistics in the presence of
overwhelming outliers for the fundamental problem of subspace recovery. Given a
dataset where an $\alpha$ fraction (less than half) of the data is distributed
uniformly in an unknown $k$ dimensional subspace in $d$ dimensions, and with no
additional assumptions on the remaining data, the goal is to recover a succinct
list of $O(\frac{1}{\alpha})$ subspaces one of which is nontrivially correlated
with the planted subspace. We provide the first polynomial time algorithm for
the 'list decodable subspace recovery' problem, and subsume it under a more
general framework of list decoding over distributions that are ""certifiably
resilient"" capturing state of the art results for list decodable mean
estimation and regression.
"
"stat.TH","  Presented is an inductive formula for computing the exact moments of the
distribution of Pearson's correlation over permutation of a data sample. These
exact formulas for the moments open the door to the possibility of more precise
and computationally efficient methods of evaluating the p-value for a
hypothesis test of Pearson's correlation.
"
"stat.TH","  This paper studies some temporal dependence properties and addresses the
issue of parametric estimation for a class of state-dependent autoregressive
models for nonlinear time series in which we assume a stochastic autoregressive
coefficient depending on the first lagged value of the process itself. We call
such a model state-dependent first-order autoregressive process, (SDAR). We
introduce some assumptions under which this class of models is strictly
stationary and uniformly ergodic and we establish consistency and asymptotic
normality of the quasi-maximum likelihood estimator of the parameters. In order
to capture the potentiality of the model, we present an empirical application
to nonlinear time series provided by the weekly realized volatility extracted
from returns of some European financial indices. The comparison of forecasting
accuracy is made considering an alternative approach provided by a two-regime
SETAR model
"
"stat.TH","  The problem of mean square optimal estimation of linear functionals which
depend on the unobserved values of a periodically correlated stochastic
sequence is considered. The estimates are based on observations of the sequence
with a noise. Formulas for calculation the mean square errors and the spectral
characteristics of the optimal estimates of functionals are derived in the case
of spectral certainty, where the spectral densities of the sequences are
exactly known. Formulas that determine the least favorable spectral densities
and the minimax spectral characteristics are proposed in the case of spectral
uncertainty, where the spectral densities of the sequences are not exactly
known while some classes of admissible spectral densities are specified.
"
"stat.TH","  In this paper, we introduce a new three-parameter distribution based on the
combination of re-parametrization of the so-called EGNB2 and transmuted
exponential distributions. This combination aims to modify the transmuted
exponential distribution via the incorporation of an additional parameter,
mainly adding a high degree of flexibility on the mode and impacting the
skewness and kurtosis of the tail. We explore some mathematical properties of
this distribution including the hazard rate function, moments, the moment
generating function, the quantile function, various entropy measures and
(reversed) residual life functions. A statistical study investigates estimation
of the parameters using the method of maximum likelihood. The distribution
along with other existing distributions are fitted to two environmental data
sets and its superior performance is assessed by using some goodness-of-fit
tests. As a result, some environmental measures associated with these data are
obtained such as the return level and mean deviation about this level.
"
"stat.TH","  In this paper, we consider a model called CHARME (Conditional Heteroscedastic
Autoregressive Mixture of Experts), a class of generalized mixture of nonlinear
nonparametric AR-ARCH time series. Under certain Lipschitz-type conditions on
the autoregressive and volatility functions, we prove that this model is
stationary, ergodic and $\tau$-weakly dependent. These conditions are much
weaker than those presented in the literature that treats this model. Moreover,
this result forms the theoretical basis for deriving an asymptotic theory of
the underlying (non)parametric estimation, which we present for this model. As
an application, from the universal approximation property of neural networks
(NN), we develop a learning theory for the NN-based autoregressive functions of
the model, where the strong consistency and asymptotic normality of the
considered estimator of the NN weights and biases are guaranteed under weak
conditions.
"
"stat.TH","  We characterize the unbiasedness of the score function, viewed as an
inference function, for a class of finite mixture models. The models studied
represent the situation where there is a stratification of the observations in
a finite number of groups. We show that if the observations belonging to the
same group follow the same distribution and the K distributions associated with
each group are distinct elements of a sufficiently regular parametric family of
probability measures, then the score function for estimating the parameters
identifying the distribution of each group is unbiased. However, if one
introduces a mixture in the scenario described above, so that for some
observations it is only known that they belong to some of the groups with a
given probability (not all in { 0, 1}), then the score function becomes biased.
We argue then that under further mild regularity conditions, the maximum
likelihood estimate is not consistent.
"
"stat.TH","  In this paper, we investigate the extreme-value methodology, to propose an
improved estimator of the conditional tail expectation ($CTE$) for a loss
distribution with a finite mean but infinite variance. The present work
introduces a new estimator of the $CTE$ based on the bias-reduced estimators of
high quantile for heavy-tailed distributions. The asymptotic normality of the
proposed estimator is established and checked, in a simulation study. Moreover,
we compare, in terms of bias and mean squared error, our estimator with the
known old estimator.
"
"stat.TH","  We study the problem of community detection in multi-layer networks, where
pairs of nodes can be related in multiple modalities. We introduce a general
framework, i.e., mixture multi-layer stochastic block model (MMSBM), which
includes many earlier models as special cases. We propose a tensor-based
algorithm (TWIST) to reveal both global/local memberships of nodes, and
memberships of layers. We show that the TWIST procedure can accurately detect
the communities with small misclassification error as the number of nodes
and/or the number of layers increases. Numerical studies confirm our
theoretical findings. To our best knowledge, this is the first systematic study
on the mixture multi-layer networks using tensor decomposition. The method is
applied to two real datasets: worldwide trading networks and malaria parasite
genes networks, yielding new and interesting findings.
"
"stat.TH","  We study the Extended Kalman Filter in constant dynamics, offering a bayesian
perspective of stochastic optimization. We obtain high probability bounds on
the cumulative excess risk in an unconstrained setting. In order to avoid any
projection step we propose a two-phase analysis. First, for linear and logistic
regressions, we prove that the algorithm enters a local phase where the
estimate stays in a small region around the optimum. We provide explicit bounds
with high probability on this convergence time. Second, for generalized linear
regressions, we provide a martingale analysis of the excess risk in the local
phase, improving existing ones in bounded stochastic optimization. The EKF
appears as a parameter-free online algorithm with O(d^2) cost per iteration
that optimally solves some unconstrained optimization problems.
"
"stat.TH","  The widely claimed replicability crisis in science may lead to revised
standards of significance. The customary frequentist confidence intervals,
calibrated through hypothetical repetitions of the experiment that is supposed
to have produced the data at hand, rely on a feeble concept of replicability.
In particular, contradictory conclusions may be reached when a substantial
enlargement of the study is undertaken. To redefine statistical confidence in
such a way that inferential conclusions are non-contradictory, with large
enough probability, under enlargements of the sample, we give a new reading of
a proposal dating back to the 60's, namely Robbins' confidence sequences.
Directly bounding the probability of reaching, in the future, conclusions that
contradict the current ones, Robbins' confidence sequences ensure a clear-cut
form of replicability when inference is performed on accumulating data. Their
main frequentist property is easy to understand and to prove. We show that
Robbins' confidence sequences may be justified under various views of
inference: they are likelihood-based, can incorporate prior information, and
obey the strong likelihood principle. They are easy to compute, even when
inference is on a parameter of interest, especially using a closed-form
approximation from normal asymptotic theory.
"
"stat.TH","  We focus on the problem of establishing the optimal upper bounds on
generalized order statistics which are based on the underlying cdf belonging to
the family of distributions with decreasing failure rate and decreasing failure
rate on the average. This issue has been previously considered by Bieniek
[Projection bounds on expectations of generalized order statistics from DFR and
DFRA families, Statistics, 2006; 40: 339--351], who established upper
nonnegative mean-variance bounds with use of the projections of the
compositions of density functions of the uniform generalized order statistic
and the exponential distribution function onto the properly chosen convex
cones. In this paper we obtain possibly negative upper bounds, by improving the
zero bounds obtained by Bieniek for some particular cases of gOSs. We express
the bounds in the scale units generated by the central absolute moments of
arbitrary orders. We also describe the attainability conditions.
"
"stat.TH","  There is currently a gap in theory for point patterns that lie on the surface
of objects, with researchers focusing on patterns that lie in a Euclidean
space, typically planar and spatial data. Methodology for planar and spatial
data thus relies on Euclidean geometry and is therefore inappropriate for
analysis of point patterns observed in non-Euclidean spaces. Recently, there
has been extensions to the analysis of point patterns on a sphere, however,
many other shapes are left unexplored. This is in part due to the challenge of
defining the notion of stationarity for a point process existing on such a
space due to the lack of rotational and translational isometries. Here, we
construct functional summary statistics for Poisson processes defined on convex
shapes in three dimensions. Using the Mapping Theorem, a Poisson process can be
transformed from any convex shape to a Poisson process on the unit sphere which
has rotational symmetries that allow for functional summary statistics to be
constructed. We present the first and second order properties of such summary
statistics and demonstrate how they can be used to test whether an observed
pattern exhibits complete spatial randomness or spatial preference on the
original convex space. A study of the Type I and II errors of our test
statistics are explored through simulations on ellipsoids of varying
dimensions.
"
"stat.TH","  In an increasingly interconnected world, understanding and summarizing the
structure of these networks becomes increasingly relevant. However, this task
is nontrivial; proposed summary statistics are as diverse as the networks they
describe, and a standardized hierarchy has not yet been established. In
contrast, vector-valued random variables admit such a description in terms of
their cumulants (e.g., mean, (co)variance, skew, kurtosis). Here, we introduce
the natural analogue of cumulants for networks, building a hierarchical
description based on correlations between an increasing number of connections,
seamlessly incorporating additional information, such as directed edges, node
attributes, and edge weights. These graph cumulants provide a principled and
unifying framework for quantifying the propensity of a network to display any
substructure of interest (such as cliques to measure clustering). Moreover,
they give rise to a natural hierarchical family of maximum entropy models for
networks (i.e., ERGMs) that do not suffer from the ""degeneracy problem"", a
common practical pitfall of other ERGMs.
"
"stat.TH","  We introduce a novel procedure to perform Bayesian non-parametric inference
with right-censored data, the \emph{beta-Stacy bootstrap}. This approximates
the posterior law of summaries of the survival distribution (e.g. the mean
survival time), which is often difficult in the non-parametric case. More
precisely, our procedure approximates the joint posterior law of functionals of
the beta-Stacy process, a non-parametric process prior widely used in survival
analysis. It also represents the missing link that unifies other common
Bayesian bootstraps for complete or censored data based on non-parametric
priors. It is defined by an exact sampling algorithm that does not require
tuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy
bootstrap by analyzing survival data from a real clinical trial.
"
"stat.TH","  This paper is concerned with estimation and inference for the location of a
change point in the mean of independent high-dimensional data. Our change point
location estimator maximizes a new U-statistic based objective function, and
its convergence rate and asymptotic distribution after suitable centering and
normalization are obtained under mild assumptions. Our estimator turns out to
have better efficiency as compared to the least squares based counterpart in
the literature. Based on the asymptotic theory, we construct a confidence
interval by plugging in consistent estimates of several quantities in the
normalization. We also provide a bootstrap-based confidence interval and state
its asymptotic validity under suitable conditions. Through simulation studies,
we demonstrate favorable finite sample performance of the new change point
location estimator as compared to its least squares based counterpart, and our
bootstrap-based confidence intervals, as compared to several existing
competitors. The asymptotic theory based on high-dimensional U-statistic is
substantially different from those developed in the literature and is of
independent interest.
"
"stat.TH","  Finding anonymization mechanisms to protect personal data is at the heart of
recent machine learning research. Here, we consider the consequences of local
differential privacy constraints on goodness-of-fit testing, i.e. the
statistical problem assessing whether sample points are generated from a fixed
density $f_0$, or not. The observations are kept hidden and replaced by a
stochastic transformation satisfying the local differential privacy constraint.
In this setting, we propose a testing procedure which is based on an estimation
of the quadratic distance between the density $f$ of the unobserved samples and
$f_0$. We establish minimax separation rates for our test in the discrete and
continuous settings. To the best of our knowledge, we provide the first minimax
optimal test and associated private transformation under a local differential
privacy constraint over Besov balls in the continuous setting, quantifying the
price to pay for data privacy. We also present a test that is adaptive to the
smoothness parameter of the unknown density and remains minimax optimal up to a
logarithmic factor. Finally, we note that our results can be translated to the
discrete case, where the treatment of probability vectors is shown to be
equivalent to that of piecewise constant densities in our setting. That is why
we work with a unified setting for both the continuous and the discrete cases.
"
"stat.TH","  We investigate stochastic comparisons of lifetimes of series and parallel
systems with dependent and heterogeneous components having lifetimes following
the proportional odds (PO) model. The joint distribution of component lifetimes
is modeled by Archimedean survival copulas. We discuss some potential
applications of our findings on stochastic comparisons between lifetimes of two
series systems arising from random variables with associated random shocks.
"
"stat.TH","  Discretized Langevin diffusions are efficient Monte Carlo methods for
sampling from high dimensional target densities that are log-Lipschitz-smooth
and (strongly) log-concave. In particular, the Euclidean Langevin Monte Carlo
sampling algorithm has received much attention lately, leading to a detailed
understanding of its non-asymptotic convergence properties and of the role that
smoothness and log-concavity play in the convergence rate. Distributions that
do not possess these regularity properties can be addressed by considering a
Riemannian Langevin diffusion with a metric capturing the local geometry of the
log-density. However, the Monte Carlo algorithms derived from discretizations
of such Riemannian Langevin diffusions are notoriously difficult to analyze. In
this paper, we consider Langevin diffusions on a Hessian-type manifold and
study a discretization that is closely related to the mirror-descent scheme. We
establish for the first time a non-asymptotic upper-bound on the sampling error
of the resulting Hessian Riemannian Langevin Monte Carlo algorithm. This bound
is measured according to a Wasserstein distance induced by a Riemannian metric
ground cost capturing the Hessian structure and closely related to a
self-concordance-like condition. The upper-bound implies, for instance, that
the iterates contract toward a Wasserstein ball around the target density whose
radius is made explicit. Our theory recovers existing Euclidean results and can
cope with a wide variety of Hessian metrics related to highly non-flat
geometries.
"
"stat.TH","  In this paper, we introduce new indices adapted to outputs valued in general
metric spaces. This new class of indices encompasses the classical ones; in
particular, the so-called Sobol indices and the Cram{\'e}r-von-Mises indices.
Furthermore, we provide asymptotically Gaussian estimators of these indices
based on U-statistics. Surprisingly, we prove the asymp-totic normality
straightforwardly. Finally, we illustrate this new procedure on a toy model and
on two real-data examples.
"
"stat.TH","  The relative $\alpha$-entropy is the R\'enyi analog of relative entropy and
arises prominently in information-theoretic problems. Recent information
geometric investigations on this quantity have enabled the generalization of
the Cram\'{e}r-Rao inequality, which provides a lower bound for the variance of
an estimator of an escort of the underlying parametric probability
distribution. However, this framework remains unexamined in the Bayesian
framework. In this paper, we propose a general Riemannian metric based on
relative $\alpha$-entropy to obtain a generalized Bayesian Cram\'{e}r-Rao
inequality. This establishes a lower bound for the variance of an unbiased
estimator for the $\alpha$-escort distribution starting from an unbiased
estimator for the underlying distribution. We show that in the limiting case
when the entropy order approaches unity, this framework reduces to the
conventional Bayesian Cram\'{e}r-Rao inequality. Further, in the absence of
priors, the same framework yields the deterministic Cram\'{e}r-Rao inequality.
"
"stat.TH","  M-type smoothing splines are a broad class of spline estimators that include
the popular least-squares smoothing spline but also spline estimators that are
less susceptible to outlying observations and model-misspecification. However,
available asymptotic theory only covers smoothing spline estimators based on
smooth objective functions and consequently leaves out frequently used
resistant estimators such as quantile and Huber-type smoothing splines. We
provide a general treatment in this paper and, assuming only the convexity of
the objective function, show that the least-squares (super-)convergence rates
can be extended to M-type estimators whose asymptotic properties have not been
hitherto described. We further show that auxiliary scale estimates may be
handled under significantly weaker assumptions than those found in the
literature and we establish optimal rates of convergence for the derivatives,
which have not been obtained outside the least-squares framework. A simulation
study and a real-data example illustrate the competitive performance of
non-smooth M-type splines in relation to the least-squares spline on regular
data and their superior performance on data that contain anomalies.
"
"stat.TH","  A fundamental challenge in contextual bandits is to develop flexible,
general-purpose algorithms with computational requirements no worse than
classical supervised learning tasks such as classification and regression.
Algorithms based on regression have shown promising empirical success, but
theoretical guarantees have remained elusive except in special cases. We
provide the first universal and optimal reduction from contextual bandits to
online regression. We show how to transform any oracle for online regression
with a given value function class into an algorithm for contextual bandits with
the induced policy class, with no overhead in runtime or memory requirements.
We characterize the minimax rates for contextual bandits with general,
potentially nonparametric function classes, and show that our algorithm is
minimax optimal whenever the oracle obtains the optimal rate for regression.
Compared to previous results, our algorithm requires no distributional
assumptions beyond realizability, and works even when contexts are chosen
adversarially.
"
"stat.TH","  The Gaussian process (GP) model, which has been extensively applied as priors
of functions, has demonstrated excellent performance. The specification of a
large number of parameters affects the computational efficiency and the
feasibility of implementation of a control strategy. We propose a linear model
to approximate GPs; this model expands the GP model by a series of basis
functions. Several examples and simulation studies are presented to demonstrate
the advantages of the proposed method. A control strategy is provided with the
proposed linear model.
"
"stat.TH","  Recent work on policy learning from observational data has highlighted the
importance of efficient policy evaluation and has proposed reductions to
weighted (cost-sensitive) classification. But, efficient policy evaluation need
not yield efficient estimation of policy parameters. We consider the estimation
problem given by a weighted surrogate-loss classification reduction of policy
learning with any score function, either direct, inverse-propensity weighted,
or doubly robust. We show that, under a correct specification assumption, the
weighted classification formulation need not be efficient for policy
parameters. We draw a contrast to actual (possibly weighted) binary
classification, where correct specification implies a parametric model, while
for policy learning it only implies a semiparametric model. In light of this,
we instead propose an estimation approach based on generalized method of
moments, which is efficient for the policy parameters. We propose a particular
method based on recent developments on solving moment problems using neural
networks and demonstrate the efficiency and regret benefits of this method
empirically.
"
"stat.TH","  ""All models are wrong, but some are useful"", wrote George E. P. Box (1979).
Machine learning has focused on the usefulness of probability models for
prediction in social systems, but is only now coming to grips with the ways in
which these models are wrong---and the consequences of those shortcomings. This
paper attempts a comprehensive, structured overview of the specific conceptual,
procedural, and statistical limitations of models in machine learning when
applied to society. Machine learning modelers themselves can use the described
hierarchy to identify possible failure points and think through how to address
them, and consumers of machine learning models can know what to question when
confronted with the decision about if, where, and how to apply machine
learning. The limitations go from commitments inherent in quantification
itself, through to showing how unmodeled dependencies can lead to
cross-validation being overly optimistic as a way of assessing model
performance.
"
"stat.TH","  In this paper, we introduce principal asymmetric least squares (PALS) as a
unified framework for linear and nonlinear sufficient dimension reduction.
Classical methods such as sliced inverse regression (Li, 1991) and principal
support vector machines (Li, Artemiou and Li, 2011) may not perform well in the
presence of heteroscedasticity, while our proposal addresses this limitation by
synthesizing different expectile levels. Through extensive numerical studies,
we demonstrate the superior performance of PALS in terms of both computation
time and estimation accuracy. For the asymptotic analysis of PALS for linear
sufficient dimension reduction, we develop new tools to compute the derivative
of an expectation of a non-Lipschitz function.
"
"stat.TH","  A solution manifold is the collection of points in a $d$-dimensional space
satisfying a system of $s$ equations with $s<d$. Solution manifolds occur in
several statistical problems including hypothesis testing, curved-exponential
families, constrained mixture models, partial identifications, and
nonparametric set estimation. We analyze solution manifolds both theoretically
and algorithmically. In terms of theory, we derive five useful results: the
smoothness theorem, the stability theorem (which implies the consistency of a
plug-in estimator), the convergence of a gradient flow, the local center
manifold theorem and the convergence of the gradient descent algorithm. To
numerically approximate a solution manifold, we propose a Monte Carlo gradient
descent algorithm. In the case of likelihood inference, we design a manifold
constraint maximization procedure to find the maximum likelihood estimator on
the manifold. We also develop a method to approximate a posterior distribution
defined on a solution manifold.
"
"stat.TH","  This research shows that under certain mathematical conditions, a threshold
autoregressive model (TAR) can represent the leverage effect based on its
conditional variance function. Furthermore, the analytical expressions for the
third and fourth moment of the TAR model are obtained when it is weakly
stationary.
"
"stat.TH","  Utilization of multiple trajectories of a dynamical system model provides us
with several benefits in approximation of time series. For short term
predictions a high accuracy can be achieved via switches to new trajectory at
any time. Different long term trends (tendency to different stationary points)
of the phase portrait characterize various scenarios of the process realization
influenced by externalities. The dynamical system's phase portrait analysis
helps to see if the equations properly describe the reality. We also extend the
dynamical systems approach (discussed in \cite{R5}) to the dynamical systems
with external control.
  We illustrate these ideas with the help of new examples of the rental
properties HOMES.mil platform data. We also compare the qualitative properties
of HOMES.mil and Wikipedia.org platforms' phase portraits and the corresponding
differences of the two platforms' users. In our last example with COVID-19 data
we discuss the high accuracy of the short term prediction of confirmed
infection cases, recovery cases and death cases in various countries.
"
"stat.TH","  Spatial birth-death processes are generalisations of simple birth-death
processes, where the birth and death dynamics depend on the spatial locations
of individuals. In this article, we further let individuals move during their
life time according to a continuous Markov process. This generalisation, that
we call a spatial birth-death-move process, finds natural applications in
computer vision, bio-imaging and individual-based modelling in ecology. In a
first part, we verify that birth-death-move processes are well-defined
homogeneous Markov processes, we study their convergence to an invariant
measure and we establish their underlying martingale properties. In a second
part, we address the non-parametric estimation of their birth, death and total
intensity functions, in presence of continuous-time or discrete-time
observations. We introduce a kernel estimator that we prove to be consistent
under fairly simple conditions, in both settings. We also discuss how we can
take advantage of structural assumptions made on the intensity functions, and
we explain how bandwidth selection by likelihood cross-validation can be
conducted. A simulation study completes the theoretical results. We finally
apply our model to the analysis of the spatio-temporal dynamics of proteins
involved in exocytosis mechanisms in cells.
"
"stat.TH","  We propose and analyze a novel theoretical and algorithmic framework for
structured prediction. While so far the term has referred to discrete output
spaces, here we consider more general settings, such as manifolds or spaces of
probability measures. We define structured prediction as a problem where the
output space lacks a vectorial structure. We identify and study a large class
of loss functions that implicitly defines a suitable geometry on the problem.
The latter is the key to develop an algorithmic framework amenable to a sharp
statistical analysis and yielding efficient computations. When dealing with
output spaces with infinite cardinality, a suitable implicit formulation of the
estimator is shown to be crucial.
"
"stat.TH","  Data taking values on discrete sample spaces are the embodiment of modern
biological research. ""Omics"" experiments produce millions of symbolic outcomes
in the form of reads (i.e., DNA sequences of a few dozens to a few hundred
nucleotides). Unfortunately, these intrinsically non-numerical datasets are
often highly contaminated, and the possible sources of contamination are
usually poorly characterized. This contrasts with numerical datasets where
Gaussian-type noise is often well-justified. To overcome this hurdle, we
introduce the notion of latent weight, which measures the largest expected
fraction of samples from a contaminated probabilistic source that conform to a
model in a well-structured class of desired models. We examine various
properties of latent weights, which we specialize to the class of exchangeable
probability distributions. As proof of concept, we analyze DNA methylation data
from the 22 human autosome pairs. Contrary to what it is usually assumed, we
provide strong evidence that highly specific methylation patterns are
overrepresented at some genomic locations when contamination is taken into
account.
"
"stat.TH","  We study the problem of learning halfspaces with Massart noise in the
distribution-specific PAC model. We give the first computationally efficient
algorithm for this problem with respect to a broad family of distributions,
including log-concave distributions. This resolves an open question posed in a
number of prior works. Our approach is extremely simple: We identify a smooth
{\em non-convex} surrogate loss with the property that any approximate
stationary point of this loss defines a halfspace that is close to the target
halfspace. Given this structural result, we can use SGD to solve the underlying
learning problem.
"
"stat.TH","  In this article, we consider two forms of shrinkage estimators of the mean
$\theta$ of a multivariate normal distribution $X\sim N_{p}\left(\theta,
\sigma^{2}I_{p}\right)$ where $\sigma^{2}$ is unknown. We take the prior law
$\theta \sim N_{p}\left(\upsilon, \tau^{2}I_{p}\right)$ and we constuct a
Modified Bayes estimator $\delta_{B}^{\ast}$ and an Empirical Modified Bayes
estimator $\delta_{EB}^{\ast}$. We are interested in studying the minimaxity
and the limits of risks ratios of these estimators, to the maximum likelihood
estimator $X$, when $n$ and $p$ tend to infinity.
"
"stat.TH","  This paper studies the optimal rate of estimation in a finite Gaussian
location mixture model in high dimensions without separation conditions. We
assume that the number of components $k$ is bounded and that the centers lie in
a ball of bounded radius, while allowing the dimension $d$ to be as large as
the sample size $n$. Extending the one-dimensional result of Heinrich and Kahn
\cite{HK2015}, we show that the minimax rate of estimating the mixing
distribution in Wasserstein distance is $\Theta((d/n)^{1/4} + n^{-1/(4k-2)})$,
achieved by an estimator computable in time $O(nd^2+n^{5/4})$. Furthermore, we
show that the mixture density can be estimated at the optimal parametric rate
$\Theta(\sqrt{d/n})$ in Hellinger distance; however, no computationally
efficient algorithm is known to achieve the optimal rate.
  Both the theoretical and methodological development rely on a careful
application of the method of moments. Central to our results is the observation
that the information geometry of finite Gaussian mixtures is characterized by
the moment tensors of the mixing distribution, whose low-rank structure can be
exploited to obtain a sharp local entropy bound.
"
"stat.TH","  Upper and lower class functions for the maximum likelihood estimator of the
arrival and the service rates in a $GI/G/1$ queue are studied and the results
are verified for M/M/1 queue.
"
"stat.TH","  What does it mean to say that a quantity is identifiable from the data?
Statisticians seem to agree on a definition in the context of parametric
statistical models --- roughly, a parameter $\theta$ in a model $\mathcal{P} =
\{P_\theta: \theta \in \Theta\}$ is identifiable if the mapping $\theta \mapsto
P_\theta$ is injective. This definition raises important questions: Are
parameters the only quantities that can be identified? Is the concept of
identification meaningful outside of parametric statistics? Does it even
require the notion of a statistical model? Partial and idiosyncratic answers to
these questions have been discussed in econometrics, biological modeling, and
in some subfields of statistics like causal inference. This paper proposes a
unifying theory of identification that incorporates existing definitions for
parametric and nonparametric models and formalizes the process of
identification analysis. The applicability of this framework is illustrated
through a series of examples and two extended case studies.
"
"stat.TH","  Causal theory is now widely developed with many applications to medicine and
public health. However within the discipline of reliability, although causation
is a key concept in this field, there has been much less theoretical attention.
In this paper, we will demonstrate how some aspects of established causal
methodology can be translated via trees, and more specifically chain event
graphs, into domain of reliability theory to help the probability modeling of
failures. We further show how various domain specific concepts of causality
particular to reliability can be imported into more generic causal algebras and
so demonstrate how these disciplines can inform each other. This paper is
informed by a detailed analysis of maintenance records associated with a large
electrical distribution company. Causal hypotheses embedded within these
natural language texts are extracted and analyzed using the new graphical
framework we introduced here.
"
"stat.TH","  The log-concave projection is an operator that maps a d-dimensional
distribution P to an approximating log-concave density. Prior work by
D{\""u}mbgen et al. (2011) establishes that, with suitable metrics on the
underlying spaces, this projection is continuous, but not uniformly continuous.
In this work we prove a local uniform continuity result for log-concave
projection---in particular, establishing that this map is locally
H{\""o}lder-(1/4) continuous. A matching lower bound verifies that this exponent
cannot be improved. We also examine the implications of this continuity result
for the empirical setting---given a sample drawn from a distribution P, we
bound the squared Hellinger distance between the log-concave projection of the
empirical distribution of the sample, and the log-concave projection of P. In
particular, this yields interesting results for the misspecified setting, where
P is not itself log-concave.
"
"stat.TH","  As the main problem, we consider covering of a $d$-dimensional cube by $n$
balls with reasonably large $d$ (10 or more) and reasonably small $n$, like
$n=100$ or $n=1000$. We do not require the full coverage but only 90\% or 95\%
coverage. We establish that efficient covering schemes have several important
properties which are not seen in small dimensions and in asymptotical
considerations, for very large $n$. One of these properties can be termed `do
not try to cover the vertices' as the vertices of the cube and their close
neighbourhoods are very hard to cover and for large $d$ there are far too many
of them. We clearly demonstrate that, contrary to a common belief, placing
balls at points which form a low-discrepancy sequence in the cube, makes for a
very inefficient covering scheme. For a family of random coverings, we are able
to provide very accurate approximations to the coverage probability. We then
extend our results to the problems of coverage of a cube by smaller cubes and
quantization, the latter being also referred to as facility location. Along
with theoretical considerations and derivation of approximations, we discuss
results of a large-scale numerical investigation.
"
"stat.TH","  Classical change point analysis aims at (1) detecting abrupt changes in the
mean of a possibly non-stationary time series and at (2) identifying regions
where the mean exhibits a piecewise constant behavior. In many applications
however, it is more reasonable to assume that the mean changes gradually in a
smooth way. Those gradual changes may either be non-relevant (i.e., small), or
relevant for a specific problem at hand, and the present paper presents
statistical methodology to detect the latter. More precisely, we consider the
common nonparametric regression model $X_{i} = \mu (i/n) + \varepsilon_{i}$
with possibly non-stationary errors and propose a test for the null hypothesis
that the maximum absolute deviation of the regression function $\mu$ from a
functional $g (\mu )$ (such as the value $\mu (0)$ or the integral
$\int_{0}^{1} \mu (t) dt$) is smaller than a given threshold on a given
interval $[x_{0},x_{1}] \subseteq [0,1]$. A test for this type of hypotheses is
developed using an appropriate estimator, say $\hat d_{\infty, n}$, for the
maximum deviation $ d_{\infty}= \sup_{t \in [x_{0},x_{1}]} |\mu (t) - g( \mu)
|$. We derive the limiting distribution of an appropriately standardized
version of $\hat d_{\infty,n}$, where the standardization depends on the
Lebesgue measure of the set of extremal points of the function
$\mu(\cdot)-g(\mu)$. A refined procedure based on an estimate of this set is
developed and its consistency is proved. The results are illustrated by means
of a simulation study and a data example.
"
"stat.TH","  Monitoring several correlated quality characteristics of a process is common
in modern manufacturing and service processes. For this purpose, control charts
have been developed to detect assignable causes before producing nonconforming
products. Although a lot of attention has been paid to monitoring the
multivariate process mean, not many control charts are available for monitoring
the covariance matrix. This paper presents a comprehensive overview of the
literature on control charts for monitoring covariance matrix in multivariate
statistical process monitoring (MSPM) framework. It classifies the research
that have previously appeared in the literature. We highlight the challenging
areas for research and provide some directions for future research.
"
"stat.TH","  On 12 February 2020 the Royal Statistical Society hosted a meeting to discuss
the forthcoming paper ``Graphical models for extremes'' by Sebastian Engelke
and Adrien Hitz [arXiv:1812.01734]. This short note is a supplement to my
discussion contribution. It contains the proofs. It is shown that the
traditional notion of extremal independence agrees with the newly introduced
notion of extremal independence, which subsequently allows for a meaningful
interpretation of disconnected graphs in the context of the discussion paper.
The notation and references used in this note are adopted from the discussion
paper.
"
"stat.TH","  Three different inferential problems related to a two dimensional categorical
data from a Bayesian perspective have been discussed in this article. Conjugate
prior distribution with symmetric and asymmetric hyper parameters are
considered. Newly conceived asymmetric prior is based on perceived preferences
of categories. An extension of test of independence by introducing a notion of
measuring association between the parameters has been shown using correlation
matrix. Probabilities of different parametric combinations have been estimated
from the posterior distribution using closed form integration, Monte-Carlo
integration and MCMC methods to draw further inference from categorical data.
Bayesian computation is done using R programming language and illustrated with
appropriate data sets. Study has highlighted the application of Bayesian
inference exploiting the distributional form of underlying parameters.
"
"stat.TH","  Every philosophy has holes, and it is the responsibility of proponents of a
philosophy to point out these problems. Here are a few holes in Bayesian data
analysis: (1) the usual rules of conditional probability fail in the quantum
realm, (2) flat or weak priors lead to terrible inferences about things we care
about, (3) subjective priors are incoherent, (4) Bayesian decision picks the
wrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6)
for Cantorian reasons we need to check our models, but this destroys the
coherence of Bayesian inference. Some of the problems of Bayesian statistics
arise from people trying to do things they shouldn't be trying to do, but other
holes are not so easily patched. In particular, it may be a good idea to avoid
flat, weak, or conventional priors, but such advice, if followed, would go
against the vast majority of Bayesian practice and requires us to confront the
fundamental incoherence of Bayesian inference. This does not mean that we think
Bayesian inference is a bad idea, but it does mean that there is a tension
between Bayesian logic and Bayesian workflow which we believe can only be
resolved by considering Bayesian logic as a tool, a way of revealing inevitable
misfits and incoherences in our model assumptions, rather than as an end in
itself.
"
"stat.TH","  A central goal of causal inference is to detect and estimate the treatment
effects of a given treatment or intervention on an outcome variable of
interest, where a member known as the heterogeneous treatment effect (HTE) is
of growing popularity in recent practical applications such as the personalized
medicine. In this paper, we model the HTE as a smooth nonparametric difference
between two less smooth baseline functions, and determine the tight statistical
limits of the nonparametric HTE estimation as a function of the covariate
geometry. In particular, a two-stage nearest-neighbor-based estimator throwing
away observations with poor matching quality is near minimax optimal. We also
establish the tight dependence on the density ratio without the usual
assumption that the covariate densities are bounded away from zero, where a key
step is to employ a novel maximal inequality which could be of independent
interest.
"
"stat.TH","  A method of estimating the joint probability mass function of a pair of
discrete random variables is described. This estimator is used to construct the
conditional Shannon-R\'eyni-Tsallis entropies estimates. From there almost sure
rates of convergence and asymptotic normality are established. The theorical
results are validated by simulations.
"
"stat.TH","  Higher-order tensors arise frequently in applications such as neuroimaging,
recommendation system, social network analysis, and psychological studies. We
consider the problem of low-rank tensor estimation from possibly incomplete,
ordinal-valued observations. Two related problems are studied, one on tensor
denoising and another on tensor completion. We propose a multi-linear
cumulative link model, develop a rank-constrained M-estimator, and obtain
theoretical accuracy guarantees. Our mean squared error bound enjoys a faster
convergence rate than previous results, and we show that the proposed estimator
is minimax optimal under the class of low-rank models. Furthermore, the
procedure developed serves as an efficient completion method which guarantees
consistent recovery of an order-$K$ $(d,\ldots,d)$-dimensional low-rank tensor
using only $\tilde{\mathcal{O}}(Kd)$ noisy, quantized observations. We
demonstrate the outperformance of our approach over previous methods on the
tasks of clustering and collaborative filtering.
"
"stat.TH","  In finite population causal inference exact randomization tests can be
constructed for sharp null hypotheses, i.e. hypotheses which fully impute the
missing potential outcomes. Oftentimes inference is instead desired for the
weak null that the sample average of the treatment effects takes on a
particular value while leaving the subject-specific treatment effects
unspecified. Without proper care, tests valid for sharp null hypotheses may be
anti-conservative should only the weak null hold, creating the risk of
misinterpretation when randomization tests are deployed in practice. We develop
a general framework for unifying modes of inference for sharp and weak nulls,
wherein a single procedure simultaneously delivers exact inference for sharp
nulls and asymptotically valid inference for weak nulls. To do this, we employ
randomization tests based upon prepivoted test statistics, wherein a test
statistic is first transformed by a suitably constructed cumulative
distribution function and its randomization distribution assuming the sharp
null is then enumerated. For a large class of commonly employed test
statistics, we show that prepivoting may be accomplished by employing the
push-forward of a sample-based Gaussian measure based upon a suitably
constructed covariance estimator. In essence, the approach enumerates the
randomization distribution (assuming the sharp null) of a P-value for a
large-sample test known to be valid under the weak null, and uses the resulting
randomization distribution to perform inference. The versatility of the method
is demonstrated through a host of examples, including rerandomized designs and
regression-adjusted estimators in completely randomized designs.
"
"stat.TH","  $k$-means clustering is a fundamental problem in unsupervised learning. The
problem concerns finding a partition of the data points into $k$ clusters such
that the within-cluster variation is minimized. Despite its importance and wide
applicability, a theoretical understanding of the $k$-means problem has not
been completely satisfactory. Existing algorithms with theoretical performance
guarantees often rely on sophisticated (sometimes artificial) algorithmic
techniques and restricted assumptions on the data. The main challenge lies in
the non-convex nature of the problem; in particular, there exist additional
local solutions other than the global optimum. Moreover, the simplest and most
popular algorithm for $k$-means, namely Lloyd's algorithm, generally converges
to such spurious local solutions both in theory and in practice.
  In this paper, we approach the $k$-means problem from a new perspective, by
investigating the structures of these spurious local solutions under a
probabilistic generative model with $k$ ground truth clusters. As soon as
$k=3$, spurious local minima provably exist, even for well-separated and
balanced clusters. One such local minimum puts two centers at one true cluster,
and the third center in the middle of the other two true clusters. For general
$k$, one local minimum puts multiple centers at a true cluster, and one center
in the middle of multiple true clusters. Perhaps surprisingly, we prove that
this is essentially the only type of spurious local minima under a separation
condition. Our results pertain to the $k$-means formulation for mixtures of
Gaussians or bounded distributions. Our theoretical results corroborate
existing empirical observations and provide justification for several improved
algorithms for $k$-means clustering.
"
"stat.TH","  We consider the problem of combining data from observational and experimental
sources to make causal conclusions. This problem is increasingly relevant, as
the modern era has yielded passive collection of massive observational datasets
in areas such as e-commerce and electronic health. These data may be used to
supplement experimental data, which is frequently expensive to obtain. In
Rosenman et al. (2018), we considered this problem under the assumption that
all confounders were measured. Here, we relax the assumption of
unconfoundedness. To derive combined estimators with desirable properties, we
make use of results from the Stein Shrinkage literature. Our contributions are
threefold. First, we propose a generic procedure for deriving shrinkage
estimators in this setting, making use of a generalized unbiased risk estimate.
Second, we develop two new estimators, prove finite sample conditions under
which they have lower risk than an estimator using only experimental data, and
show that each achieves a notion of asymptotic optimality. Third, we draw
connections between our approach and results in sensitivity analysis, including
proposing a method for evaluating the feasibility of our estimators.
"
"stat.TH","  We consider estimation of a total causal effect from observational data via
covariate adjustment. Ideally, adjustment sets are selected based on a given
causal graph, reflecting knowledge of the underlying causal structure. Valid
adjustment sets are, however, not unique. Recent research has introduced a
graphical criterion for an 'optimal' valid adjustment set (O-set). For a given
graph, adjustment by the O-set yields the smallest asymptotic variance compared
to other adjustment sets in certain parametric and non-parametric models. In
this paper, we provide three new results on the O-set. First, we give a novel,
more intuitive graphical characterisation: We show that the O-set is the parent
set of the outcome node(s) in a suitable latent projection graph, which we call
the forbidden projection. An important property is that the forbidden
projection preserves all information relevant to total causal effect estimation
via covariate adjustment, making it a useful methodological tool in its own
right. Second, we extend the existing IDA algorithm to use the O-set, and argue
that the algorithm remains semi-local. This is implemented in the R-package
pcalg. Third, we present assumptions under which the O-set can be viewed as the
target set of popular non-graphical variable selection algorithms such as
stepwise backward selection.
"
"stat.TH","  Let $B^{a,b}:=\{B_t^{a,b},t\geq0\}$ be a weighted fractional Brownian motion
of parameters $a>-1$, $|b|<1$, $|b|<a+1$. We consider a least square-type
method to estimate the drift parameter $\theta>0$ of the weighted fractional
Ornstein-Uhlenbeck process $X:=\{X_t,t\geq0\}$ defined by $X_0=0; \ dX_t=\theta
X_tdt+dB_t^{a,b}$.
  In this work, we provide least squares-type estimators for $\theta$ based
continuous-time and discrete-time observations of $X$. The strong consistency
and the asymptotic behavior in distribution of the estimators are studied for
all $(a,b)$ such that $a>-1$, $|b|<1$, $|b|<a+1$. Here we extend the results of
\cite{SYY2,SYY} (resp. \cite{CSC}), where the strong consistency and the
asymptotic distribution of the estimators are proved for
  $-\frac12<a<0$, $-a<b<a+1$ (resp. $-1<a<0$, $-a<b<a+1$).
"
"stat.TH","  We prove the consistency of the Power-Law Fit PLFit method proposed by
Clauset et al.(2009) to estimate the power-law exponent in data coming from a
distribution function with regularly-varying tail. In the complex systems
community, PLFit has emerged as the method of choice to estimate the power-law
exponent. Yet, its mathematical properties are still poorly understood.
  The difficulty in PLFit is that it is a minimum-distance estimator. It first
chooses a threshold that minimizes the Kolmogorov-Smirnov distance between the
data points larger than the threshold and the Pareto tail, and then applies the
Hill estimator to this restricted data. Since the number of order statistics
used is random, the general theory of consistency of power-law exponents from
extreme value theory does not apply. Our proof consists in first showing that
the Hill estimator is consistent for general intermediate sequences for the
number of order statistics used, even when that number is random. Here, we call
a sequence intermediate when it grows to infinity, while remaining much smaller
than the sample size. The second, and most involved, step is to prove that the
optimizer in PLFit is with high probability an intermediate sequence, unless
the distribution has a Pareto tail above a certain value. For the latter
special case, we give a separate proof.
"
"stat.TH","  We study theoretically, for the first time, the Dirichlet kernel estimator
introduced by Aitchison & Lauder (1985) for the estimation of multivariate
densities supported on the $d$-dimensional simplex. The simplex is an important
case as it is the natural domain of compositional data and has been neglected
in the literature on asymmetric kernels. Dirichlet kernel estimators, which
generalize the unidimensional Beta kernel estimator from Chen (1999), are free
of boundary bias and non-negative everywhere on the simplex. We show that they
achieve the optimal convergence rate $O(n^{-4/(d+4)})$ for the mean squared
error and the mean integrated squared error, we prove their asymptotic
normality and uniform strong consistency, and we also find an asymptotic
expression for the mean absolute error.
"
"stat.TH","  A number of researchers have independently introduced topologies on the set
of laws of stochastic processes that extend the usual weak topology. Depending
on the respective scientific background this was motivated by applications and
connections to various areas (e.g. Plug-Pichler - stochastic programming,
Hellwig - game theory, Aldous - stability of optimal stopping, Hoover-Keisler -
model theory). Remarkably, all these seemingly independent approaches define
the same adapted weak topology in finite discrete time. Our first main result
is to construct an adapted variant of the empirical measure that consistently
estimates the laws of stochastic processes in full generality. A natural
compatible metric for the weak adapted topology is the given by an adapted
refinement of the Wasserstein distance, as established in the seminal works of
Pflug-Pichler. Specifically, the adapted Wasserstein distance allows to control
the error in stochastic optimization problems, pricing and hedging problems,
optimal stopping problems, etc. in a Lipschitz fashion. The second main result
of this article yields quantitative bounds for the convergence of the adapted
empirical measure with respect to adapted Wasserstein distance. Surprisingly,
we obtain virtually the same optimal rates and concentration results that are
known for the classical empirical measure wrt. Wasserstein distance.
"
"stat.TH","  We study convex empirical risk minimization for high-dimensional inference in
binary models. Our first result sharply predicts the statistical performance of
such estimators in the linear asymptotic regime under isotropic Gaussian
features. Importantly, the predictions hold for a wide class of convex loss
functions, which we exploit in order to prove a bound on the best achievable
performance among them. Notably, we show that the proposed bound is tight for
popular binary models (such as Signed, Logistic or Probit), by constructing
appropriate loss functions that achieve it. More interestingly, for binary
linear classification under the Logistic and Probit models, we prove that the
performance of least-squares is no worse than 0.997 and 0.98 times the optimal
one. Numerical simulations corroborate our theoretical findings and suggest
they are accurate even for relatively small problem dimensions.
"
"stat.TH","  A general shape identification inverse problem is studied in a Bayesian
framework. This problem requires the determination of the unknown shape of a
domain in the Euclidean space from finite-dimensional observation data with
some Gaussian random noise. Then, the stability of posterior is studied for
observation data. For each point of the space, the conditional probability that
the point is included in the unknown domain given the observation data is
considered. The stability is also studied for this probability distribution. As
a model problem for our inverse problem, a heat inverse problem is considered.
This problem requires the determination of the unknown shape of cavities in a
heat conductor from temperature data of some portion of the surface of the heat
conductor. To apply the above stability results to this model problem, one
needs the measurability and some boundedness of the forward operator. These
properties are shown.
"
"stat.TH","  Self-supervision is key to extending use of deep learning for label scarce
domains. For most of self-supervised approaches data transformations play an
important role. However, up until now the impact of transformations have not
been studied. Furthermore, different transformations may have different impact
on the system. We provide novel insights into the use of data transformation in
self-supervised tasks, specially pertaining to clustering. We show
theoretically and empirically that certain set of transformations are helpful
in convergence of self-supervised clustering. We also show the cases when the
transformations are not helpful or in some cases even harmful. We show faster
convergence rate with valid transformations for convex as well as certain
family of non-convex objectives along with the proof of convergence to the
original set of optima. We have synthetic as well as real world data
experiments. Empirically our results conform with the theoretical insights
provided.
"
"stat.TH","  Upon a consistent topological statistical theory the application of
structural statistics requires a quantification of the proximity structure of
model spaces. An important tool to study these structures are Pseudo-Riemannian
metrices, which in the category of statistical models are induced by
statistical divergences. The present article extends the notation of
topological statistical models by a differential structure to statistical
manifolds and introduces the differential geometric foundations to study
distribution families by their differential-, Riemannian- and symplectic
geometry.
"
"stat.TH","  In this paper, we aim to give a theoretical approximation for the penalty
level of $\ell_{1}$-regularization problems. This can save much time in
practice compared with the traditional methods, such as cross-validation. To
achieve this goal, we develop two Gaussian approximation methods, which are
based on a moderate deviation theorem and Stein's method respectively. Both of
them give efficient approximations and have good performances in simulations.
We apply the two Gaussian approximation methods into three types of ultra-high
dimensional $\ell_{1}$ penalized regressions: lasso, square-root lasso, and
weighted $\ell_{1}$ penalized Poisson regression. The numerical results
indicate that our two ways to estimate the penalty levels achieve high
computational efficiency. Besides, our prediction errors outperform that based
on the 10-fold cross-validation.
"
"stat.TH","  We prove the consistency of the $\ell_1$ penalized negative binomial
regression (NBR). A real data application about German health care demand shows
that the $\ell_1$ penalized NBR produces a more concise but more accurate
model, comparing to the classical NBR.
"
"stat.TH","  In the inverse Gaussian sequence space model with additional noisy
observations of the operator, we derive nonasymptotic minimax radii of testing
for ellipsoid-type alternatives simultaneously for both the signal detection
problem (testing against zero) and the goodness-of-fit testing problem (testing
against a prescribed sequence) without any regularity assumption on the null
hypothesis. The radii are the maximum of two terms, each of which only depends
on one of the noise levels. Interestingly, the term involving the noise level
of the operator explicitly depends on the null hypothesis and vanishes in the
signal detection case. The minimax radii are established by first showing a
lower bound for arbitrary null hypotheses and noise levels. For the upper bound
we consider two testing procedures, a direct test based on estimating the
energy in the image space and an indirect test. Under mild assumptions, we
prove that the testing radius of the indirect test achieves the lower bound,
which shows the minimax optimality of the radius and the test. We highlight the
assumptions under which the direct test also performs optimally. Furthermore,
we apply a classical Bonferroni method for making both the indirect and the
direct test adaptive with respect to the regularity of the alternative. The
radii of the adaptive tests are deteriorated by an additional log-factor, which
we show to be unavoidable. The results are illustrated considering Sobolev
spaces and mildly or severely ill-posed inverse problems.
"
"stat.TH","  Driven by a wide range of applications, many principal subspace estimation
problems have been studied individually under different structural constraints.
This paper presents a unified framework for the statistical analysis of a
general structured principal subspace estimation problem which includes as
special cases non-negative PCA/SVD, sparse PCA/SVD, subspace constrained
PCA/SVD, and spectral clustering. General minimax lower and upper bounds are
established to characterize the interplay between the information-geometric
complexity of the structural set for the principal subspaces, the
signal-to-noise ratio (SNR), and the dimensionality. The results yield
interesting phase transition phenomena concerning the rates of convergence as a
function of the SNRs and the fundamental limit for consistent estimation.
Applying the general results to the specific settings yields the minimax rates
of convergence for those problems, including the previous unknown optimal rates
for non-negative PCA/SVD, sparse SVD and subspace constrained PCA/SVD.
"
"stat.TH","  This paper considers maximum-a-posteriori (MAP) and linear discriminant based
MAP detectors to detect changes in the mean and covariance of a stochastic
input, driving specific network nodes, using noisy measurements from sensors
non-collocated with the input nodes. We explicitly characterize both detectors'
performance in terms of the network edge weights and input and sensor nodes'
location. In the asymptotic measurement regime, when the input and measurement
noise are jointly Gaussian, we show that the detectors' performance can be
studied using the input to output gain of the system's transfer function
matrix. Using this result, we obtain conditions for which the detection
performance associated with the sensors on a given network cut is better (or
worse) than that of the sensors associated with the subnetwork induced by the
cut and not containing the input nodes. Our results also provide structural
insights into the sensor placement from a detection-theoretic viewpoint. We
validate our theoretical findings via multiple numerical examples.
"
"stat.TH","  We develop a generic data-driven method for estimator selection in off-policy
policy evaluation settings. We establish a strong performance guarantee for the
method, showing that it is competitive with the oracle estimator, up to a
constant factor. Via in-depth case studies in contextual bandits and
reinforcement learning, we demonstrate the generality and applicability of the
method. We also perform comprehensive experiments, demonstrating the empirical
efficacy of our approach and comparing with related approaches. In both case
studies, our method compares favorably with existing methods.
"
"stat.TH","  Bernstein estimators are well-known to avoid the boundary bias problem of
traditional kernel estimators. The theoretical properties of these estimators
have been studied extensively on compact intervals and hypercubes, but never on
the simplex, except for the mean squared error of the density estimator in
Tenbusch (1994) when $d = 2$. The simplex is an important case as it is the
natural domain of compositional data. In this paper, we make an effort to prove
several asymptotic results (bias, variance, mean squared error (MSE), mean
integrated squared error (MISE), asymptotic normality, uniform strong
consistency) for Bernstein estimators of cumulative distribution functions and
density functions on the $d$-dimensional simplex. Our results generalize the
ones in Leblanc (2012) and Babu et al. (2002), who treated the case $d = 1$,
and significantly extend those found in Tenbusch (1994). In particular, our
rates of convergence for the MSE and MISE are optimal.
"
"stat.TH","  This article provides a strong law of large numbers for integration on
digital nets randomized by a nested uniform scramble. The motivating problem is
optimization over some variables of an integral over others, arising in
Bayesian optimization. This strong law requires that the integrand have a
finite moment of order $p$ for some $p>1$. Previously known results implied a
strong law only for Riemann integrable functions. Previous general weak laws of
large numbers for scrambled nets require a square integrable integrand. We
generalize from $L^2$ to $L^p$ for $p>1$ via the Riesz-Thorin interpolation
theorem
"
"stat.TH","  We propose an entropy-based information measure, namely the Discounted Least
Information Theory of Entropy (DLITE), which not only exhibits important
characteristics expected as an information measure but also satisfies
conditions of a metric. Classic information measures such as Shannon Entropy,
KL Divergence, and Jessen-Shannon Divergence have manifested some of these
properties while missing others. This work fills an important gap in the
advancement of information theory and its application, where related properties
are desirable.
"
"stat.TH","  This short review describes mathematical techniques for statistical analysis
and prediction in dynamical systems. Two problems are discussed, namely (i) the
supervised learning problem of forecasting the time evolution of an observable
under potentially incomplete observations at forecast initialization; and (ii)
the unsupervised learning problem of identification of observables of the
system with a coherent dynamical evolution. We discuss how ideas from from
operator-theoretic ergodic theory combined with statistical learning theory
provide an effective route to address these problems, leading to methods
well-adapted to handle nonlinear dynamics, with convergence guarantees as the
amount of training data increases.
"
"stat.TH","  We derive concentration inequalities for differentially private median and
mean estimators building on the ""Propose, Test, Release"" (PTR) mechanism
introduced by Dwork and Lei (2009). We introduce a new general version of the
PTR mechanism that allows us to derive high probability error bounds for
differentially private estimators. Our algorithms provide the first statistical
guarantees for differentially private estimation of the median and mean without
any boundedness assumptions on the data, and without assuming that the target
population parameter lies in some known bounded interval. Our procedures do not
rely on any truncation of the data and provide the first sub-Gaussian high
probability bounds for differentially private median and mean estimation, for
possibly heavy tailed random variables.
"
"stat.TH","  It has been known since Elliott (1998) that efficient methods of inference on
cointegrating relationships break down when autoregressive roots are near but
not exactly equal to unity. This paper addresses this problem within the
framework of a VAR with non-unit roots. We develop a characterisation of
cointegration, based on the impulse response function implied by the VAR, that
remains meaningful even when roots are not exactly unity. Under this
characterisation, the long-run equilibrium relationships between the series are
identified with a subspace associated to the largest characteristic roots of
the VAR. We analyse the asymptotics of maximum likelihood estimators of this
subspace, thereby generalising Johansen's (1995) treatment of the cointegrated
VAR with exactly unit roots. Inference is complicated by nuisance parameter
problems similar to those encountered in the context of predictive regressions,
and can be dealt with by approaches familiar from that setting.
"
"stat.TH","  We prove the existence and uniqueness of L^p bounded mild solutions for a
class of semilinear stochastic evolutions equations driven by a general class
of L\'evy processes without Gaussian component including both the non square
integrable and the square integrable cases on a probability space.
"
"stat.TH","  An important challenge in statistical analysis concerns the control of the
finite sample bias of estimators. This problem is magnified in high-dimensional
settings where the number of variables $p$ diverges with the sample size $n$,
as well as for nonlinear models and/or models with discrete data. For these
complex settings, we propose to use a general simulation-based approach and
show that the resulting estimator has a bias of order $\mathcal{O}(0)$, hence
providing an asymptotically optimal bias reduction. It is based on an initial
estimator that can be slightly asymptotically biased, making the approach very
generally applicable. This is particularly relevant when classical estimators,
such as the maximum likelihood estimator, can only be (numerically)
approximated. We show that the iterative bootstrap of Kuk (1995) provides a
computationally efficient approach to compute this bias reduced estimator. We
illustrate our theoretical results in simulation studies for which we develop
new bias reduced estimators for the logistic regression, with and without
random effects. These estimators enjoy additional properties such as robustness
to data contamination and to the problem of separability.
"
"stat.TH","  In Bayesian non-parametric density estimation a question of interest is how
the number of components in the model grows with the number of observations. We
state the growth rate of the number of components for a finite admixture model
both in expectation and in distribution. The tools we use in our analysis
combine the concept of a Choquet measure with classic results in stochastic
geometry on the number of extrema of random polytopes. We show that if our
admixture weights are probability vectors from a unit $(J-1)$-simplex then the
number of admixture components grows as $(\log n)^{J-1}$; in the standard
mixture case we recover the $\log n$ rate. We also state a central limit
theorem for the number of mixture components. In addition, we state the
convergence of the sequence of the empirical measures generated by our model to
the Choquet measure. Lastly, we relate our model to a classical non-parametric
density estimator based on a P\'olya tree.
"
"stat.TH","  The bias of the sample means of the arms in multi-armed bandits is an
important issue in adaptive data analysis that has recently received
considerable attention in the literature. Existing results relate in precise
ways the sign and magnitude of the bias to various sources of data adaptivity,
but do not apply to the conditional inference setting in which the sample means
are computed only if some specific conditions are satisfied. In this paper, we
characterize the sign of the conditional bias of monotone functions of the
rewards, including the sample mean. Our results hold for arbitrary conditioning
events and leverage natural monotonicity properties of the data collection
policy. We further demonstrate, through several examples from sequential
testing and best arm identification, that the sign of the conditional and
marginal bias of the sample mean of an arm can be different, depending on the
conditioning event. Our analysis offers new and interesting perspectives on the
subtleties of assessing the bias in data adaptive settings.
"
"stat.TH","  In this work, we study the event occurrences of user activities on online
social network platforms. To characterize the social activity interactions
among network users, we propose a network group Hawkes (NGH) process model.
Particularly, the observed network structure information is employed to model
the users' dynamic posting behaviors. Furthermore, the users are clustered into
latent groups according to their dynamic behavior patterns. To estimate the
model, a constraint maximum likelihood approach is proposed. Theoretically, we
establish the consistency and asymptotic normality of the estimators. In
addition, we show that the group memberships can be identified consistently. To
conduct estimation, a branching representation structure is firstly introduced,
and a stochastic EM (StEM) algorithm is developed to tackle the computational
problem. Lastly, we apply the proposed method to a social network data
collected from Sina Weibo, and identify the infuential network users as an
interesting application.
"
"stat.TH","  Pearson's correlation is one of the most widely used measures of association
today, the importance of which to modern science cannot be understated. Two of
the most common methods for computing the p-value for a hypothesis test of this
correlation method are a t-statistic and permutation sampling. When a dataset
comes from a bivariate normal distribution under specific data transformations
a t-statistic is exact. However, for datasets which do not follow this
stipulation, both approaches are merely estimations of the distribution of over
permutations of data. In this paper we explicitly show the dependency of the
permutation distribution of Pearson's correlation on the central moments of the
data and derive an inductive formula which allows the computation of these
exact moments. This has direct implications for computing the p-value for
general datasets which could lead to more computationally accurate methods.
"
"stat.TH","  Graphical model selection in Markov random fields is a fundamental problem in
statistics and machine learning. Two particularly prominent models, the Ising
model and Gaussian model, have largely developed in parallel using different
(though often related) techniques, and several practical algorithms with
rigorous sample complexity bounds have been established for each. In this
paper, we adapt a recently proposed algorithm of Klivans and Meka (FOCS, 2017),
based on the method of multiplicative weight updates, from the Ising model to
the Gaussian model, via non-trivial modifications to both the algorithm and its
analysis. The algorithm enjoys a sample complexity bound that is qualitatively
similar to others in the literature, has a low runtime $O(mp^2)$ in the case of
$m$ samples and $p$ nodes, and can trivially be implemented in an online
manner.
"
"stat.TH","  We introduce a constrained optimal transport problem where origins $x$ can
only be transported to destinations $y\geq x$. Our statistical motivation is to
describe the sharp upper bound for the variance of the treatment effect $Y-X$
given marginals when the effect is monotone, or $Y\geq X$. We thus focus on
supermodular costs (or submodular rewards) and introduce a coupling $P_{*}$
that is optimal for all such costs and yields the sharp bound. This coupling
admits manifold characterizations---geometric, order-theoretic, as optimal
transport, through the cdf, and via the transport kernel---that explain its
structure and imply useful bounds. When the first marginal is atomless, $P_{*}$
is concentrated on the graphs of two maps which can be described in terms of
the marginals, the second map arising due to the binding constraint.
"
"stat.TH","  In this paper, we introduce a computational framework for recovering a
high-resolution approximation of an unknown function from its low-resolution
indirect measurements as well as high-resolution training observations by
merging the frameworks of generalized sampling and functional principal
component analysis. In particular, we increase the signal resolution via a data
driven approach, which models the function of interest as a realization of a
random field and leverages a training set of observations generated via the
same underlying random process. We study the performance of the resulting
estimation procedure and show that high-resolution recovery is indeed possible
provided appropriate low-rank and angle conditions hold and provided the
training set is sufficiently large relative to the desired resolution.
Moreover, we show that the size of the training set can be reduced by
leveraging sparse representations of the functional principal components.
Furthermore, the effectiveness of the proposed reconstruction procedure is
illustrated by various numerical examples.
"
"stat.TH","  This paper deals with the problem of model selection for a general class of
integer-valued time series.
  We propose a penalized criterion based on the Poisson quasi-likelihood of the
model.
  Under certain regularity conditions, the consistency of the procedure as well
as the consistency and the asymptotic normality of the Poisson quasi-likelihood
estimator of the selected model are established.
  Simulation experiments are conducted for some classical models such as
Poisson, binary INGARCH and negative binomial model with nonlinear dynamic.
Also, an application to a real dataset is provided.
"
"stat.TH","  Statistical inference using pairwise comparison data has been an effective
approach to analyzing complex and sparse networks. In this paper we propose a
general framework for modeling the mutual interaction in a probabilistic
network, which enjoys ample flexibility in terms of parametrization. Within
this set-up, we establish that the maximum likelihood estimator (MLE) for the
latent scores of the subjects is uniformly consistent under a near-minimal
condition on network sparsity. This condition is sharp in terms of the leading
order asymptotics describing the sparsity. The proof utilizes a novel chaining
technique based on the error-induced metric as well as careful counting of
comparison graph structures. Our results guarantee that the MLE is a valid
estimator for inference in large-scale comparison networks where data is
asymptotically deficient. Numerical simulations are provided to complement the
theoretical analysis.
"
"stat.TH","  We propose a new family of specification tests called kernel conditional
moment (KCM) tests. Our tests are built on a novel representation of
conditional moment restrictions in a reproducing kernel Hilbert space (RKHS)
called conditional moment embedding (CMME). After transforming the conditional
moment restrictions into a continuum of unconditional counterparts, the test
statistic is defined as the maximum moment restriction (MMR) within the unit
ball of the RKHS. We show that the MMR not only fully characterizes the
original conditional moment restrictions, leading to consistency in both
hypothesis testing and parameter estimation, but also has an analytic
expression that is easy to compute as well as closed-form asymptotic
distributions. Our empirical studies show that the KCM test has a promising
finite-sample performance compared to existing tests.
"
"stat.TH","  Motivated by extreme value theory, max-linear Bayesian networks have been
recently introduced and studied as an alternative to linear structural equation
models. However, for max-linear systems the classical independence results for
Bayesian networks are far from exhausting valid conditional independence
statements. We use tropical linear algebra to derive a compact representation
of the conditional distribution given a partial observation, and exploit this
to obtain a complete description of all conditional independence relations. In
the context-specific case, where conditional independence is queried relative
to a specific value of the conditioning variables, we introduce the notion of a
source DAG to disclose the valid conditional independence relations. In the
context-free case we characterize conditional independence through a modified
separation concept, $\ast$-separation, combined with a tropical eigenvalue
condition. We also introduce the notion of an impact graph which describes how
extreme events spread deterministically through the network and we give a
complete characterization of such impact graphs. Our analysis opens up several
interesting questions concerning conditional independence and tropical
geometry.
"
"stat.TH","  We develop an extension of the Knockoff Inference procedure, introduced by
Barber and Candes (2015). This new method, called Aggregation of Multiple
Knockoffs (AKO), addresses the instability inherent to the random nature of
Knockoff-based inference. Specifically, AKO improves both the stability and
power compared with the original Knockoff algorithm while still maintaining
guarantees for False Discovery Rate control. We provide a new inference
procedure, prove its core properties, and demonstrate its benefits in a set of
experiments on synthetic and real datasets.
"
"stat.TH","  Stochastic gradient algorithm is a key ingredient of many machine learning
methods, particularly appropriate for large-scale learning.However, a major
caveat of large data is their incompleteness.We propose an averaged stochastic
gradient algorithm handling missing values in linear models. This approach has
the merit to be free from the need of any data distribution modeling and to
account for heterogeneous missing proportion.In both streaming and
finite-sample settings, we prove that this algorithm achieves convergence rate
of $\mathcal{O}(\frac{1}{n})$ at the iteration $n$, the same as without missing
values. We show the convergence behavior and the relevance of the algorithm not
only on synthetic data but also on real data sets, including those collected
from medical register.
"
"stat.TH","  We study generalised linear regression and classification for a synthetically
generated dataset encompassing different problems of interest, such as learning
with random features, neural networks in the lazy training regime, and the
hidden manifold model. We consider the high-dimensional regime and using the
replica method from statistical physics, we provide a closed-form expression
for the asymptotic generalisation performance in these problems, valid in both
the under- and over-parametrised regimes and for a broad choice of generalised
linear model loss functions. In particular, we show how to obtain analytically
the so-called double descent behaviour for logistic regression with a peak at
the interpolation threshold, we illustrate the superiority of orthogonal
against random Gaussian projections in learning with random features, and
discuss the role played by correlations in the data generated by the hidden
manifold model. Beyond the interest in these particular problems, the
theoretical formalism introduced in this manuscript provides a path to further
extensions to more complex tasks.
"
"stat.TH","  This note provides the Stein equation for weighted sums of independent
$\chi^{2}$ distributions.
"
"stat.TH","  In this paper, we consider the Whittle estimator for the parameters of a
stationary solution of a continuous-time linear state space model sampled at
low frequencies. In our context the driving process is a L\'evy process which
allows flexible margins of the underlying model. The L\'evy process is supposed
to have finite second moments. It is well known that then the class of
stationary solutions of linear state space models and the class of multivariate
CARMA processes coincides. We prove that the Whittle estimator, which is based
on the periodogram, is strongly consistent and asymptotically normally
distributed. A comparison with the classical setting of discrete-time ARMA
models shows that in the continuous-time setting the limit covariance matrix of
the Whittle estimator has an additional correction term for non-Gaussian
models. For the proof, we investigate as well the asymptotic normality of the
integrated periodogram which is interesting for its own. It can be used to
construct goodness of fit tests. Furthermore, for univariate state space
processes, which are CARMA processes, we introduce an adjusted version of the
Whittle estimator and derive as well the asymptotic properties of this
estimator. The practical applicability of our estimators is demonstrated
through a simulation study.
"
"stat.TH","  Many tools are available to bound the convergence rate of Markov chains in
total variation (TV) distance. Such results can be used to establish central
limit theorems (CLT) that enable error evaluations of Monte Carlo estimates in
practice. However, convergence analysis based on TV distance is often
non-scalable to high-dimensional Markov chains (Qin and Hobert (2018);
Rajaratnam and Sparks (2015)). Alternatively, robust bounds in Wasserstein
distance are often easier to obtain, thanks to a coupling argument. Our work is
concerned with the implication of such convergence results, in particular, do
they lead to CLTs of the corresponding Markov chains? One indirect and
typically non-trivial way is to first convert Wasserstein bounds into total
variation bounds. Alternatively, we provide two CLTs that directly depend on
(sub-geometric) convergence rates in Wasserstein distance. Our CLTs hold for
Lipschitz functions under certain moment conditions. Finally, we apply these
CLTs to four sets of Markov chain examples including a class of nonlinear
autoregressive processes, an exponential integrator version of the metropolis
adjusted Langevin algorithm (EI-MALA), an unadjusted Langevin algorithm (ULA),
and a special autoregressive model that generates reducible chains.
"
"stat.TH","  This paper studies the statistical theory of batch data reinforcement
learning with function approximation. Consider the off-policy evaluation
problem, which is to estimate the cumulative value of a new target policy from
logged history generated by unknown behavioral policies. We study a
regression-based fitted Q iteration method, and show that it is equivalent to a
model-based method that estimates a conditional mean embedding of the
transition operator. We prove that this method is information-theoretically
optimal and has nearly minimal estimation error. In particular, by leveraging
contraction property of Markov processes and martingale concentration, we
establish a finite-sample instance-dependent error upper bound and a
nearly-matching minimax lower bound. The policy evaluation error depends
sharply on a restricted $\chi^2$-divergence over the function class between the
long-term distribution of the target policy and the distribution of past data.
This restricted $\chi^2$-divergence is both instance-dependent and
function-class-dependent. It characterizes the statistical limit of off-policy
evaluation. Further, we provide an easily computable confidence bound for the
policy evaluator, which may be useful for optimistic planning and safe policy
improvement.
"
"stat.TH","  Interest in predicting multivariate probability distributions is growing due
to the increasing availability of rich datasets and computational developments.
Scoring functions enable the comparison of forecast accuracy, and can
potentially be used for estimation. A scoring function for multivariate
distributions that has gained some popularity is the energy score. This is a
generalization of the continuous ranked probability score (CRPS), which is
widely used for univariate distributions. A little-known, alternative
generalization is the multivariate CRPS (MCRPS). We propose a theoretical
framework for scoring functions for multivariate distributions, which
encompasses the energy score and MCRPS, as well as the quadratic score, which
has also received little attention. We demonstrate how this framework can be
used to generate new scores. For univariate distributions, it is
well-established that the CRPS can be expressed as the integral over a quantile
score. We show that, in a similar way, scoring functions for multivariate
distributions can be ""disintegrated"" to obtain scoring functions for level
sets. Using this, we present scoring functions for different types of level
set, including those for densities and cumulative distributions. To compute the
scoring functions, we propose a simple numerical algorithm. We illustrate our
proposals using simulated and stock returns data.
"
"stat.TH","  Sample- and computationally-efficient distribution estimation is a
fundamental tenet in statistics and machine learning. We present
$\mathrm{SURF}$, an algorithm for approximating distributions by piecewise
polynomials. $\mathrm{SURF}$ is simple, replacing existing general-purpose
optimization techniques by straight-forward approximation of each potential
polynomial piece by a simple empirical-probability interpolation, and using
plain divide-and-conquer to merge the pieces. It is universal, as well-known
low-degree polynomial-approximation results imply that it accurately
approximates a large class of common distributions. $\mathrm{SURF}$ is robust
to distribution mis-specification as for any degree $d\le 8$, it estimates any
distribution to an $\ell_1$ distance $ <3 $ times that of the nearest
degree-$d$ piecewise polynomial, improving known factor upper bounds of 3 for
single polynomials and 15 for polynomials with arbitrarily many pieces. It is
fast, using optimal sample complexity, and running in near sample-linear time.
In experiments, $\mathrm{SURF}$ significantly outperforms state-of-the art
algorithms.
"
"stat.TH","  Consider two random vectors $\mathbf C_1^{1/2}\mathbf x \in \mathbb R^p$ and
$\mathbf C_2^{1/2}\mathbf y\in \mathbb R^q$, where the entries of $\mathbf x$
and $\mathbf y$ are i.i.d. random variables with mean zero and variance one,
and $\mathbf C_1$ and $\mathbf C_2$ are $p \times p$ and $q\times q$
deterministic population covariance matrices. With $n$ independent samples of
$(\mathbf C_1^{1/2}\mathbf x,\mathbf C_2^{1/2}\mathbf y)$, we study the sample
correlation between these two vectors using canonical correlation analysis. We
denote by $S_{xx}$ and $S_{yy}$ the sample covariance matrices for $\mathbf
C_1^{1/2}\mathbf x$ and $\mathbf C_2^{1/2}\mathbf y$, respectively, and
$S_{xy}$ the sample cross-covariance matrix. Then the sample canonical
correlation coefficients are the square roots of the eigenvalues of the sample
canonical correlation matrix $\cal C_{XY}:=S_{xx}^{-1}S_{xy}S_{yy}^{-1}S_{yx}$.
Under the high-dimensional setting with ${p}/{n}\to c_1 \in (0, 1)$ and
${q}/{n}\to c_2 \in (0, 1-c_1)$ as $n\to \infty$, we prove that the largest
eigenvalue of $\mathcal C_{XY}$ converges to the Tracy-Widom distribution as
long as we have $\lim_{s \rightarrow \infty}s^4 [\mathbb{P}(\vert x_{ij} \vert
\geq s)+ \mathbb{P}(\vert y_{ij} \vert \geq s)]=0$. This extends the result in
[16], which established the Tracy-Widom limit of the largest eigenvalue of
$\mathcal C_{XY}$ under the assumption that all moments are finite. Our proof
is based on a linearization method, which reduces the problem to the study of a
$(p+q+2n)\times (p+q+2n)$ random matrix $H$. In particular, we shall prove an
optimal local law on its inverse $G:=H^{-1}$, i.e the resolvent. This local law
is the main tool for both the proof of the Tracy-Widom law in this paper, and
the study in [22,23] on the canonical correlation coefficients of
high-dimensional random vectors with finite rank correlations.
"
"stat.TH","  In modern data science, dynamic tensor data is prevailing in numerous
applications. An important task is to characterize the relationship between
such dynamic tensor and external covariates. However, the tensor data is often
only partially observed, rendering many existing methods inapplicable. In this
article, we develop a regression model with partially observed dynamic tensor
as the response and external covariates as the predictor. We introduce the
low-rank, sparsity and fusion structures on the regression coefficient tensor,
and consider a loss function projected over the observed entries. We develop an
efficient non-convex alternating updating algorithm, and derive the
finite-sample error bound of the actual estimator from each step of our
optimization algorithm. Unobserved entries in tensor response have imposed
serious challenges. As a result, our proposal differs considerably in terms of
estimation algorithm, regularity conditions, as well as theoretical properties,
compared to the existing tensor completion or tensor response regression
solutions. We illustrate the efficacy of our proposed method using simulations,
and two real applications, a neuroimaging dementia study and a digital
advertising study.
"
"stat.TH","  This paper develops the asymptotic theory for parametric and nonparametric
regression models when the errors have a fractional local to unity root (FLUR)
model structure. FLUR models are stationary time series with semi-long range
dependence property in the sense that their covariance function resembles that
of a long memory model for moderate lags but eventually diminishes
exponentially fast according to the presence of a decay factor governed by a
noncentrality parameter. When this parameter is sample size dependent, the
asymptotic normality for these regression models admit a wide range of
stochastic processes with behavior that includes long, semi-long, and short
memory processes.
"
"stat.TH","  We study the law of the iterated logarithm (Khinchin (1924), Kolmogorov
(1929)) and related strong invariance principles in stochastic geometry. As
potential applications, we think of well-known functionals such as functionals
defined on the $k$-nearest neighbors graph and important functionals in
topological data analysis such as the Euler characteristic and persistent Betti
numbers.
"
"stat.TH","  Eigenspaces of covariance matrices play an important role in statistical
machine learning, arising in variety of modern algorithms. Quantitatively, it
is convenient to describe the eigenspaces in terms of spectral projectors. This
work focuses on hypothesis testing for the spectral projectors, both in one-
and two-sample scenario. We present new tests, based on a specific matrix norm
developed in order to utilize the structure of the spectral projectors. A new
resampling technique of independent interest is introduced and analyzed: it
serves as an alternative to the well-known multiplier bootstrap, significantly
reducing computational complexity of bootstrap-based methods. We provide
theoretical guarantees for the type-I error of our procedures, which remarkably
improve the previously obtained results in the field. Moreover, we analyze
power of our tests. Numerical experiments illustrate good performance of the
proposed methods compared to previously developed ones.
"
"stat.TH","  We propose a new unit-root test based on Lagrange Multipliers, where we
extend the null hypothesis to an integrated moving-average process (IMA(1,1))
and the alternative to a first-order threshold autoregressive moving-average
process (TARMA(1,1)). This new theoretical framework provides tests with good
size without pre-modelling steps. Moreover, leveraging on the versatile
capability of the TARMA(1,1), our test has power against a wide range of linear
and nonlinear alternatives. We prove the consistency and asymptotic similarity
of the test. The proof of tightness of the test is of independent and general
theoretical interest. Moreover, we propose a wild bootstrap version of the
statistic. Our proposals outperform most existing tests in many contexts. We
support the view that rejection does not necessarily imply nonlinearity so that
unit-root tests should not be used uncritically to select a model. Finally, we
present an application to real exchange rates.
"
"stat.TH","  Inference on vertex-aligned graphs is of wide theoretical and practical
importance. There are, however, few flexible and tractable statistical models
for correlated graphs, and even fewer comprehensive approaches to parametric
inference on data arising from such graphs. In this paper, we consider the
correlated Bernoulli random graph model (allowing different Bernoulli
coefficients and edge correlations for different pairs of vertices), and we
introduce a new variance-reducing technique---called \emph{balancing}---that
can refine estimators for model parameters. Specifically, we construct a
disagreement statistic and show that it is complete and sufficient; balancing
can be interpreted as Rao-Blackwellization with this disagreement statistic. We
show that for unbiased estimators of functions of model parameters, balancing
generates uniformly minimum variance unbiased estimators (UMVUEs). However,
even when unbiased estimators for model parameters do {\em not} exist---which,
as we prove, is the case with both the heterogeneity correlation and the total
correlation parameters---balancing is still useful, and lowers mean squared
error. In particular, we demonstrate how balancing can improve the efficiency
of the alignment strength estimator for the total correlation, a parameter that
plays a critical role in graph matchability and graph matching runtime
complexity.
"
"stat.TH","  The single-index model is a statistical model for intrinsic regression where
the responses are assumed to depend on a single yet unknown linear combination
of the predictors, allowing to express the regression function as $ \mathbb{E}
[ Y | X ] = f ( \langle v , X \rangle ) $ for some unknown index vector $v$ and
link function $f$. Estimators converging at the $1$-dimensional min-max rate
exist, but their implementation has exponential cost in the ambient dimension.
Recent attempts at mitigating the computational cost yield estimators that are
computable in polynomial time, but do not achieve the optimal rate. Conditional
methods estimate the index vector $v$ by averaging moments of $X$ conditioned
on $Y$, but do not provide generalization bounds on $f$. In this paper we
develop an extensive non-asymptotic analysis of several conditional methods,
and propose a new one that combines some benefits of the existing approaches.
In particular, we establish $\sqrt{n}$-consistency for all conditional methods
considered. Moreover, we prove that polynomial partitioning estimates achieve
the $1$-dimensional min-max rate for regression of H\""older functions when
combined to any $\sqrt{n}$-consistent index estimator. Overall this yields an
estimator for dimension reduction and regression of single-index models that
attains statistical and computational optimality, thereby closing the
statistical-computational gap for this problem.
"
"stat.TH","  Canonical correlation analysis (CCA) has become a key tool for population
neuroimaging, allowing investigation of associations between many imaging and
non-imaging measurements. As other variables are often a source of variability
not of direct interest, previous work has used CCA on residuals from a model
that removes these effects, then proceeded directly to permutation inference.
We show that such a simple permutation test leads to inflated error rates. The
reason is that residualisation introduces dependencies among the observations
that violate the exchangeability assumption. Even in the absence of nuisance
variables, however, a simple permutation test for CCA also leads to excess
error rates for all canonical correlations other than the first. The reason is
that a simple permutation scheme does not ignore the variability already
explained by previous canonical variables. Here we propose solutions for both
problems: in the case of nuisance variables, we show that transforming the
residuals to a lower dimensional basis where exchangeability holds results in a
valid permutation test; for more general cases, with or without nuisance
variables, we propose estimating the canonical correlations in a stepwise
manner, removing at each iteration the variance already explained, while
dealing with different number of variables in both sides. We also discuss how
to address the multiplicity of tests, proposing an admissible test that is not
conservative, and provide a complete algorithm for permutation inference for
CCA.
"
"stat.TH","  We discuss parametric estimation of a degenerate diffusion system from
time-discrete observations. The first component of the degenerate diffusion
system has a parameter $\theta_1$ in a non-degenerate diffusion coefficient and
a parameter $\theta_2$ in the drift term. The second component has a drift term
parameterized by $\theta_3$ and no diffusion term. Asymptotic normality is
proved in three different situations for an adaptive estimator for $\theta_3$
with some initial estimators for ($\theta_1$ , $\theta_2$), an adaptive
one-step estimator for ($\theta_1$ , $\theta_2$ , $\theta_3$) with some initial
estimators for them, and a joint quasi-maximum likelihood estimator for
($\theta_1$ , $\theta_2$ , $\theta_3$) without any initial estimator. Our
estimators incorporate information of the increments of both components. Thanks
to this construction, the asymptotic variance of the estimators for $\theta_1$
is smaller than the standard one based only on the first component. The
convergence of the estimators for $\theta_3$ is much faster than the other
parameters. The resulting asymptotic variance is smaller than that of an
estimator only using the increments of the second component.
"
"stat.TH","  We present a novel approach to test for heteroscedasticity of a
non-stationary time series that is based on Gini's mean difference of
logarithmic local sample variances. In order to analyse the large sample
behaviour of our test statistic, we establish new limit theorems for
U-statistics of dependent triangular arrays. We derive the asymptotic
distribution of the test statistic under the null hypothesis of a constant
variance and show that the test is consistent against a large class of
alternatives, including multiple structural breaks in the variance. Our test is
applicable even in the case of non-stationary processes, assuming a locally
stationary mean function. The performance of the test and its comparatively low
computation time are illustrated in an extensive simulation study. As an
application, we analyse data from civil engineering, monitoring crack widths in
concrete bridge surfaces.
"
"stat.TH","  We study the linear ill-posed inverse problem with noisy data in the
statistical learning setting. Approximate reconstructions from random noisy
data are sought with general regularization schemes in Hilbert scale. We
discuss the rates of convergence for the regularized solution under the prior
assumptions and a certain link condition. We express the error in terms of
certain distance functions. For regression functions with smoothness given in
terms of source conditions the error bound can then be explicitly established.
"
"stat.TH","  We propose two nonparametric statistical tests of goodness of fit for
conditional distributions: given a conditional probability density function
$p(y|x)$ and a joint sample, decide whether the sample is drawn from
$p(y|x)r_x(x)$ for some density $r_x$. Our tests, formulated with a Stein
operator, can be applied to any differentiable conditional density model, and
require no knowledge of the normalizing constant. We show that 1) our tests are
consistent against any fixed alternative conditional model; 2) the statistics
can be estimated easily, requiring no density estimation as an intermediate
step; and 3) our second test offers an interpretable test result providing
insight on where the conditional model does not fit well in the domain of the
covariate. We demonstrate the interpretability of our test on a task of
modeling the distribution of New York City's taxi drop-off location given a
pick-up point. To our knowledge, our work is the first to propose such
conditional goodness-of-fit tests that simultaneously have all these desirable
properties.
"
"stat.TH","  In survival analysis, the lifetime under study is not always observed. In
certain applications, for some individuals, the value of the lifetime is only
known to be smaller or larger than some random duration. This framework
represent an extension of standard situations where the lifetime is only left
or only right randomly censored. We consider the case where the independent
observation units include also some covariates, and we propose two
semiparametric regression models. The new models extend the standard Cox
proportional hazard model to the situation of a more complex censoring
mechanism. However, like in Cox's model, in both models the nonparametric
baseline hazard function still could be expressed as an explicit functional of
the distribution of the observations. This allows to define the estimator of
the finite-dimensional parameters as the maximum of a likelihood-type criterion
which is an explicit function of the data. Given an estimate of the
finite-dimensional parameter, the estimation of the baseline cumulative hazard
function is straightforward.
"
"stat.TH","  Outcome-dependent sampling designs are common in many different scientific
fields including epidemiology, ecology, and economics. As with all
observational studies, such designs often suffer from unmeasured confounding,
which generally precludes the nonparametric identification of causal effects.
Nonparametric bounds can provide a way to narrow the range of possible values
for a nonidentifiable causal effect without making additional untestable
assumptions. The nonparametric bounds literature has almost exclusively focused
on settings with random sampling, and the bounds have often been derived with a
particular linear programming method. We derive novel bounds for the causal
risk difference, often referred to as the average treatment effect, in six
settings with outcome-dependent sampling and unmeasured confounding for a
binary outcome and exposure. Our derivations of the bounds illustrate two
approaches that may be applicable in other settings where the bounding problem
cannot be directly stated as a system of linear constraints. We illustrate our
derived bounds in a real data example involving the effect of vitamin D
concentration on mortality.
"
"stat.TH","  The statistical analysis of Randomized Numerical Linear Algebra (RandNLA)
algorithms within the past few years has mostly focused on their performance as
point estimators. However, this is insufficient for conducting statistical
inference, e.g., constructing confidence intervals and hypothesis testing,
since the distribution of the estimator is lacking. In this article, we develop
an asymptotic analysis to derive the distribution of RandNLA sampling
estimators for the least-squares problem. In particular, we derive the
asymptotic distribution of a general sampling estimator with arbitrary sampling
probabilities. The analysis is conducted in two complementary settings, i.e.,
when the objective of interest is to approximate the full sample estimator or
is to infer the underlying ground truth model parameters. For each setting, we
show that the sampling estimator is asymptotically normally distributed under
mild regularity conditions. Moreover, the sampling estimator is asymptotically
unbiased in both settings. Based on our asymptotic analysis, we use two
criteria, the Asymptotic Mean Squared Error (AMSE) and the Expected Asymptotic
Mean Squared Error (EAMSE), to identify optimal sampling probabilities. Several
of these optimal sampling probability distributions are new to the literature,
e.g., the root leverage sampling estimator and the predictor length sampling
estimator. Our theoretical results clarify the role of leverage in the sampling
process, and our empirical results demonstrate improvements over existing
methods.
"
"stat.TH","  The goal of this short note is to provide simple proofs for the ""folklore
facts"" on the sample complexity of learning a discrete probability distribution
over a known domain of size $k$ to various distances $\varepsilon$, with error
probability $\delta$.
"
"stat.TH","  We study the central limit theorem for sums of independent tensor powers,
$\frac{1}{\sqrt{d}}\sum\limits_{i=1}^d X_i^{\otimes p}$. We focus on the
high-dimensional regime where $X_i \in \mathbb{R}^n$ and $n$ may scale with
$d$. Our main result is a proposed threshold for convergence. Specifically, we
show that, under some regularity assumption, if $n^{2p-1}\gg d$, then the
normalized sum converges to a Gaussian. The results apply, among others, to
symmetric uniform log-concave measures and to product measures. This
generalizes several results found in the literature.
  Our main technique is a novel application of optimal transport to Stein's
method which accounts for the low dimensional structure which is inherent in
$X_i^{\otimes p}$.
"
"stat.TH","  This paper deals with non-parametric density estimation on $\bR^2$ from i.i.d
observations. It is assumed that after unknown rotation of the coordinate
system the coordinates of the observations are independent random variables
whose densities belong to a H\""older class with unknown parameters. The minimax
and adaptive minimax theories for this structural statistical model are
developed.
"
"stat.TH","  In many applications, data is collected in batches, some of which are corrupt
or even adversarial. Recent work derived optimal robust algorithms for
estimating discrete distributions in this setting. We consider a general
framework of robust learning from batches, and determine the limits of both
classification and distribution estimation over arbitrary, including
continuous, domains. Building on these results, we derive the first robust
agnostic computationally-efficient learning algorithms for piecewise-interval
classification, and for piecewise-polynomial, monotone, log-concave, and
gaussian-mixture distribution estimation.
"
"stat.TH","  The i.i.d. censoring model for survival analysis assumes two independent
sequences of i.i.d. positive random variables, $(T_i^*)_{1\le i\le n}$ and
$(U_i)_{1\le i\le n}$. The data consists of observations on the random sequence
$\big(T_i=\min(T_i^*,U_i)$ together with accompanying censor indicators. Values
of $T_i$ with $T_i^*\le U_i$ are said to be uncensored, those with $T_i^*> U_i$
are censored. We assume that the distributions of the $T_i^*$ and $U_i$ are in
the domain of attraction of the Gumbel distribution and obtain the asymptotic
distributions, as sample size $n\to\infty$, of the maximum values of the
censored and uncensored lifetimes in the data, and of statistics related to
them. These enable us to examine questions concerning the possible existence of
cured individuals in the population.
"
"stat.TH","  This paper describes a flexible framework for generalized low-rank tensor
estimation problems that includes many important instances arising from
applications in computational imaging, genomics, and network analysis. The
proposed estimator consists of finding a low-rank tensor fit to the data under
generalized parametric models. To overcome the difficulty of non-convexity in
these problems, we introduce a unified approach of projected gradient descent
that adapts to the underlying low-rank structure. Under mild conditions on the
loss function, we establish both an upper bound on statistical error and the
linear rate of computational convergence through a general deterministic
analysis. Then we further consider a suite of generalized tensor estimation
problems, including sub-Gaussian tensor denoising, tensor regression, and
Poisson and binomial tensor PCA. We prove that the proposed algorithm achieves
the minimax optimal rate of convergence in estimation error. Finally, we
demonstrate the superiority of the proposed framework via extensive experiments
on both simulated and real data.
"
"stat.TH","  This paper addresses two fundamental features of quantities modeled and
analysed in statistical science, their dimensions (e.g. time) and measurement
scales (units). Examples show that subtle issues can arise when dimensions and
measurement scales are ignored. Special difficulties arise when the models
involve transcendental functions. A transcendental function important in
statistics is the logarithm which is used in likelihood calculations and is a
singularity in the family of Box-Cox algebraic functions. Yet neither the
argument of the logarithm nor its value can have units of measurement. Physical
scientists have long recognized that dimension/scale difficulties can be
side-stepped by nondimensionalizing the model; after all, models of natural
phenomena cannot depend on the units by which they are measured, and the
celebrated Buckingham Pi theorem is a consequence. The paper reviews that
theorem, recognizing that the statistical invariance principle arose with
similar aspirations. However, the potential relationship between the theorem
and statistical invariance has not been investigated until very recently. The
main result of the paper is an exploration of that link, which leads to an
extension of the Pi-theorem that puts it in a stochastic framework and thus
quantifies uncertainties in deterministic physical models.
"
"stat.TH","  In observational studies, weighting methods that directly optimize the
balance between treatment and covariates have received much attention lately;
however these have mainly focused on binary treatments. Inspired by domain
adaptation, we show that such methods can be actually reformulated as specific
implementations of a discrepancy minimization problem aimed at tackling a shift
of distribution from observational to interventional data. More precisely, we
introduce a new framework, Covariate Balance via Discrepancy Minimization
(CBDM), that provably encompasses most of the existing balancing weight methods
and formally extends them to treatments of arbitrary types (e.g., continuous or
multivariate). We establish theoretical guarantees for our framework that both
offer generalizations of properties known when the treatment is binary, and
give a better grasp on what hyperparameters to choose in non-binary settings.
Based on such insights, we propose a particular implementation of CBDM for
estimating dose-response curves and demonstrate through experiments its
competitive performance relative to other existing approaches for continuous
treatments.
"
"stat.TH","  Bandit learning algorithms typically involve the balance of exploration and
exploitation. However, in many practical applications, worst-case scenarios
needing systematic exploration are seldom encountered. In this work, we
consider a smoothed setting for structured linear contextual bandits where the
adversarial contexts are perturbed by Gaussian noise and the unknown parameter
$\theta^*$ has structure, e.g., sparsity, group sparsity, low rank, etc. We
propose simple greedy algorithms for both the single- and multi-parameter
(i.e., different parameter for each context) settings and provide a unified
regret analysis for $\theta^*$ with any assumed structure. The regret bounds
are expressed in terms of geometric quantities such as Gaussian widths
associated with the structure of $\theta^*$. We also obtain sharper regret
bounds compared to earlier work for the unstructured $\theta^*$ setting as a
consequence of our improved analysis. We show there is implicit exploration in
the smoothed setting where a simple greedy algorithm works.
"
"stat.TH","  Engineers and computational scientists often study the behavior of their
simulations by repeated solutions with variations in their parameters, which
can be for instance boundary values or initial conditions. Through such
simulation ensembles, uncertainty in a solution is studied as a function of the
various input parameters. Solutions of numerical simulations are often temporal
functions, spatial maps or spatio-temporal outputs. The usual way to deal with
such complex outputs is to limit the analysis to several probes in the
temporal/spatial domain. This leads to smaller and more tractable ensembles of
functional outputs (curves) with their associated input parameters: augmented
ensembles of curves. This article describes a system for the interactive
exploration and analysis of such augmented ensembles. Descriptive statistics on
the functional outputs are performed by Principal Component Analysis
projection, kernel density estimation and the computation of High Density
Regions. This makes possible the calculation of functional quantiles and
outliers. Brushing and linking the elements of the system allows in-depth
analysis of the ensemble. The system allows for functional descriptive
statistics, cluster detection and finally for the realization of a visual
sensitivity analysis via cobweb plots. We present two synthetic examples and
then validate our approach in an industrial use-case concerning a marine
current study using a hydraulic solver.
"
"stat.TH","  We consider a high-dimensional mixture of two Gaussians in the noisy regime
where even an oracle knowing the centers of the clusters misclassifies a small
but finite fraction of the points. We provide a rigorous analysis of the
generalization error of regularized convex classifiers, including ridge, hinge
and logistic regression, in the high-dimensional limit where the number $n$ of
samples and their dimension $d$ go to infinity while their ratio is fixed to
$\alpha= n/d$. We discuss surprising effects of the regularization that in some
cases allows to reach the Bayes-optimal performances. We also illustrate the
interpolation peak at low regularization, and analyze the role of the
respective sizes of the two clusters.
"
"stat.TH","  Sparse linear regression methods generally have a free hyperparameter which
controls the amount of sparsity, and is subject to a bias-variance tradeoff.
This article considers the use of Aggregated hold-out to aggregate over values
of this hyperparameter, in the context of linear regression with the Huber loss
function. Aggregated hold-out (Agghoo) is a procedure which averages estimators
selected by hold-out (cross-validation with a single split). In the theoretical
part of the article, it is proved that Agghoo satisfies a non-asymptotic oracle
inequality when it is applied to sparse estimators which are parametrized by
their zero-norm. In particular , this includes a variant of the Lasso
introduced by Zou, Hasti{\'e} and Tibshirani. Simulations are used to compare
Agghoo with cross-validation. They show that Agghoo performs better than CV
when the intrinsic dimension is high and when there are confounders correlated
with the predictive covariates.
"
"stat.TH","  The profile of a sample is the multiset of its symbol frequencies. We show
that for samples of discrete distributions, profile entropy is a fundamental
measure unifying the concepts of estimation, inference, and compression.
Specifically, profile entropy a) determines the speed of estimating the
distribution relative to the best natural estimator; b) characterizes the rate
of inferring all symmetric properties compared with the best estimator over any
label-invariant distribution collection; c) serves as the limit of profile
compression, for which we derive optimal near-linear-time block and sequential
algorithms. To further our understanding of profile entropy, we investigate its
attributes, provide algorithms for approximating its value, and determine its
magnitude for numerous structural distribution families.
"
"stat.TH","  A probabilistic generative network model with $n$ nodes and $m$ overlapping
layers is obtained as a superposition of $m$ mutually independent Bernoulli
random graphs of varying size and strength. When $n$ and $m$ are large and of
the same order of magnitude, the model admits a sparse limiting regime with a
tunable power-law degree distribution and nonvanishing clustering coefficient.
This article presents an asymptotic formula for the joint degree distribution
of adjacent nodes. This yields a simple analytical formula for the model
assortativity, and opens up ways to analyze rank correlation coefficients
suitable for random graphs with heavy-tailed degree distributions.
"
"stat.TH","  Deep learning methods continue to have a decided impact on machine learning,
both in theory and in practice. Statistical theoretical developments have been
mostly concerned with approximability or rates of estimation when recovering
infinite dimensional objects (curves or densities). Despite the impressive
array of available theoretical results, the literature has been largely silent
about uncertainty quantification for deep learning. This paper takes a step
forward in this important direction by taking a Bayesian point of view. We
study Gaussian approximability of certain aspects of posterior distributions of
sparse deep ReLU architectures in non-parametric regression. Building on tools
from Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises
theorems for linear and quadratic functionals, which guarantee that implied
Bayesian credible regions have valid frequentist coverage. Our results provide
new theoretical justifications for (Bayesian) deep learning with ReLU
activation functions, highlighting their inferential potential.
"
"stat.TH","  Investigators often evaluate treatment effects by considering settings in
which all individuals are assigned a treatment of interest, assuming that an
unlimited number of treatment units are available. However, many real-life
treatments are of limited supply and cannot be provided to all individuals in
the population. For example, patients on the liver transplant waiting list
cannot be assigned a liver transplant immediately at the time they reach
highest priority because a suitable organ is not likely to be immediately
available. In these cases, investigators may still be interested in the effects
of treatment strategies in which a finite number of organs are available at a
given time, that is, treatment regimes that satisfy resource constraints. Here,
we describe an estimand that can be used to define causal effects of treatment
strategies that satisfy resource constraints: proportionally-representative
interventions for limited resources. We derive a simple class of inverse
probability weighted estimators, and apply one such estimator to evaluate the
effect of restricting or expanding utilization of ""increased risk"" liver organs
to treat patients with end-stage liver disease. Our method is designed to
evaluate policy-relevant interventions in the setting of finite treatment
resources.
"
"stat.TH","  We develop a new class of distribution--free multiple testing rules for false
discovery rate (FDR) control under general dependence. A key element in our
proposal is a symmetrized data aggregation (SDA) approach to incorporating the
dependence structure via sample splitting, data screening and information
pooling. The proposed SDA filter first constructs a sequence of ranking
statistics that fulfill global symmetry properties, and then chooses a
data--driven threshold along the ranking to control the FDR. The SDA filter
substantially outperforms the knockoff method in power under moderate to strong
dependence, and is more robust than existing methods based on asymptotic
$p$-values. We first develop finite--sample theory to provide an upper bound
for the actual FDR under general dependence, and then establish the asymptotic
validity of SDA for both the FDR and false discovery proportion (FDP) control
under mild regularity conditions. The procedure is implemented in the R package
\texttt{SDA}. Numerical results confirm the effectiveness and robustness of SDA
in FDR control and show that it achieves substantial power gain over existing
methods in many settings.
"
"stat.TH","  We propose a new powerful family of tests of univariate normality. These
tests are based on an initial value problem in the space of characteristic
functions originating from the fixed point property of the normal distribution
in the zero bias transform. Limit distributions of the test statistics are
provided under the null hypothesis, as well as under contiguous and fixed
alternatives. Using the covariance structure of the limiting Gaussian process
from the null distribution, we derive explicit formulas for the first four
cumulants of the limiting random element and apply the results by fitting a
distribution from the Pearson system. A comparative Monte Carlo power study
shows that the new tests are serious competitors to the strongest well
established tests.
"
"stat.TH","  In hypothesis testing, a false discovery occurs when a hypothesis is
incorrectly rejected due to noise in the sample. When adaptively testing
multiple hypotheses, the probability of a false discovery increases as more
tests are performed. Thus the problem of False Discovery Rate (FDR) control is
to find a procedure for testing multiple hypotheses that accounts for this
effect in determining the set of hypotheses to reject. The goal is to minimize
the number (or fraction) of false discoveries, while maintaining a high true
positive rate (i.e., correct discoveries).
  In this work, we study False Discovery Rate (FDR) control in multiple
hypothesis testing under the constraint of differential privacy for the sample.
Unlike previous work in this direction, we focus on the online setting, meaning
that a decision about each hypothesis must be made immediately after the test
is performed, rather than waiting for the output of all tests as in the offline
setting. We provide new private algorithms based on state-of-the-art results in
non-private online FDR control. Our algorithms have strong provable guarantees
for privacy and statistical performance as measured by FDR and power. We also
provide experimental results to demonstrate the efficacy of our algorithms in a
variety of data environments.
"
"stat.TH","  In this paper, we have obtained conditions on parameters that result in
dispersive ordering and star ordering among two unequal sets of random
variables from Proportional hazard rate and Proportional reversed hazard rate
family of distributions. The n-independent random variables under observation
belonging to a multiple-outlier model are given as corollary. The conditions
obtained involve simple inequalities among the parameters and class of life
distribution corresponding to ageing. Some stochastic ordering results for
sample minimum and maximum with dependency based on Archimedean copula has also
been studied by varying the location parameters.
"
"stat.TH","  In this paper, we have discussed the usual stochastic ordering relations
between two systems. Each system consists of n mutually independent components.
The components follow Exponentiated (Extended) Chen distribution with three
parameters $\alpha, \beta, \lambda$. Two popular systems are taken into
consideration, one is the series system and another is the parallel system. The
results in this paper are obtained by varying one parameter and the other
parameters are kept constant. The hazard rate ordering or reversed hazard rate
ordering relations that are not possible for series or parallel systems have
been demonstrated with the help of counterexamples.
"
"stat.TH","  In this paper, we have discussed the stochastic comparison of the smallest
and largest ordered statistic from independent heterogeneous Weibull-G random
variables and Gompertz Makeham random variables. We compare systems arising
from taking different model parameters and obtain stochastic ordering results
under the condition of multivariate chain majorization. Using the notion of
vector majorization, we compare different systems and obtain stochastic
ordering results.
"
"stat.TH","  This paper is concerned with false discovery rate (FDR) control in
large-scale multiple testing problems. We first propose a new data-driven
testing procedure for controlling the FDR in large-scale t-tests for one-sample
mean problem. The proposed procedure achieves exact FDR control in finite
sample settings when the populations are symmetric no matter the number of
tests or sample sizes. Comparing with the existing bootstrap method for FDR
control, the proposed procedure is computationally efficient. We show that the
proposed method can control the FDR asymptotically for asymmetric populations
even when the test statistics are not independent. We further show that the
proposed procedure with a simple correction is as accurate as the bootstrap
method to the second-order degree, and could be much more effective than the
existing normal calibration. We extend the proposed procedure to two-sample
mean problem. Empirical results show that the proposed procedures have better
FDR control than existing ones when the proportion of true alternative
hypotheses is not too low, while maintaining reasonably good detection ability.
"
"stat.TH","  We propose a method for estimation in high-dimensional linear models with
nominal categorical data. Our estimator, called SCOPE, fuses levels together by
making their corresponding coefficients exactly equal. This is achieved using
the minimax concave penalty on differences between the order statistics of the
coefficients for a categorical variable, thereby clustering the coefficients.
We provide an algorithm for exact and efficient computation of the global
minimum of the resulting nonconvex objective in the case with a single variable
with potentially many levels, and use this within a block coordinate descent
procedure in the multivariate case. We show that an oracle least squares
solution that exploits the unknown level fusions is a limit point of the
coordinate descent with high probability, provided the true levels have a
certain minimum separation; these conditions are known to be minimal in the
univariate case. We demonstrate the favourable performance of SCOPE across a
range of real and simulated datasets. An R package CatReg implementing SCOPE
for linear models and also a version for logistic regression is available on
CRAN.
"
"stat.TH","  Estimation of the tail index of heavy-tailed distributions and its
applications are essential in many research areas. We propose a class of
weighted least squares (WLS) estimators for the Parzen tail index. Our approach
is based on the method developed by \cite{Holan2010}. We investigate
consistency and asymptotic normality of the WLS estimators. Through a
simulation study, we make a comparison with the Hill, Pickands, DEdH (Dekkers,
Einmahl and de Haan) and ordinary least squares (OLS) estimators using the mean
square error as criterion. The results show that in a restricted model some
members of the WLS estimators are competitive with the Pickands, DEdH and OLS
estimators.
"
"stat.TH","  We propose a class of weighted least squares estimators for the tail index of
a distribution function with a regularly varying upper tail. Our approach is
based on the method developed by \cite{Holan2010} for the Parzen tail index.
Asymptotic normality of the estimators is proved. Through a simulation study,
these and earlier estimators are compared in the Pareto and Hall models using
the mean squared error as criterion. The results show that the weighted least
squares estimator is better than the other estimators investigated.
"
"stat.TH","  This paper presents mathematical results in support of the methodology of the
probabilistic learning on manifolds (PLoM) recently introduced by the authors,
which has been used with success for analyzing complex engineering systems. The
PLoM considers a given initial dataset constituted of a small number of points
given in an Euclidean space, which are interpreted as independent realizations
of a vector-valued random variable for which its non-Gaussian probability
measure is unknown but is, \textit{a priori}, concentrated in an unknown subset
of the Euclidean space. The objective is to construct a learned dataset
constituted of additional realizations that allow the evaluation of converged
statistics. A transport of the probability measure estimated with the initial
dataset is done through a linear transformation constructed using a
reduced-order diffusion-maps basis. In this paper, it is proven that this
transported measure is a marginal distribution of the invariant measure of a
reduced-order It\^o stochastic differential equation that corresponds to a
dissipative Hamiltonian dynamical system. This construction allows for
preserving the concentration of the probability measure. This property is shown
by analyzing a distance between the random matrix constructed with the PLoM and
the matrix representing the initial dataset, as a function of the dimension of
the basis. It is further proven that this distance has a minimum for a
dimension of the reduced-order diffusion-maps basis that is strictly smaller
than the number of points in the initial dataset. Finally, a brief numerical
application illustrates the mathematical results.
"
"stat.TH","  This paper investigates a statistical procedure for testing the equality of
two independent estimated covariance matrices when the number of potentially
dependent data vectors is large and proportional to the size of the vectors,
that is, the number of variables. Inspired by the spike models used in random
matrix theory, we concentrate on the largest eigenvalues of the matrices in
order to determine significance. To avoid false rejections we must guard
against residual spikes and need a sufficiently precise description of the
behaviour of the largest eigenvalues under the null hypothesis.
  In this paper we propose some ""invariant"" theorems that allows us to extend
the test of arXiv:2002.12741 for perturbation of order $1$ to some general
tests for order $k$. The statistics introduced in this paper allow the user to
test the equality of two populations based on high-dimensional multivariate
data.
  Simulations show that these tests have more power of detection than standard
multivariate approaches.
"
"stat.TH","  Modelling edge weights play a crucial role in the analysis of network data,
which reveals the extent of relationships among individuals. Due to the
diversity of weight information, sharing these data has become a complicated
challenge in a privacy-preserving way. In this paper, we consider the case of
the non-denoising process to achieve the trade-off between privacy and weight
information in the generalized $\beta$-model. Under the edge differential
privacy with a discrete Laplace mechanism, the Z-estimators from estimating
equations for the model parameters are shown to be consistent and
asymptotically normally distributed. The simulations and a real data example
are given to further support the theoretical results.
"
"stat.TH","  This paper investigates a statistical procedure for testing the equality of
two independent estimated covariance matrices when the number of potentially
dependent data vectors is large and proportional to the size of the vectors,
that is, the number of variables. Inspired by the spike models used in random
matrix theory, we concentrate on the largest eigenvalues of the matrices in
order to determine significance. To avoid false rejections we must guard
against residual spikes and need a sufficiently precise description of the
behaviour of the largest eigenvalues under the null hypothesis. In this paper,
we lay a foundation by treating alternatives based on perturbations of order
$1$, that is, a single large eigenvalue. Our statistic allows the user to test
the equality of two populations. Future work will extend the result to
perturbations of order $k$ and demonstrate conservativeness of the procedure
for more general matrices.
"
"stat.TH","  This paper concerns the use of a particular class of determinantal point
processes (DPP), a class of repulsive spatial point processes, for Monte Carlo
integration. Let $d\ge 1$, $I\subseteq \overline d=\{1,\dots,d\}$ with
$\iota=|I|$. Using a single set of $N$ quadrature points $\{u_1,\dots,u_N\}$
defined, once for all, in dimension $d$ from the realization of the DPP model,
we investigate ""minimal"" assumptions on the integrand in order to obtain
unbiased Monte Carlo estimates of $\mu(f_I)=\int_{[0,1]^\iota} f_I(u)
\mathrm{d} u$ for any known $\iota$-dimensional integrable function on
$[0,1]^\iota$. In particular, we show that the resulting estimator has variance
with order $N^{-1-(2s\wedge 1)/d}$ when the integrand belongs to some Sobolev
space with regularity $s > 0$. When $s>1/2$ (which includes a large class of
non-differentiable functions), the variance is asymptotically explicit and the
estimator is shown to satisfy a Central Limit Theorem.
"
"stat.TH","  We derive bounds for the Orlicz norm of the deviation of a random variable
defined on $\mathbb{R}^n$ from its Gaussian mean value. The random variables
are assumed to be smooth and the bound itself depends on the Orlicz norm of the
gradient. Applications to non-parametric Information Geometry are discussed.
"
"stat.TH","  Modern large-scale statistical models require to estimate thousands to
millions of parameters. This is often accomplished by iterative algorithms such
as gradient descent, projected gradient descent or their accelerated versions.
What are the fundamental limits to these approaches? This question is well
understood from an optimization viewpoint when the underlying objective is
convex. Work in this area characterizes the gap to global optimality as a
function of the number of iterations. However, these results have only indirect
implications in terms of the gap to statistical optimality.
  Here we consider two families of high-dimensional estimation problems:
high-dimensional regression and low-rank matrix estimation, and introduce a
class of `general first order methods' that aim at efficiently estimating the
underlying parameters. This class of algorithms is broad enough to include
classical first order optimization (for convex and non-convex objectives), but
also other types of algorithms. Under a random design assumption, we derive
lower bounds on the estimation error that hold in the high-dimensional
asymptotics in which both the number of observations and the number of
parameters diverge. These lower bounds are optimal in the sense that there
exist algorithms whose estimation error matches the lower bounds up to
asymptotically negligible terms. We illustrate our general results through
applications to sparse phase retrieval and sparse principal component analysis.
"
"stat.TH","  We propose a time-varying generalization of the Bradley-Terry model that
allows for nonparametric modeling of dynamic global rankings of distinct teams.
We develop a novel estimator that relies on kernel smoothing to pre-process the
pairwise comparisons over time and is applicable in sparse settings where the
Bradley-Terry may not be fit. We obtain necessary and sufficient conditions for
the existence and uniqueness of our estimator. We also derive time-varying
oracle bounds for both the estimation error and the excess risk in the
model-agnostic setting where the Bradley-Terry model is not necessarily the
true data generating process. We thoroughly test the practical effectiveness of
our model using both simulated and real world data and suggest an efficient
data-driven approach for bandwidth tuning.
"
"stat.TH","  In many biomedical applications, outcome is measured as a ``time-to-event''
(eg. disease progression or death). To assess the connection between features
of a patient and this outcome, it is common to assume a proportional hazards
model, and fit a proportional hazards regression (or Cox regression). To fit
this model, a log-concave objective function known as the ``partial
likelihood'' is maximized. For moderate-sized datasets, an efficient
Newton-Raphson algorithm that leverages the structure of the objective can be
employed. However, in large datasets this approach has two issues: 1) The
computational tricks that leverage structure can also lead to computational
instability; 2) The objective does not naturally decouple: Thus, if the dataset
does not fit in memory, the model can be very computationally expensive to fit.
This additionally means that the objective is not directly amenable to
stochastic gradient-based optimization methods. To overcome these issues, we
propose a simple, new framing of proportional hazards regression: This results
in an objective function that is amenable to stochastic gradient descent. We
show that this simple modification allows us to efficiently fit survival models
with very large datasets. This also facilitates training complex, eg.
neural-network-based, models with survival data.
"
"stat.TH","  Recursive max-linear vectors model causal dependence between node variables
by a structural equation model, expressing each node variable as a max-linear
function of its parental nodes in a directed acyclic graph (DAG) and some
exogenous innovation. For such a model, there exists a unique minimum DAG,
represented by the Kleene star matrix of its edge weight matrix, which
identifies the model and can be estimated. For a more realistic statistical
modeling we introduce some random observational noise. A probabilistic analysis
of this new noisy model reveals that the unique minimum DAG representing the
distribution of the non-noisy model remains unchanged and identifiable.
Moreover, the distribution of the minimum ratio estimators of the model
parameters at their left limits are completely determined by the distribution
of the noise variables up to a positive constant. Under a regular variation
condition on the noise variables we prove that the estimated Kleene star matrix
converges to a matrix of independent Weibull entries after proper centering and
scaling.
"
"stat.TH","  The emergence of the exit events from a bounded domain containing a stable
fixed point induced by non-Gaussian L\'evy fluctuations plays a pivotal role in
practical physical systems. In the limit of weak noise, we develop a
Hamiltonian formalism under the L\'evy fluctuations with exponentially light
jumps for one- and two-dimensional stochastic dynamical systems. This formalism
is based on a recently proved large deviation principle for dynamical systems
under non-Gaussian L\'evy perturbations. We demonstrate how to compute the most
probable exit path and the quasi-potential by several examples. Meanwhile, we
explore the impacts of the jump measure on the quasi-potential quantitatively
and on the most probable exit path qualitatively. Results show that the
quasi-potential can be well estimated by an approximate analytical expression.
Moreover, we discover that although the most probable exit paths are analogous
to the Gaussian case for the isotropic noise, the anisotropic noise leads to
significant changes of the structure of the exit paths. These findings shed
light on the underlying qualitative mechanism and quantitative feature of the
exit phenomenon induced by non-Gaussian noise.
"
"stat.TH","  The Newcomb-Benford probability distribution is becoming very popular in many
areas using statistics, notably in fraud detection. In such contexts, it is
important to be able to determine if a data set arises from this distribution
while controlling the risk of a Type 1 error, i.e. falsely identifying a fraud,
and a Type 2 error, i.e. not detecting that a fraud occurred. The statistical
tool to do this work is a goodness-of-fit test. For the Newcomb-Benford
distribution, the most popular such test is Pearson's chi-square test whose
power, related to the Type 2 error, is known to be weak. Consequently, other
tests have been recently introduced. The goal of the present work is to build
new goodness-of-fit tests for this distribution, based on the smooth test
principle. These tests are then compared to some of their competitors. It turns
out that the proposals of the paper are globally preferable to existing tests
and should be seriously considered in fraud detection contexts, among others.
"
"stat.TH","  We consider exact asymptotics of the minimax risk for global testing against
sparse alternatives in the context of high dimensional linear regression. Our
results characterize the leading order behavior of this minimax risk in several
regimes, uncovering new phase transitions in its behavior. This complements a
vast literature characterizing asymptotic consistency in this problem, and
provides a useful benchmark, against which the performance of specific tests
may be compared. Finally, we provide some preliminary evidence that popular
sparsity adaptive procedures might be sub-optimal in terms of the minimax risk.
"
"stat.TH","  The calibration of noise for a privacy-preserving mechanism depends on the
sensitivity of the query and the prescribed privacy level. A data steward must
make the non-trivial choice of a privacy level that balances the requirements
of users and the monetary constraints of the business entity. We analyse roles
of the sources of randomness, namely the explicit randomness induced by the
noise distribution and the implicit randomness induced by the data-generation
distribution, that are involved in the design of a privacy-preserving
mechanism. The finer analysis enables us to provide stronger privacy guarantees
with quantifiable risks. Thus, we propose privacy at risk that is a
probabilistic calibration of privacy-preserving mechanisms. We provide a
composition theorem that leverages privacy at risk. We instantiate the
probabilistic calibration for the Laplace mechanism by providing analytical
results. We also propose a cost model that bridges the gap between the privacy
level and the compensation budget estimated by a GDPR compliant business
entity. The convexity of the proposed cost model leads to a unique fine-tuning
of privacy level that minimises the compensation budget. We show its
effectiveness by illustrating a realistic scenario that avoids overestimation
of the compensation budget by using privacy at risk for the Laplace mechanism.
We quantitatively show that composition using the cost optimal privacy at risk
provides stronger privacy guarantee than the classical advanced composition.
"
"stat.TH","  Drees and Rootz\'en (2010) have established limit theorems for a general
class of empirical processes of statistics that are useful for the extreme
value analysis of time series, but do not apply to statistics of sliding
blocks, including so-called runs estimators. We generalize these results to
empirical processes which cover both the class considered by Drees and
Rootz\'en (2010) and processes of sliding blocks statistics. Using this
approach, one can analyze different types of statistics in a unified framework.
We show that statistics based on sliding blocks are asymptotically normal with
an asymptotic variance which, under rather mild conditions, is smaller than or
equal to the asymptotic variance of the corresponding estimator based on
disjoint blocks. Finally, the general theory is applied to three well-known
estimators of the extremal index. It turns out that they all have the same
limit distribution, a fact which has so far been overlooked in the literature.
"
"stat.TH","  We modify ETAS models by replacing the Pareto-like kernel proposed by Ogata
with a Mittag-Leffler type kernel. Provided that the kernel decays as a power
law with exponent $\beta + 1 \in (1,2]$, this replacement has the advantage
that the Laplace transform of the Mittag-Leffler function is known explicitly,
leading to simpler calculation of relevant quantities.
"
"stat.TH","  In spite of the accomplishments of deep learning based algorithms in numerous
applications and very broad corresponding research interest, at the moment
there is still no rigorous understanding of the reasons why such algorithms
produce useful results in certain situations. A thorough mathematical analysis
of deep learning based algorithms seems to be crucial in order to improve our
understanding and to make their implementation more effective and efficient. In
this article we provide a mathematically rigorous full error analysis of deep
learning based empirical risk minimisation with quadratic loss function in the
probabilistically strong sense, where the underlying deep neural networks are
trained using stochastic gradient descent with random initialisation. The
convergence speed we obtain is presumably far from optimal and suffers under
the curse of dimensionality. To the best of our knowledge, we establish,
however, the first full error analysis in the scientific literature for a deep
learning based algorithm in the probabilistically strong sense and, moreover,
the first full error analysis in the scientific literature for a deep learning
based algorithm where stochastic gradient descent with random initialisation is
the employed optimisation method.
"
"stat.TH","  The Central Limit Theorem (CLT) is one of the most fundamental results in
statistics. It states that the standardized sample mean of a sequence of $n$
mutually independent and identically distributed random variables with finite
first and second moments converges in distribution to a standard Gaussian as
$n$ goes to infinity. In particular, pairwise independence of the sequence is
generally not sufficient for the theorem to hold. We construct explicitly a
sequence of pairwise independent random variables having a common but arbitrary
marginal distribution $F$ (satisfying very mild conditions) for which the CLT
is not verified. We study the extent of this 'failure' of the CLT by obtaining,
in closed form, the asymptotic distribution of the sample mean of our sequence.
This is illustrated through several theoretical examples, for which we provide
associated computing codes in the R language.
"
"stat.TH","  In multiple antenna systems employing time-division duplexing, spatial
precoder design at the base station (BS) leverages channel state information
acquired through uplink pilot transmission, under the assumption of channel
reciprocity. Malicious eavesdroppers can start pilot spoofing attacks to alter
such design, so as to improve their eavesdropping performance in downlink. The
aim of this paper is to study the effects of pilot spoofing attacks on uplink
channel estimation, by assuming that the BS knows the angle of arrivals (AoAs)
of the legitimate channels. Specifically, after assessing the performance of
the simple least squares estimator (LSE), we consider more sophisticated
estimators, such as the maximum likelihood estimator (MLE) and different
versions of the minimum mean square error estimator (MMSEE), involving
different degrees of a priori information about the pilot spoofing attacks.
Theoretical analysis and numerical simulations are used to compare the
performance of such estimators. In particular, we analytically demonstrate that
the spoofing effects in the high signal-to-noise regime can be completely
suppressed, under certain conditions involving the AoAs of the legitimate and
spoofing channels. Moreover, we show that even an imperfect knowledge of the
AoAs and of the average transmission power of the spoofing signals allows the
MLE and MMSEE to achieve significant performance gains over the LSE.
"
"stat.TH","  While the combination of multi-antenna and relaying techniques has been
extensively studied for Long Term Evolution Advanced (LTE-A) and Internet of
Things (IoT) applications, it is expected to still play an important role in
5th Generation (5G) networks. However, the expected benefits of these
technologies cannot be achieved without a proper system design. In this paper,
we consider the problem of jointly optimizing terminal precoders/decoders and
relay forwarding matrices on the basis of the sum mean square error (MSE)
criterion in multiple-input multiple-output (MIMO) two-way relay systems, where
two multi-antenna nodes mutually exchange information via multi-antenna
amplify-and-forward relays. This problem is nonconvex and a local optimal
solution is typically found by using iterative algorithms based on alternating
optimization. We show how the constrained minimization of the sum-MSE can be
relaxed to obtain two separated subproblems which, under mild conditions, admit
a closed-form solution. Compared to iterative approaches, the proposed design
is more suited to be integrated in 5G networks, since it is computationally
more convenient and its performance exhibits a better scaling in the number of
relays.
"
"stat.TH","  We propose a new statistical estimation framework for a large family of
global sensitivity analysis methods. Our approach is based on rank statistics
and uses an empirical correlation coefficient recently introduced by Sourav
Chatterjee. We show how to apply this approach to compute not only the
Cram\'er-von-Mises indices, which are directly related to Chatterjee's notion
of correlation, but also Sobol indices at any order, higher-order moment
indices, and Shapley effects. We establish consistency of the resulting
estimators and demonstrate their numerical efficiency, especially for small
sample sizes.
"
"stat.TH","  Thompson sampling is one of the most widely used algorithms for many online
decision problems, due to its simplicity in implementation and superior
empirical performance over other state-of-the-art methods. Despite its
popularity and empirical success, it has remained an open problem whether
Thompson sampling can match the minimax lower bound $\Omega(\sqrt{KT})$ for
$K$-armed bandit problems, where $T$ is the total time horizon. In this paper,
we solve this long open problem by proposing a variant of Thompson sampling
called MOTS that adaptively clips the sampling instance of the chosen arm at
each time step. We prove that this simple variant of Thompson sampling achieves
the minimax optimal regret bound $O(\sqrt{KT})$ for finite time horizon $T$, as
well as the asymptotic optimal regret bound for Gaussian rewards when $T$
approaches infinity. To our knowledge, MOTS is the first Thompson sampling type
algorithm that achieves the minimax optimality for multi-armed bandit problems.
"
"stat.TH","  The normal distribution is well-known for several results that it is the only
to fulfil. The aim of the present paper is to show that many of these
characterizations actually follow from the fact that the derivative of the
log-density of the normal distribution is the (negative) identity function.
This \emph{a priori} very simple yet surprising observation allows a deeper
understanding of existing characterizations and paves the way to an immediate
extension to a general density $x\mapsto p(x)$ by replacing $-x$ in these
results with $(\log p(x))'$.
"
"stat.TH","  Suppose that we wish to estimate a finite-dimensional summary of one or more
function-valued features of an underlying data-generating mechanism under a
nonparametric model. One approach to estimation is by plugging in flexible
estimates of these features. Unfortunately, in general, such estimators may not
be asymptotically efficient, which often makes these estimators difficult to
use as a basis for inference. Though there are several existing methods to
construct asymptotically efficient plug-in estimators, each such method either
can only be derived using knowledge of efficiency theory or is only valid under
stringent smoothness assumptions. Among existing methods, sieve estimators
stand out as particularly convenient because efficiency theory is not required
in their construction, their tuning parameters can be selected data adaptively,
and they are universal in the sense that the same fits lead to efficient
plug-in estimators for a rich class of estimands. Inspired by these desirable
properties, we propose two novel universal approaches for estimating
function-valued features that can be analyzed using sieve estimation theory.
Compared to traditional sieve estimators, these approaches are valid under more
general conditions on the smoothness of the function-valued features by
utilizing flexible estimates that can be obtained, for example, using machine
learning.
"
"stat.TH","  Recent empirical and theoretical studies have shown that many learning
algorithms -- from linear regression to neural networks -- can have test
performance that is non-monotonic in quantities such the sample size and model
size. This striking phenomenon, often referred to as ""double descent"", has
raised questions of if we need to re-think our current understanding of
generalization. In this work, we study whether the double-descent phenomenon
can be avoided by using optimal regularization. Theoretically, we prove that
for certain linear regression models with isotropic data distribution,
optimally-tuned $\ell_2$ regularization achieves monotonic test performance as
we grow either the sample size or the model size. We also demonstrate
empirically that optimally-tuned $\ell_2$ regularization can mitigate double
descent for more general models, including neural networks. Our results suggest
that it may also be informative to study the test risk scalings of various
algorithms in the context of appropriately tuned regularization.
"
"stat.TH","  In this paper we consider high-dimensional multiclass classification by
sparse multinomial logistic regression. We propose first a feature selection
procedure based on penalized maximum likelihood with a complexity penalty on
the model size and derive the nonasymptotic bounds for misclassification excess
risk of the resulting classifier. We establish also their tightness by deriving
the corresponding minimax lower bounds. In particular, we show that there exist
two regimes corresponding to small and large number of classes. The bounds can
be reduced under the additional low noise condition. To find a penalized
maximum likelihood solution with a complexity penalty requires, however, a
combinatorial search over all possible models. To design a feature selection
procedure computationally feasible for high-dimensional data, we propose
multinomial logistic group Lasso and Slope classifiers and show that they also
achieve the minimax order.
"
"stat.TH","  The purpose of this thesis is to develop new theories on high-dimensional
structured signal recovery under a rather weak assumption on the measurements
that only a finite number of moments exists. High-dimensional recovery has been
one of the emerging topics in the last decade partly due to the celebrated work
of Candes, Romberg and Tao (e.g. [CRT06, CRT04]). The original analysis there
(and the works thereafter) necessitates a strong concentration argument
(namely, the restricted isometry property), which only holds for a rather
restricted class of measurements with light-tailed distributions. It had long
been conjectured that high-dimensional recovery is possible even if restricted
isometry type conditions do not hold, but the general theory was beyond the
grasp until very recently, when the works [Men14a, KM15] propose a new
small-ball method. In these two papers, the authors initiated a new analysis
framework for general empirical risk minimization (ERM) problems with respect
to the square loss, which is robust and can potentially allow heavy-tailed loss
functions. The materials in this thesis are partly inspired by [Men14a], but
are of a different mindset: rather than directly analyzing the existing ERMs
for signal recovery for which it is difficult to avoid strong moment
assumptions, we show that, in many circumstances, by carefully re-designing the
ERMs to start with, one can still achieve the minimax optimal statistical rate
of signal recovery with very high probability under much weaker assumptions
than existing works.
"
"stat.TH","  It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler,
$\alpha$-divergences, and Jeffreys' divergences between densities belonging to
a same exponential family have generic closed-form formulas relying on the
strictly convex and real-analytic cumulant function characterizing the
exponential family. In this work, we report (dis)similarity formulas which
bypass the explicit use of the cumulant function and highlight the role of
quasi-arithmetic means and their multivariate mean operator extensions. In
practice, these cumulant-free formulas are handy when implementing these
(dis)similarities using legacy Application Programming Interfaces (APIs) since
our method requires only to partially factorize the densities canonically of
the considered exponential family.
"
"stat.TH","  The absolute-moment method is widespread for estimating the Hurst exponent of
a fractional Brownian motion $X$. But this method is biased when applied to a
stationary version of $X$, in particular an inverse Lamperti transform of $X$,
with a linear time contraction of parameter $\theta$. We present an adaptation
of the absolute-moment method to this framework and we compare it to the
maximum likelihood method, with simulations. The conclusion is mainly in favour
of the adapted absolute-moment method for several reasons: it makes it possible
to confirm visually that the model is well specified, it is computationally
more performing, the estimation of $\theta$ is more accurate.
"
"stat.TH","  We study logistic regression with total variation penalty on the canonical
parameter and show that the resulting estimator satisfies a sharp oracle
inequality: the excess risk of the estimator is adaptive to the number of jumps
of the underlying signal or an approximation thereof. In particular when there
are finitely many jumps, and jumps up are sufficiently separated from jumps
down, then the estimator converges with a parametric rate up to a logarithmic
term $\log n / n$, provided the tuning parameter is chosen appropriately of
order $1/ \sqrt n$. Our results extend earlier results for quadratic loss to
logistic loss. We do not assume any a priori known bounds on the canonical
parameter but instead only make use of the local curvature of the theoretical
risk.
"
"stat.TH","  We prove a strong law of large numbers for simultaneously testing parameters
of a large number of dependent, Lancaster bivariate random variables with
infinite supports, and discuss its implications.
"
"stat.TH","  The main goal of this article is to study how an auxiliary information can be
used to improve the power of two famous statistical tests: the $ Z$-test and
the chi-square test. This information can be of any nature - probability of
sets of partitions, expectation of a function, ... - and is not even required
to be an exact information, it can be given by an estimate based on a larger
sample for example. Some definitions of auxiliary information can be found in
the statistical literature and will be recalled. In this article, the notion of
auxiliary information is discussed here from a very general point of view.
These two statistical tests are modified so that the auxiliary information is
taken into account. One show in particular that the power of these tests is
increased exponentially. Some statistical examples are treated to show the
concreteness of this method.
"
"stat.TH","  Wasserstein geometry and information geometry are two important structures
introduced in a manifold of probability distributions. The former is defined by
using the transportation cost between two distributions, so it reflects the
metric structure of the base manifold on which distributions are defined.
Information geometry is constructed based on the invariance criterion that the
geometry is invariant under reversible transformations of the base space. Both
have their own merits for applications. Statistical inference is constructed on
information geometry, where the Fisher metric plays a fundamental role, whereas
Wasserstein geometry is useful for applications to computer vision and AI. We
propose statistical inference based on the Wasserstein geometry in the case
that the base space is 1-dimensional. By using the location-scale model, we
derive the $W$-estimator explicitly and studies its asymptotic behaviors.
"
"stat.TH","  In this paper, we provide a central limit theorem for the finite-dimensional
marginal distributions of empirical processes $(Z_n(f))_{f\in\mathcal{F}}$
whose index set $\mathcal{F}$ is a family of cluster functionals valued on
blocks of values of a stationary random field. The practicality and
applicability of the result depends mainly on the usual Lindeberg condition and
a sequence $T_n$ which summarizes the dependence between the blocks of the
random field values. Finally, as application, we use the previous result in
order to show the Gaussian asymptotic behavior of the iso-extremogram estimator
introduced in this paper.
"
"stat.TH","  We consider a stationary $AR(p)$ model. The autoregression parameters are
unknown as well as the distribution of innovations. Based on the residuals from
the parameter estimates, an analog of empirical distribution function is
defined and the tests of Kolmogorov's and $\omega^2$ type is constructed for
testing hypotheses on the normality of innovations. We obtain the asymptotic
power of these tests under local alternatives.
"
"stat.TH","  We introduce a new method for high-dimensional, online changepoint detection
in settings where a $p$-variate Gaussian data stream may undergo a change in
mean. The procedure works by performing likelihood ratio tests against simple
alternatives of different scales in each coordinate, and then aggregating test
statistics across scales and coordinates. The algorithm is online in the sense
that both its storage requirements and worst-case computational complexity per
new observation are independent of the number of previous observations; in
practice, it may even be significantly faster than this. We prove that the
patience, or average run length under the null, of our procedure is at least at
the desired nominal level, and provide guarantees on its response delay under
the alternative that depend on the sparsity of the vector of mean change.
Simulations confirm the practical effectiveness of our proposal, which is
implemented in the R package 'ocd', and we also demonstrate its utility on a
seismology data set.
"
"stat.TH","  Consider a $p$-dimensional population ${\mathbf x} \in\mathbb{R}^p$ with iid
coordinates in the domain of attraction of a stable distribution with index
$\alpha\in (0,2)$. Since the variance of ${\mathbf x}$ is infinite, the sample
covariance matrix ${\mathbf S}_n=n^{-1}\sum_{i=1}^n {{\mathbf x}_i}{\mathbf
x}'_i$ based on a sample ${\mathbf x}_1,\ldots,{\mathbf x}_n$ from the
population is not well behaved and it is of interest to use instead the sample
correlation matrix ${\mathbf R}_n= \{\operatorname{diag}({\mathbf
S}_n)\}^{-1/2}\, {\mathbf S}_n \{\operatorname{diag}({\mathbf S}_n)\}^{-1/2}$.
This paper finds the limiting distributions of the eigenvalues of ${\mathbf
R}_n$ when both the dimension $p$ and the sample size $n$ grow to infinity such
that $p/n\to \gamma \in (0,\infty)$. The family of limiting distributions
$\{H_{\alpha,\gamma}\}$ is new and depends on the two parameters $\alpha$ and
$\gamma$. The moments of $H_{\alpha,\gamma}$ are fully identified as sum of two
contributions: the first from the classical Mar\v{c}enko-Pastur law and a
second due to heavy tails. Moreover, the family $\{H_{\alpha,\gamma}\}$ has
continuous extensions at the boundaries $\alpha=2$ and $\alpha=0$ leading to
the Mar\v{c}enko-Pastur law and a modified Poisson distribution, respectively.
  Our proofs use the method of moments, the path-shortening algorithm developed
in [18] and some novel graph counting combinatorics. As a consequence, the
moments of $H_{\alpha,\gamma}$ are expressed in terms of combinatorial objects
such as Stirling numbers of the second kind. A simulation study on these
limiting distributions $H_{\alpha,\gamma}$ is also provided for comparison with
the Mar\v{c}enko-Pastur law.
"
"stat.TH","  Let $\left\{X^{1}_k\right\}_{k=1}^{\infty},
\left\{X^{2}_k\right\}_{k=1}^{\infty}, \cdots,
\left\{X^{d}_k\right\}_{k=1}^{\infty}$ be $d$ independent sequences of
Bernoulli random variables with success-parameters $p_1, p_2, \cdots, p_d$
respectively, where $d \geq 2$ is a positive integer, and $ 0<p_j<1$ for all
$j=1,2,\cdots,d.$ Let \begin{equation*} S^{j}(n) = \sum_{i=1}^{n} X^{j}_{i} =
X^{j}_{1} + X^{j}_{2} + \cdots + X^{j}_{n}, \quad n =1,2 , \cdots.
\end{equation*} We declare a ""rencontre"" at time $n$, or, equivalently, say
that $n$ is a ""rencontre-time,"" if \begin{equation*} S^{1}(n) = S^{2}(n) =
\cdots = S^{d}(n). \end{equation*} We motivate and study the distribution of
the first (provided it is finite) rencontre time.
"
"stat.TH","  The theoretical foundation for a number of model selection criteria is
established in the context of inhomogeneous point processes and under various
asymptotic settings: infill, increasing domain, and combinations of these. For
inhomogeneous Poisson processes we consider Akaike information criterion and
the Bayesian information criterion, and in particular we identify the point
process analogue of sample size needed for the Bayesian information criterion.
Considering general inhomogeneous point processes we derive new composite
likelihood and composite Bayesian information criteria for selecting a
regression model for the intensity function. The proposed model selection
criteria are evaluated using simulations of Poisson processes and cluster point
processes.
"
"stat.TH","  This paper serves as a postscript of sorts to Tibshirani (2014); Wang et al.
(2014), who developed continuous-time formulations and properties of trend
filtering, a discrete-time smoothing tool proposed (independently) by Steidl et
al. (2006); Kim et al. (2009). The central object of study is the falling
factorial basis, as it was called by Tibshirani (2014); Wang et al. (2014). Its
span turns out to be a space of piecewise polynomials that has a classical
place in spline theory, called discrete splines (Mangasarian and Schumaker,
1971, 1973; Schumaker, 2007). At the Tibshirani (2014); Wang et al. (2014), we
were not fully aware of these connections. The current paper attempts to
rectify this by making these connections explicit, reviewing (and making use
of) some of the important existing work on discrete splines, and contributing
several new perspectives and new results on discrete splines along the way.
"
"stat.TH","  The ability to quantify complex relationships within multivariate time series
is a key component of modelling many physical systems, from the climate to
brains and other biophysical phenomena. Unfortunately, even testing the
significance of simple dependence measures, such as Pearson correlation, is
complicated by altered sampling properties when autocorrelation is present in
the individual time series. Moreover, it has been recently established that
commonly used multivariate dependence measures---such as Granger
causality---can produce substantially inaccurate results when applying
classical hypothesis-testing procedures to digitally-filtered time series.
Here, we suggest that the digital filtering-induced bias in Granger causality
is an effect of autocorrelation, and we present a principled statistical
framework for the hypothesis testing of a large family of linear-dependence
measures between multiple autocorrelated time series. Our approach unifies the
theoretical foundations established by Bartlett and others on variance
estimators for autocorrelated signals with the more intricate multivariate
measures of linear dependence. Specifically, we derive the sampling
distributions and subsequent hypothesis tests for any measure that can be
decomposed into terms that involve independent partial correlations, which we
show includes Granger causality and mutual information under a multivariate
linear-Gaussian model. In doing so, we provide the first exact tests for
inferring linear dependence between vector autoregressive processes with
limited data. Using numerical simulations and brain-imaging datasets, we
demonstrate that our newly developed tests maintain the expected false-positive
rate (FPR) with minimally-sufficient samples, while the classical
log-likelihood ratio tests can yield an unbounded FPR depending on the
parameters chosen.
"
"stat.TH","  The Anna Karenina principle is named after the opening sentence in the
eponymous novel: Happy families are all alike; every unhappy family is unhappy
in its own way. The Two Envelopes Problem (TEP) is a much-studied paradox in
probability theory, mathematical economics, logic, and philosophy. Time and
again a new analysis is published in which an author claims finally to explain
what actually goes wrong in this paradox. Each author (the present author
included) emphasizes what is new in their approach and concludes that earlier
approaches did not get to the root of the matter. We observe that though a
logical argument is only correct if every step is correct, an apparently
logical argument which goes astray can be thought of as going astray at
different places. This leads to a comparison between the literature on TEP and
a successful movie franchise: it generates a succession of sequels, and even
prequels, each with a different director who approaches the same basic premise
in a personal way. We survey resolutions in the literature with a view to
synthesis, correct common errors, and give a new theorem on order properties of
an exchangeable pair of random variables, at the heart of most TEP variants and
interpretations. A theorem on asymptotic independence between the amount in
your envelope and the question whether it is smaller or larger shows that the
pathological situation of improper priors or infinite expectation values has
consequences as we merely approach such a situation.
"
"stat.TH","  Bayesian model comparison is often based on the posterior distribution over
the set of compared models. This distribution is often observed to concentrate
on a single model even when other measures of model fit or forecasting ability
indicate no strong preference. Furthermore, a moderate change in the data
sample can easily shift the posterior model probabilities to concentrate on
another model. We document overconfidence in two high-profile applications in
economics and neuroscience. To shed more light on the sources of overconfidence
we derive the sampling variance of the Bayes factor in univariate and
multivariate linear regression. The results show that overconfidence is likely
to happen when i) the compared models give very different approximations of the
data-generating process, ii) the models are very flexible with large degrees of
freedom that are not shared between the models, and iii) the models
underestimate the true variability in the data.
"
"stat.TH","  We consider a complex-valued linear mixture model, under discrete weakly
stationary processes. We recover latent components of interest, which have
undergone a linear mixing. We study asymptotic properties of a classical
unmixing estimator, that is based on simultaneous diagonalization of the
covariance matrix and an autocovariance matrix with lag $\tau$. Our main
contribution is that our asymptotic results can be applied to a large class of
processes. In related literature, the processes are typically assumed to have
weak correlations. We extend this class and consider the unmixing estimator
under stronger dependency structures. In particular, we analyze the asymptotic
behavior of the unmixing estimator under both, long- and short-range dependent
complex-valued processes. Consequently, our theory covers unmixing estimators
that converge slower than the usual $\sqrt{T}$ and unmixing estimators that
produce non-Gaussian asymptotic distributions. The presented methodology is a
powerful prepossessing tool and highly applicable in several fields of
statistics. Complex-valued processes are frequently encountered in, for
example, biomedical applications and signal processing. In addition, our
approach can be applied to model real-valued problems that involve temporally
uncorrelated pairs. These are encountered in, for example, applications in
finance.
"
"stat.TH","  Principal Moment Analysis is a method designed for dimension reduction,
analysis and visualization of high dimensional multivariate data. It
generalizes Principal Component Analysis and allows for significant statistical
modeling flexibility, when approximating an unknown underlying probability
distribution, by enabling direct analysis of general approximate measures.
Through https://principalmomentanalysis.github.io/ we provide an
implementation, together with a graphical user interface, of a simplex based
version of Principal Moment Analysis.
"
"stat.TH","  The statistical theory of extremes is extended to observations that are
non-stationary and not independent. The non-stationarity over time and space is
controlled via the scedasis (tail scale) in the marginal distributions. Spatial
dependence stems from multivariate extreme value theory. We establish
asymptotic theory for both the weighted sequential tail empirical process and
the weighted tail quantile process based on all observations, taken over time
and space. The results yield two statistical tests for homoscedasticity in the
tail, one in space and one in time. Further, we show that the common extreme
value index can be estimated via a pseudo-maximum likelihood procedure based on
pooling all (non-stationary and dependent) observations. Our leading example
and application is rainfall in Northern Germany.
"
"stat.TH","  This paper focuses on the time series generated by the event counts of
stationary Hawkes processes. When the exact locations of points are not
observed, but only counts over time intervals of fixed size, existing methods
of estimation are not applicable. We first establish a strong mixing condition
with polynomial decay rate for Hawkes processes, from their Poisson cluster
structure. This allows us to propose a spectral approach to the estimation of
Hawkes processes, based on Whittle's method, which provides consistent and
asymptotically normal estimates under common regularity conditions on their
reproduction kernels. Simulated datasets and a case-study illustrate the
performances of the estimation, notably of the Hawkes reproduction mean and
kernel when time intervals are relatively large.
"
"stat.TH","  Frequentist coverage of $(1-\alpha)$-highest posterior density (HPD) credible
sets is studied in a signal plus noise model under a large class of noise
distributions. We consider a specific class of spike-and-slab prior
distributions. Different regimes are identified and we derive closed form
expressions for the $(1-\alpha)$-HPD on each of these regimes. Similar to the
earlier work by Marchand and Strawderman, it is shown that under suitable
conditions, the frequentist coverage can drop to $1-3\alpha/2.$
"
"stat.TH","  We develop a new approach for the estimation of a multivariate function based
on the economic axioms of monotonicity and quasiconvexity. We prove the
existence of the nonparametric least squares estimator (LSE) for a monotone and
quasiconvex function and provide two characterizations for it. One of these
characterizations is useful from the theoretical point of view, while the other
helps in the computation of the estimator. We show that the LSE is almost
surely unique and is the solution to a mixed-integer quadratic optimization
problem. We prove consistency and find finite sample risk bounds for the LSE
under both fixed lattice and random design settings for the covariates. We
illustrate the superior performance of the LSE against existing estimators via
simulation. Finally, we use the LSE to estimate the production function for the
Japanese plywood industry and the cost function for hospitals across the US.
"
"stat.TH","  Local differential privacy has recently received increasing attention from
the statistics community as a valuable tool to protect the privacy of
individual data owners without the need of a trusted third party. Similar to
the classic notion of randomized response, the idea is that data owners
randomize their true information locally and only release the perturbed data.
Many different protocols for such local perturbation procedures can be
designed. In all the estimation problems studied in the literature so far,
however, no significant difference in terms of minimax risk between purely
non-interactive protocols and protocols that allow for some amount of
interaction between individual data providers could be observed. In this paper
we show that for estimating the integrated square of a density, sequentially
interactive procedures improve substantially over the best possible
non-interactive procedure in terms of minimax rate of estimation. In
particular, in the non-interactive scenario we identify an elbow in the minimax
rate at $s=\frac34$, whereas in the sequentially interactive scenario the elbow
is at $s=\frac12$. This is markedly different from both, the case of direct
observations, where the elbow is well known to be at $s=\frac14$, as well as
from the case where Laplace noise is added to the original data, where an elbow
at $s= \frac94$ is obtained. The fact that a particular locally differentially
private, but interactive, mechanism improves over the simple non-interactive
one is also of great importance for practical implementations of local
differential privacy.
"
"stat.TH","  The multi-index model is a simple yet powerful high-dimensional regression
model which circumvents the curse of dimensionality assuming $ \mathbb{E} [ Y |
X ] = g(A^\top X) $ for some unknown index space $A$ and link function $g$. In
this paper we introduce a method for the estimation of the index space, and
study the propagation error of an index space estimate in the regression of the
link function. The proposed method approximates the index space by the span of
linear regression slope coefficients computed over level sets of the data.
Being based on ordinary least squares, our approach is easy to implement and
computationally efficient. We prove a tight concentration bound that shows
$N^{-1/2}$-convergence, but also faithfully describes the dependence on the
chosen partition of level sets, hence giving indications on the hyperparameter
tuning. The estimator's competitiveness is confirmed by extensive comparisons
with state-of-the-art methods, both on synthetic and real data sets. As a
second contribution, we establish minimax optimal generalization bounds for
k-nearest neighbors and piecewise polynomial regression when trained on samples
projected onto any $N^{-1/2}$-consistent estimate of the index space, thus
providing complete and provable estimation of the multi-index model.
"
"stat.TH","  We study probability density functions that are log-concave. Despite the
space of all such densities being infinite-dimensional, the maximum likelihood
estimate is the exponential of a piecewise linear function determined by
finitely many quantities, namely the function values, or heights, at the data
points. We explore in what sense exact solutions to this problem are possible.
First, we show that the heights given by the maximum likelihood estimate are
generically transcendental. For a cell in one dimension, the maximum likelihood
estimator is expressed in closed form using the generalized W-Lambert function.
Even more, we show that finding the log-concave maximum likelihood estimate is
equivalent to solving a collection of polynomial-exponential systems of a
special form. Even in the case of two equations, very little is known about
solutions to these systems. As an alternative, we use Smale's alpha-theory to
refine approximate numerical solutions and to certify solutions to log-concave
density estimation.
"
"stat.TH","  In reliability theory and survival analysis, the residual entropy is known as
a measure suitable to describe the dynamic information content in stochastic
systems conditional on survival. Aiming to analyze the variability of such
information content, in this paper we introduce the variance of the residual
lifetimes, ""residual varentropy"" in short. After a theoretical investigation of
some properties of the residual varentropy, we illustrate certain applications
related to the proportional hazards model and the first-passage times of an
Ornstein-Uhlenbeck jump-diffusion process.
"
"stat.TH","  We propose a difference-based nonparametric methodology for the estimation
and inference of the time-varying auto-covariance functions of a locally
stationary time series when it is contaminated by a complex trend with both
abrupt and smooth changes. Simultaneous confidence bands (SCB) with
asymptotically correct coverage probabilities are constructed for the
auto-covariance functions under complex trend. A simulation-assisted
bootstrapping method is proposed for the practical construction of the SCB.
Detailed simulation and a real data example round out our presentation.
"
"stat.TH","  We build and study a data-driven procedure for the estimation of the
stationary density f of an additive fractional SDE. To this end, we also prove
some new concentrations bounds for discrete observations of such dynamics in
stationary regime.
"
"stat.TH","  We introduce a new mixture autoregressive model which combines Gaussian and
Student's $t$ mixture components. The model has very attractive properties
analogous to the Gaussian and Student's $t$ mixture autoregressive models, but
it is more flexible as it enables to model series which consist of both
conditionally homoscedastic Gaussian regimes and conditionally heteroscedastic
Student's $t$ regimes. The usefulness of our model is demonstrated in an
empirical application to the monthly U.S. interest rate spread between the
3-month Treasury bill rate and the effective federal funds rate.
"
"stat.TH","  Variational and Bayesian methods are two approaches that have been widely
used to solve image reconstruction problems. In this paper, we propose original
connections between Hamilton--Jacobi (HJ) partial differential equations and a
broad class of Bayesian methods and posterior mean estimators with Gaussian
data fidelity term and log-concave prior. Whereas solutions to certain
first-order HJ PDEs with initial data describe maximum a posteriori estimators
in a Bayesian setting, here we show that solutions to some viscous HJ PDEs with
initial data describe a broad class of posterior mean estimators. These
connections allow us to establish several representation formulas and optimal
bounds involving the posterior mean estimate. In particular, we use these
connections to HJ PDEs to show that some Bayesian posterior mean estimators can
be expressed as proximal mappings of twice continuously differentiable
functions, and furthermore we derive a representation formula for these
functions.
"
"stat.TH","  In this paper, we use the class of Wasserstein metrics to study asymptotic
properties of posterior distributions. Our first goal is to provide sufficient
conditions for posterior consistency. In addition to the well-known Schwartz's
Kullback--Leibler condition on the prior, the true distribution and most
probability measures in the support of the prior are required to possess
moments up to an order which is determined by the order of the Wasserstein
metric. We further investigate convergence rates of the posterior distributions
for which we need stronger moment conditions. The required tail conditions are
sharp in the sense that the posterior distribution may be inconsistent or
contract slowly to the true distribution without these conditions. Our study
involves techniques that build on recent advances on Wasserstein convergence of
empirical measures. We apply the results to density estimation with a Dirichlet
process mixture prior and conduct a simulation study for further illustration.
"
"stat.TH","  We point out necessary and sufficient conditions of uniform consistency of
nonparametric sets of alternatives for widespread nonparametric tests.
Nonparametric sets of alternatives can be defined both in terms of distribution
function and in terms of density (or signals in the problem of signal detection
in Gaussian white noise). In this part of paper such conditions are provided
for $\chi^2-$tests with increasing number of cells, Cramer-von Mises tests,
tests generated $\mathbb{L}_2$- norms of kernel estimators and tests generated
quadratic forms of estimators of Fourier coefficients.
"
"stat.TH","  The so far most general identification result in the context of nonparametric
transformation models is proven. The result is constructive in the sense that
it provides an explicit expression of the transformation function.
"
"stat.TH","  In this paper, we propose an estimator of the generalized maximum mean
discrepancy between several distributions, constructed by modifying a naive
estimator. Asymptotic normality is obtained for this estimator both under
equality of these distributions and under the alternative hypothesis.
"
"stat.TH","  Given an iid sample of a distribution supported on a smooth manifold
M\subsetR^d , which is assumed to be absolutely continuous w.r.t the Hausdorff
measure inherited from the ambient space, we tackle the problem of the
estimation of the level sets of the density f . A consistent estimator in both
Hausdorff distance and distance in measure is proposed. The estimator is the
level set of the kernel-based estimator of the density f . We prove that the
kernel-based density estimator converges uniformly to the unknown density f ,
the consistency of the level set and the consistency of the boundary of the
level set estimator. The performance of our proposal is illustrated through
some simulated examples.
"
"stat.TH","  We investigate the benign overfitting phenomenon in the large deviation
regime where the bounds on the prediction risk hold with probability
$1-e^{-\zeta n}$, for some absolute constant $\zeta$. We prove that these
bounds can converge to $0$ for the quadratic loss. We obtain this result by a
new analysis of the interpolating estimator with minimal Euclidean norm,
relying on a preliminary localization of this estimator with respect to the
Euclidean norm. This new analysis complements and strengthens particular cases
obtained in previous works for the square loss and is extended to other loss
functions. To illustrate this, we also provide excess risk bounds for the Huber
and absolute losses, two widely spread losses in robust statistics.
"
"stat.TH","  In this paper we develop statistical inference tools for high dimensional
functional time series. We introduce a new concept of physical dependent
processes in the space of square integrable functions, which adopts the idea of
basis decomposition of functional data in these spaces, and derive Gaussian and
multiplier bootstrap approximations for sums of high dimensional functional
time series. These results have numerous important statistical consequences.
Exemplarily, we consider the development of joint simultaneous confidence bands
for the mean functions and the construction of tests for the hypotheses that
the mean functions in the spatial dimension are parallel. The results are
illustrated by means of a small simulation study and in the analysis of
Canadian temperature data.
"
"stat.TH","  In matrix-valued datasets the sampled matrices often exhibit correlations
among both their rows and their columns. A useful and parsimonious model of
such dependence is the matrix normal model, in which the covariances among the
elements of a random matrix are parameterized in terms of the Kronecker product
of two covariance matrices, one representing row covariances and one
representing column covariance. An appealing feature of such a matrix normal
model is that the Kronecker covariance structure allows for standard likelihood
inference even when only a very small number of data matrices is available. For
instance, in some cases a likelihood ratio test of dependence may be performed
with a sample size of one. However, more generally the sample size required to
ensure boundedness of the matrix normal likelihood or the existence of a unique
maximizer depends in a complicated way on the matrix dimensions. This motivates
the study of how large a sample size is needed to ensure that maximum
likelihood estimators exist, and exist uniquely with probability one. Our main
result gives precise sample size thresholds in the paradigm where the number of
rows and the number of columns of the data matrices differ by at most a factor
of two. Our proof uses invariance properties that allow us to consider data
matrices in canonical form, as obtained from the Kronecker canonical form for
matrix pencils.
"
"stat.TH","  We derive improved regression and classification rates for support vector
machines using Gaussian kernels under the assumption that the data has some
low-dimensional intrinsic structure that is described by the box-counting
dimension. Under some standard regularity assumptions for regression and
classification we prove learning rates, in which the dimension of the ambient
space is replaced by the box-counting dimension of the support of the data
generating distribution. In the regression case our rates are minimax optimal,
whereas in the classification case our rates are of the form of the best known.
Furthermore, we show that a training validation approach for choosing the
hyperparameters of an SVM in a data dependent way achieves the same rates
adaptively, that is without any knowledge on the data generating distribution.
"
"stat.TH","  We propose a non-parametric, two-sample Bayesian test for checking whether or
not two data sets share a common distribution. The test makes use of data
splitting ideas and does not require priors for high-dimensional parameter
vectors as do other nonparametric Bayesian procedures. We provide evidence that
the new procedure provides more stable Bayes factors than do methods based on
P\'olya trees. Somewhat surprisingly, the behavior of the proposed Bayes
factors when the two distributions are the same is usually superior to that of
P\'olya tree Bayes factors. We showcase the effectiveness of the test by
proving its consistency, conducting a simulation study and applying the test to
Higgs boson data.
"
"stat.TH","  In a standard Bayesian setting, there is often ambiguity in prior choice, as
one may have not sufficient information to uniquely identify a suitable prior
probability measure encapsulating initial beliefs. To overcome this, we specify
a set $\mathcal{P}$ of plausible prior probability measures. As more and more
observations are collected, $\mathcal{P}$ is updated using Jeffrey's rule of
conditioning, a generalization of Bayesian updating which proves to be more
philosophically compelling in many situations. We build the sequence
$(\mathcal{P}^*_k)$ of successive updates of $\mathcal{P}$ and we provide an
ergodic theory to analyze its limit, for both countable and uncountable sample
spaces. A result of this ergodic theory is a strong law of large numbers in the
uncountable setting. We also give a rule, that we call Jeffrey-Geometric rule,
to update lower probabilities associated with the elements of
$(\mathcal{P}^*_k)$.
"
"stat.TH","  This paper revisits the offline change-point detection problem from a
statistical learning perspective. Instead of assuming that the underlying pre-
and post-change distributions are known, it is assumed that we have partial
knowledge of these distributions based on empirically observed statistics in
the form of training sequences. Our problem formulation finds a variety of
real-life applications from detecting when climate change occurred to detecting
when a virus mutated. Using the training sequences as well as the test sequence
consisting of a single-change and allowing for the erasure or rejection option,
we derive the optimal resolution between the estimated and true change-points
under two different asymptotic regimes on the undetected error probability --
namely, the large and moderate deviations regimes. In both regimes, strong
converses are also proved. In the moderate deviations case, the optimal
resolution is a simple function of a symmetrized version of the chi-square
distance.
"
"stat.TH","  An independence model for discrete random variables is a Segre-Veronese
variety in a probability simplex. Any metric on the set of joint states of the
random variables induces a Wasserstein metric on the probability simplex. The
unit ball of this polyhedral norm is dual to the Lipschitz polytope. Given any
data distribution, we seek to minimize its Wasserstein distance to a fixed
independence model. The solution to this optimization problem is a piecewise
algebraic function of the data. We compute this function explicitly in small
instances, we examine its combinatorial structure and algebraic degrees in the
general case, and we present some experimental case studies.
"
"stat.TH","  Bayesian statistical inference loses predictive optimality when generative
models are misspecified.
  Working within an existing coherent loss-based generalisation of Bayesian
inference, we show existing Modular/Cut-model inference is coherent, and write
down a new family of Semi-Modular Inference (SMI) schemes, indexed by an
influence parameter, with Bayesian inference and Cut-models as special cases.
We give a meta-learning criterion and estimation procedure to choose the
inference scheme. This returns Bayesian inference when there is no
misspecification.
  The framework applies naturally to Multi-modular models. Cut-model inference
allows directed information flow from well-specified modules to misspecified
modules, but not vice versa. An existing alternative power posterior method
gives tunable but undirected control of information flow, improving prediction
in some settings. In contrast, SMI allows tunable and directed information flow
between modules.
  We illustrate our methods on two standard test cases from the literature and
a motivating archaeological data set.
"
"stat.TH","  The problem of identifying change points in high-dimensional Gaussian
graphical models (GGMs) in an online fashion is of interest, due to new
applications in biology, economics and social sciences. The offline version of
the problem, where all the data are a priori available, has led to a number of
methods and associated algorithms involving regularized loss functions.
However, for the online version, there is currently only a single work in the
literature that develops a sequential testing procedure and also studies its
asymptotic false alarm probability and power. The latter test is best suited
for the detection of change points driven by global changes in the structure of
the precision matrix of the GGM, in the sense that many edges are involved.
Nevertheless, in many practical settings the change point is driven by local
changes, in the sense that only a small number of edges exhibit changes. To
that end, we develop a novel test to address this problem that is based on the
$\ell_\infty$ norm of the normalized covariance matrix of an appropriately
selected portion of incoming data. The study of the asymptotic distribution of
the proposed test statistic under the null (no presence of a change point) and
the alternative (presence of a change point) hypotheses requires new technical
tools that examine maxima of graph-dependent Gaussian random variables, and
that of independent interest. It is further shown that these tools lead to the
imposition of mild regularity conditions for key model parameters, instead of
more stringent ones required by leveraging previously used tools in related
problems in the literature. Numerical work on synthetic data illustrates the
good performance of the proposed detection procedure both in terms of
computational and statistical efficiency across numerous experimental settings.
"
"stat.TH","  Designs found by maximizing the expected Fisher information gain can result
in a singular Fisher information matrix. This leads to non-unique classical
estimates and ill-conditioning of posterior computation. A mitigating strategy
for finding designs using Fisher information gain is proposed.
"
"stat.TH","  We study the problem of exact support recovery: given an (unknown) vector
$\theta \in \left\{-1,0,1\right\}^D$, we are given access to the noisy
measurement $$ y = X\theta + \omega,$$ where $X \in \mathbb{R}^{N \times D}$ is
a (known) Gaussian matrix and the noise $\omega \in \mathbb{R}^N$ is an
(unknown) Gaussian vector. How small we can choose $N$ and still reliably
recover the support of $\theta$? We present RAWLS (Randomly Aggregated
UnWeighted Least Squares Support Recovery): the main idea is to take random
subsets of the $N$ equations, perform a least squares recovery over this
reduced bit of information and then average over many random subsets. We show
that the proposed procedure can provably recover an approximation of $\theta$
and demonstrate its use in support recovery through numerical examples.
"
"stat.TH","  Diagonal preconditioning has been a staple technique in optimization and
machine learning. It often reduces the condition number of the design or
Hessian matrix it is applied to, thereby speeding up convergence. However,
rigorous analyses of how well various diagonal preconditioning procedures
improve the condition number of the preconditioned matrix and how that
translates into improvements in optimization are rare. In this paper, we first
provide an analysis of a popular diagonal preconditioning technique based on
column standard deviation and its effect on the condition number using random
matrix theory. Then we identify a class of design matrices whose condition
numbers can be reduced significantly by this procedure. We then study the
problem of optimal diagonal preconditioning to improve the condition number of
any full-rank matrix and provide a bisection algorithm and a potential
reduction algorithm with $O(\log(\frac{1}{\epsilon}))$ iteration complexity,
where each iteration consists of an SDP feasibility problem and a Newton update
using the Nesterov-Todd direction, respectively. Finally, we extend the optimal
diagonal preconditioning algorithm to an adaptive setting and compare its
empirical performance at reducing the condition number and speeding up
convergence for regression and classification problems with that of another
adaptive preconditioning technique, namely batch normalization, that is
essential in training machine learning models.
"
"stat.TH","  We consider a stationary linear AR($p$) model with observations subject to
gross errors (outliers). The autoregression parameters are unknown as well as
the distribution and moments of innoovations. The distribution of outliers
$\Pi$ is unknown and arbitrary, their intensity is $\gamma n^{-1/2}$ with an
unknown $\gamma$, $n$ is the sample size. The autoregression parameters are
estimated by any estimator which is $n^{1/2}$-consistent uniformly in
$\gamma\leq \Gamma<\infty$. Using the residuals from the estimated
autoregression, we construct a kind of empirical distribution function
(e.d.f.), which is a counterpart of the (inaccessible) e.d.f. of the
autoregression innovations. We obtain a stochastic expansion of this e.d.f.,
which enables us to construct the symmetrized test of Pearson's chi-square type
for the normality of distribution of innovations. We establish qualitative
robustness of these tests in terms of uniform equicontinuity of the limiting
levels (as functions of $\gamma$ and $\Pi$) with respect to $\gamma$ in a
neighborhood of $\gamma=0$.
"
"stat.TH","  This paper considers a sensor attack and fault detection problem for linear
cyber-physical systems, which are subject to system noise that can obey an
unknown light-tailed distribution. We propose a new threshold-based detection
mechanism that employs the Wasserstein metric, and which guarantees system
performance with high confidence employing a finite number of measurements. The
proposed detector may generate false alarms with a rate $\Delta$ in normal
operation, where $\Delta$ can be tuned to be arbitrarily small by means of a
benchmark distribution which is part of our mechanism. Thus, the proposed
detector is sensitive to sensor attacks and faults which have a statistical
behavior that is different from that of the system's noise. We quantify the
impact of stealthy attacks---which aim to perturb the system operation while
producing false alarms that are consistent with the natural system's
noise---via a probabilistic reachable set. To enable tractable implementation
of our methods, we propose a linear optimization problem that computes the
proposed detection measure and a semidefinite program that produces the
proposed reachable set.
"
"stat.TH","  The sparse factorization of a large matrix is fundamental in modern
statistical learning. In particular, the sparse singular value decomposition
and its variants have been utilized in multivariate regression, factor
analysis, biclustering, vector time series modeling, among others. The appeal
of this factorization is owing to its power in discovering a
highly-interpretable latent association network, either between samples and
variables or between responses and predictors. However, many existing methods
are either ad hoc without a general performance guarantee, or are
computationally intensive, rendering them unsuitable for large-scale studies.
We formulate the statistical problem as a sparse factor regression and tackle
it with a divide-and-conquer approach. In the first stage of division, we
consider both sequential and parallel approaches for simplifying the task into
a set of co-sparse unit-rank estimation (CURE) problems, and establish the
statistical underpinnings of these commonly-adopted and yet poorly understood
deflation methods. In the second stage of division, we innovate a contended
stagewise learning technique, consisting of a sequence of simple incremental
updates, to efficiently trace out the whole solution paths of CURE. Our
algorithm has a much lower computational complexity than alternating convex
search, and the choice of the step size enables a flexible and principled
tradeoff between statistical accuracy and computational efficiency. Our work is
among the first to enable stagewise learning for non-convex problems, and the
idea can be applicable in many multi-convex problems. Extensive simulation
studies and an application in genetics demonstrate the effectiveness and
scalability of our approach.
"
"stat.TH","  We present a new finite-time analysis of the estimation error of the Ordinary
Least Squares (OLS) estimator for stable linear time-invariant systems. We
characterize the number of observed samples (the length of the observed
trajectory) sufficient for the OLS estimator to be $(\varepsilon,\delta)$-PAC,
i.e., to yield an estimation error less than $\varepsilon$ with probability at
least $1-\delta$. We show that this number matches existing sample complexity
lower bounds [1,2] up to universal multiplicative factors (independent of
($\varepsilon,\delta)$ and of the system). This paper hence establishes the
optimality of the OLS estimator for stable systems, a result conjectured in
[1]. Our analysis of the performance of the OLS estimator is simpler, sharper,
and easier to interpret than existing analyses. It relies on new concentration
results for the covariates matrix.
"
"stat.TH","  The Hurst exponent is the simplest numerical summary of self-similar
long-range dependent stochastic processes. We consider the estimation of Hurst
exponent in long-range dependent curve time series. Our estimation method
begins by constructing an estimate of the long-run covariance function, which
we use, via dynamic functional principal component analysis, in estimating the
orthonormal functions spanning the dominant sub-space of functional time
series. Within the context of functional autoregressive fractionally integrated
moving average models, we compare finite-sample bias, variance and mean square
error among some time- and frequency-domain Hurst exponent estimators and make
our recommendations.
"
"stat.TH","  We establish the geometric ergodicity of the preconditioned Hamiltonian Monte
Carlo (HMC) algorithm defined on an infinite-dimensional Hilbert space, as
developed in [Beskos et al., Stochastic Process. Appl., 2011]. This algorithm
can be used as a basis to sample from certain classes of target measures which
are absolutely continuous with respect to a Gaussian measure. Our work
addresses an open question posed in [Beskos et al., Stochastic Process. Appl.,
2011], and provides an alternative to a recent proof based on exact coupling
techniques given in arXiv:1909.07962. The approach here establishes convergence
in a suitable Wasserstein distance by using the weak Harris theorem together
with a generalized coupling argument. We also show that a law of large numbers
and central limit theorem can be derived as a consequence of our main
convergence result. Moreover, our approach yields a novel proof of mixing rates
for the classical finite-dimensional HMC algorithm. As such, the methodology we
develop provides a flexible framework to tackle the rigorous convergence of
other Markov Chain Monte Carlo algorithms. Additionally, we show that the scope
of our result includes certain measures that arise in the Bayesian approach to
inverse PDE problems, cf. [Stuart, Acta Numer., 2010]. Particularly, we verify
all of the required assumptions for a certain class of inverse problems
involving the recovery of a divergence free vector field from a passive scalar,
arXiv:1808.01084v3.
"
"stat.TH","  We explore how violations of the often-overlooked standard assumption that
the random effects model matrix in a linear mixed model is fixed (and thus
independent of the random effects vector) can lead to bias in estimators of
estimable functions of the fixed effects. However, if the random effects of the
original mixed model are instead also treated as fixed effects, or if the fixed
and random effects model matrices are orthogonal with respect to the inverse of
the error covariance matrix (with probability one), or if the random effects
and the corresponding model matrix are independent, then these estimators are
unbiased. The bias in the general case is quantified and compared to a
randomized permutation distribution of the predicted random effects, producing
an informative summary graphic for each estimator of interest. This is
demonstrated through the examination of sporting outcomes used to estimate a
home field advantage.
"
"stat.TH","  We consider the setting of online logistic regression and consider the regret
with respect to the 2-ball of radius B. It is known (see [Hazan et al., 2014])
that any proper algorithm which has logarithmic regret in the number of samples
(denoted n) necessarily suffers an exponential multiplicative constant in B. In
this work, we design an efficient improper algorithm that avoids this
exponential constant while preserving a logarithmic regret. Indeed, [Foster et
al., 2018] showed that the lower bound does not apply to improper algorithms
and proposed a strategy based on exponential weights with prohibitive
computational complexity. Our new algorithm based on regularized empirical risk
minimization with surrogate losses satisfies a regret scaling as O(B log(Bn))
with a per-round time-complexity of order O(d^2).
"
"stat.TH","  We develop tail probability bounds for matrix linear combinations with
matrix-valued coefficients and matrix-valued quadratic forms. These results
extend well-known scalar case results such as the Hanson--Wright inequality,
and matrix concentration inequalities such as the matrix Bernstein inequality.
A key intermediate result is a deviation bound for matrix-valued $U$-statistics
of order two and their independent sums. As an application of these probability
tools in statistical inference, we establish the consistency of a novel
bias-adjusted spectral clustering method in multi-layer stochastic block models
with general signal structures.
"
"stat.TH","  Recent theoretical work in causal inference has explored an important class
of variables which, when conditioned on, may further amplify existing
unmeasured confounding bias (bias amplification). Despite this theoretical
work, existing simulations of bias amplification in clinical settings have
suggested bias amplification may not be as important in many practical cases as
suggested in the theoretical literature.We resolve this tension by using tools
from the semi-parametric regression literature leading to a general
characterization in terms of the geometry of OLS estimators which allows us to
extend current results to a larger class of DAGs, functional forms, and
distributional assumptions. We further use these results to understand the
limitations of current simulation approaches and to propose a new framework for
performing causal simulation experiments to compare estimators. We then
evaluate the challenges and benefits of extending this simulation approach to
the context of a real clinical data set with a binary treatment, laying the
groundwork for a principled approach to sensitivity analysis for bias
amplification in the presence of unmeasured confounding.
"
"stat.TH","  A stochastic hybrid system, also known as a switching diffusion, is a
continuous-time Markov process with state space consisting of discrete and
continuous parts. We consider parametric estimation of theQmatrix for the
discrete state transitions and of the drift coefficient for the diffusion part.
First, we derive the likelihood function under the complete observation of a
sample path in continuous-time. Then, extending a finite-dimensional filter for
hidden Markov models developed by Elliott et al. (Hidden Markov Models,
Springer, 1995) to stochastic hybrid systems, we derive the likelihood function
and the EM algorithm under a partial observation where the continuous state is
monitored continuously in time, while the discrete state is unobserved.
"
"stat.TH","  We study admissibility of a subclass of generalized Bayes estimators of a
multivariate normal vector when the variance is unknown, under scaled quadratic
loss. Minimaxity is also established for certain of these estimators.
"
"stat.TH","  We study the relative entropy of the empirical probability vector with
respect to the true probability vector in multinomial sampling of $k$
categories, which, when multiplied by sample size $n$, is also the
log-likelihood ratio statistic. We generalize a recent result and show that the
moment generating function of the statistic is bounded by a polynomial of
degree $n$ on the unit interval, uniformly over all true probability vectors.
We characterize the family of polynomials indexed by $(k,n)$ and obtain
explicit formulae. Consequently, we develop Chernoff-type tail bounds,
including a closed-form version from a large sample expansion of the bound
minimizer. Our bound dominates the classic method-of-types bound and is
competitive with the state of the art. We demonstrate with an application to
estimating the proportion of unseen butterflies.
"
"stat.TH","  Sequential change point tests aim at giving an alarm as soon as possible
after a structural break occurs while controlling the asymptotic false alarm
error. For such tests it is of particular importance to understand how quickly
a break is detected. While this is often assessed by simulations only, in this
paper, we derive the asymptotic distribution of the delay time for sequential
change point procedures based on U-statistics. This includes the
difference-of-means (DOM) sequential test, that has been discussed previously,
but also a new robust Wilcoxon sequential change point test. Similar to
asymptotic relative efficiency in an a-posteriori setting, the results allow us
to compare the detection delay of the two procedures. It is shown that the
Wilcoxon sequential procedure has a smaller detection delay for heavier tailed
distributions which is also confirmed by simulations. While the previous
literature only derives results for early change points, we obtain the
asymptotic distribution of the delay time for both early as well as late change
points. Finally, we evaluate how well the asymptotic distribution approximates
the actual stopping times for finite samples via a simulation study.
"
"stat.TH","  The main goal of this paper is to build consistent and asymptotically normal
estimators for the drift and volatility parameter of the stochastic heat
equation driven by an additive space-only white noise when the solution is
sampled discretely in the physical domain. We consider both the full space R
and the bounded domain (0,\pi). First, we establish the exact regularity of the
solution and its spatial derivative, which in turn, using power-variation
arguments, allows building the desired estimators. Second, we propose two sets
of estimators, based on sampling either the spatial derivative or the solution
itself on a discrete space-time grid. Using the so-called Malliavin-Stein's
method, we prove that these estimators are consistent and asymptotically normal
as the spatial mesh-size vanishes. More importantly, we show that naive
approximations of the derivatives appearing in the power-variation based
estimators may create nontrivial biases, which we compute explicitly. We
conclude with some numerical experiments that illustrate the theoretical
results.
"
"stat.TH","  This work studies properties of the conditional mean estimator in vector
Poisson noise. The main emphasis is to study conditions on prior distributions
that induce linearity of the conditional mean estimator. The paper consists of
two main results. The first result shows that the only distribution that
induces the linearity of the conditional mean estimator is a product gamma
distribution. Moreover, it is shown that the conditional mean estimator cannot
be linear when the dark current parameter of the Poisson noise is non-zero. The
second result produces a quantitative refinement of the first result.
Specifically, it is shown that if the conditional mean estimator is close to
linear in a mean squared error sense, then the prior distribution must be close
to a product gamma distribution in terms of their characteristic functions.
Finally, the results are compared to their Gaussian counterparts.
"
"stat.TH","  This note uses a conformal prediction procedure to provide further support on
several points discussed by Professor Efron (Efron, 2020) concerning
prediction, estimation and IID assumption. It aims to convey the following
messages: (1) Under the IID (e.g., random split of training and testing data
sets) assumption, prediction is indeed an easier task than estimation, since
prediction has a 'homeostasis property' in this case -- Even if the model used
for learning is completely wrong, the prediction results maintain valid. (2) If
the IID assumption is violated (e.g., a targeted prediction on specific
individuals), the homeostasis property is often disrupted and the prediction
results under a wrong model are usually invalid. (3) Better model estimation
typically leads to more accurate prediction in both IID and non-IID cases. Good
modeling and estimation practices are important and, in many times, crucial for
obtaining good prediction results. The discussion also provides one explanation
why the deep learning method works so well in academic exercises (with
experiments set up by randomly splitting the entire data into training and
testing data sets), but fails to deliver many `killer applications' in real
world applications.
"
"stat.TH","  Principled nonparametric tests for regression curvature in $\mathbb{R}^{d}$
are often statistically and computationally challenging. This paper introduces
the stratified incomplete local simplex (SILS) tests for joint concavity of
nonparametric multiple regression. The SILS tests with suitable bootstrap
calibration are shown to achieve simultaneous guarantees on dimension-free
computational complexity, polynomial decay of the uniform error-in-size, and
power consistency for general (global and local) alternatives. To establish
these results, a general theory for incomplete $U$-processes with stratified
random sparse weights is developed. Novel technical ingredients include maximal
inequalities for the supremum of multiple incomplete $U$-processes.
"
"stat.TH","  The extreme values theory presents specific tools for modeling and predicting
extreme phenomena. In particular, risk assessment is often analyzed through
measures for tail dependence and high values clustering. Despite technological
advances allowing an increasingly larger and more efficient data collection,
there are sometimes failures in the records, which causes difficulties in
statistical inference, especially in the tail where data are scarcer. In this
article we present a model with a simple and intuitive failures scheme, where
each record failure is replaced by the last record available. We will study its
extremal behavior with regard to local dependence and high values clustering,
as well as the temporal dependence on the tail.
"
"stat.TH","  We consider a combined state and drift estimation problem for the linear
stochastic heat equation. The infinite-dimensional Bayesian inference problem
is formulated in terms of the Kalman-Bucy filter over an extended state space,
and its long-time asymptotic properties are studied. Asymptotic posterior
contraction rates in the unknown drift function are the main contribution of
this paper. Such rates have been studied before for stationary non-parametric
Bayesian inverse problems, and here we demonstrate the consistency of our
time-dependent formulation with these previous results building upon scale
separation and a slow manifold approximation.
"
"stat.TH","  We propose non-asymptotic controls of the cumulative distribution function
$P(|X_{t}|\ge \varepsilon)$, for any $t>0$, $\varepsilon>0$ and any L\'evy
process $X$ such that its L\'evy density is bounded from above by the density
of an $\alpha$-stable type L\'evy process in a neighborhood of the origin. The
results presented are non-asymptotic and optimal, they apply to a large class
of L\'evy processes.
"
"stat.TH","  We propose a Bayesian inference framework to estimate uncertainties in
inverse scattering problems. Given the observed data, the forward model and
their uncertainties, we find the posterior distribution over a finite parameter
field representing the objects. To construct the prior distribution we use a
topological sensitivity analysis. We demonstrate the approach on the Bayesian
solution of 2D inverse problems in light and acoustic holography with synthetic
data. Statistical information on objects such as their center location,
diameter size, orientation, as well as material properties, are extracted by
sampling the posterior distribution. Assuming the number of objects known,
comparison of the results obtained by Markov Chain Monte Carlo sampling and by
sampling a Gaussian distribution found by linearization about the maximum a
posteriori estimate show reasonable agreement. The latter procedure has low
computational cost, which makes it an interesting tool for uncertainty studies
in 3D. However, MCMC sampling provides a more complete picture of the posterior
distribution and yields multi-modal posterior distributions for problems with
larger measurement noise. When the number of objects is unknown, we devise a
stochastic model selection framework.
"
"stat.TH","  We propose a nonparametric parameter estimation of confidence intervals when
the underlying has large or infinite variance. We explain the method by a
simple numerical example and provide an application to estimate the coupling
strength in neuronal networks.
"
"stat.TH","  Graph sampling is a statistical approach to study real graphs, which
represent the structure of many technological, social or biological phenomena
of interest. We develop bipartite incident graph sampling (BIGS) as a feasible
representation of graph sampling from arbitrary finite graphs. It provides also
a unified treatment of the existing unconventional sampling methods which were
studied separately in the past, including indirect, network and adaptive
cluster sampling. The sufficient and necessary conditions of feasible BIGS
representation are established, given which one can apply a family of
Hansen-Hurwitz type design-unbiased estimators in addition to the standard
Horvitz-Thompson estimator. The approach increases therefore the potentials of
efficiency gains in graph sampling. A general result regarding the relative
efficiency of the two types of estimators is obtained. Numerical examples are
given to illustrate the versatility of the proposed approach.
"
"stat.TH","  The determination of an optimal design for a given regression problem is an
intricate optimization problem, especially for models with multivariate
predictors. Design admissibility and invariance are main tools to reduce the
complexity of the optimization problem and have been successfully applied for
models with univariate predictors. In particular several authors have developed
sufficient conditions for the existence of saturated designs in univariate
models, where the number of support points of the optimal design equals the
number of parameters. These results generalize the celebrated de la Garza
phenomenon (de la Garza, 1954) which states that for a polynomial regression
model of degree $p-1$ any optimal design can be based on at most $p$ points.
This paper provides - for the first time - extensions of these results for
models with a multivariate predictor. In particular we study a geometric
characterization of the support points of an optimal design to provide
sufficient conditions for the occurrence of the de la Garza phenomenon in
models with multivariate predictors and characterize properties of admissible
designs in terms of admissibility of designs in conditional univariate
regression models.
"
"stat.TH","  Over the last three decades, there has been a considerable effort within the
applied probability community to develop techniques for bounding the
convergence rates of general state space Markov chains. Most of these results
assume the existence of drift and minorization (d\&m) conditions. It has often
been observed that convergence rate bounds based on single-step d\&m tend to be
overly conservative, especially in high-dimensional situations. This article
builds a framework for studying this phenomenon. It is shown that any
convergence rate bound based on a set of d\&m conditions cannot do better than
a certain unknown optimal bound. Strategies are designed to put bounds on the
optimal bound itself, and this allows one to quantify the extent to which a
d\&m-based convergence rate bound can be sharp. The new theory is applied to
several examples, including a Gaussian autoregressive process (whose true
convergence rate is known), and a Metropolis adjusted Langevin algorithm. The
results strongly suggest that convergence rate bounds based on single-step d\&m
conditions are quite inadequate in high-dimensional settings.
"
"stat.TH","  The $p_0$ model is an exponential random graph model for directed networks
with the bi-degree sequence as the exclusively sufficient statistic. It
captures the network feature of degree heterogeneity. The consistency and
asymptotic normality of a differentially private estimator of the parameter in
the private $p_0$ model has been established. However, the $p_0$ model only
focuses on binary edges. In many realistic networks, edges could be weighted,
taking a set of finite discrete values. In this paper, we further show that the
moment estimators of the parameters based on the differentially private
bi-degree sequence in the weighted $p_0$ model are consistent and
asymptotically normal. Numerical studies demonstrate our theoretical findings.
"
"stat.TH","  We introduce a Markov product structure for multivariate tail dependence
functions, building upon the well-known Markov product for copulas. We
investigate algebraic and monotonicity properties of this new product as well
as its role in describing the tail behaviour of the Markov product of copulas.
For the bivariate case, we show additional smoothing properties and derive a
characterization of idempotents together with the limiting behaviour of n-fold
iterations. Finally, we establish a one-to-one correspondence between bivariate
tail dependence functions and a class of positive, substochastic operators.
These operators are contractions both on $L^1(\mathbb{R}_+)$ and
$L^\infty(\mathbb{R}_+)$ and constitute a natural generalization of Markov
operators.
"
"stat.TH","  Gibbs point processes (GPPs) constitute a large and flexible class of spatial
point processes with explicit dependence between the points. They can model
attractive as well as repulsive point patterns. Feature selection procedures
are an important topic in high-dimensional statistical modeling. In this paper,
composite likelihood approach regularized with convex and non-convex penalty
functions is proposed to handle statistical inference for high-dimensional
inhomogeneous GPPs. The composite likelihood incorporates both the
pseudo-likelihood and the logistic composite likelihood. We particularly
investigate the setting where the number of covariates diverges as the domain
of observation increases. Under some conditions provided on the spatial GPP and
on the penalty functions, we show that the oracle property, the consistency and
the asymptotic normality hold. The latter two asymptotic properties also hold
in an unregularized setting which is already a contribution to the current
literature. Through simulation experiments, we validate our theoretical results
and finally, an application to a tropical forestry dataset illustrates the use
of the proposed approach.
"
"stat.TH","  In panel experiments, we randomly expose multiple units to different
interventions and measure their subsequent outcomes, sequentially repeating the
procedure numerous times. Using the potential outcomes framework, we define
finite population dynamic causal effects that capture the relative
effectiveness of alternative treatment paths. For the leading example, known as
the lag-p dynamic causal effects, we provide a nonparametric estimator that is
unbiased over the randomization distribution. We then derive the finite
population limiting distribution of our estimators as either the sample size or
the duration of the experiment increases. Our approach provides a new technique
for deriving finite population central limit theorems that exploits the
underlying Martingale property of unbiased estimators. We further describe two
methods for conducting inference on dynamic causal effects: a conservative test
for weak null hypotheses of zero average causal effects using the limiting
distribution and an exact randomization-based test for sharp null hypotheses.
We also derive the finite population probability limit of commonly-used linear
fixed effects estimators, showing that these estimators perform poorly in the
presence of dynamic causal effects. We conclude with a simulation study and an
empirical application in which we reanalyze a lab experiment on cooperation.
"
"stat.TH","  This paper considers a canonical clustering problem where one receives
unlabeled samples drawn from a balanced mixture of two elliptical distributions
and aims for a classifier to estimate the labels. Many popular methods
including PCA and k-means require individual components of the mixture to be
somewhat spherical, and perform poorly when they are stretched. To overcome
this issue, we propose a non-convex program seeking for an affine transform to
turn the data into a one-dimensional point cloud concentrating around -1 and 1,
after which clustering becomes easy. Our theoretical contributions are
two-fold: (1) we show that the non-convex loss function exhibits desirable
landscape properties as long as the sample size exceeds some constant multiple
of the dimension, and (2) we leverage this to prove that an efficient
first-order algorithm achieves near-optimal statistical precision even without
good initialization. We also propose a general methodology for multi-class
clustering tasks with flexible choices of feature transforms and loss
objectives.
"
"stat.TH","  It is a standard criterium in statistics to define an optimal estimator the
one with the minimum variance. Thus, the optimality is proved with inequality
among variances of competing estimators. The inequalities, demonstrated here,
disfavor the standard least squares estimators. Inequalities among estimators
are connected to names of Cramer, Rao and Frechet. The standard demonstrations
of these inequalities require very special analytical properties of the
probability functions, globally indicated as regular models. These limiting
conditions are too restrictive to handle realistic problems in track fitting. A
previous extension to heteroscedastic models of the Cramer-Rao-Frechet
inequalities was performed with Gaussian distributions. These demonstrations
proved beyond any possible doubts the superiority of the heteroscedastic models
compared to the standard least squares method. However, the Gaussian
distributions are typical members of the required regular models. Instead, the
realistic probability distributions, encountered in tracker detectors, are very
different from Gaussian distributions. Therefore, to have well grounded set of
inequalities, the limitations to regular models must be overtaken. The aim of
this paper is to demonstrate the inequalities for least squares estimators for
irregular models of probabilities, explicitly excluded by the
Cramer-Rao-Frechet demonstrations. Estimators for straight and parabolic tracks
will be considered. The final part deals with the form of the distributions of
simplified heteroscedastic track models reconstructed with optimal estimators
and the standard (non-optimal) estimators. A comparison among the distributions
of these different estimators shows the large loss in resolution of the
standard least-squares estimators.
"
"stat.TH","  We analyze the fluctuations of incomplete $U$-statistics over a triangular
array of independent random variables. We give criteria for a Central Limit
Theorem (CLT, for short) to hold in the sense that we prove that an
appropriately scaled and centered version of the U-statistic converges to a
normal random variable. Our method of proof relies on a martingale CLT. A
possible application -- a CLT for the hitting time for random walk on random
graphs -- will be presented in \cite{LoTe20b}
"
"stat.TH","  The Expectation Maximisation (EM) algorithm is widely used to optimise
non-convex likelihood functions with hidden variables. Many authors modified
its simple design to fit more specific situations. For instance the Expectation
(E) step has been replaced by Monte Carlo (MC) approximations, Markov Chain
Monte Carlo approximations, tempered approximations... Most of the well studied
approximations belong to the stochastic class. By comparison, the literature is
lacking when it comes to deterministic approximations. In this paper, we
introduce a theoretical framework, with state of the art convergence
guarantees, for any deterministic approximation of the E step. We analyse
theoretically and empirically several approximations that fit into this
framework. First, for cases with intractable E steps, we introduce a
deterministic alternative to the MC-EM, using Riemann sums. This method is easy
to implement and does not require the tuning of hyper-parameters. Then, we
consider the tempered approximation, borrowed from the Simulated Annealing
optimisation technique and meant to improve the EM solution. We prove that the
the tempered EM verifies the convergence guarantees for a wide range of
temperature profiles. We showcase empirically how it is able to escape
adversarial initialisations. Finally, we combine the Riemann and tempered
approximations to accomplish both their purposes.
"
"stat.TH","  In this paper, we are interested in the problem of smoothing parameter
selection in nonparametric curve estimation under dependent errors. We focus on
kernel estimation and the case when the errors form a general stationary
sequence of martingale difference random variables where neither linearity
assumption nor ""all moments are finite"" are required.We compare the behaviors
of the smoothing bandwidths obtained by minimizing three criteria: the average
squared error, the mean average squared error and a Mallows-type criterion
adapted to the dependent case. We prove that these three minimizers are
first-order equivalent in probability. We give also a normal asymptotic
behavior of the gap between the minimizer of the average square error and that
of the Mallows-type criterion.Finally, we apply our theoretical results to a
specific case of martingale difference sequence, namely the Auto-Regressive
Conditional Heteroscedastic (ARCH(1)) process.A Monte-carlo simulation study,
for this regression model with ARCH(1) process, is conducted.
"
"stat.TH","  We propose a novel strategy for multivariate extreme value index estimation.
In applications such as finance, volatility and risk present in the components
of a multivariate time series are often driven by the same underlying factors,
such as the subprime crisis in the US. To estimate the latent risk, we apply a
two-stage procedure. First, a set of independent latent series is estimated
using a method of latent variable analysis. Then, univariate risk measures are
estimated individually for the latent series to assess their contribution to
the overall risk. As our main theoretical contribution, we derive conditions
under which the effect of the first step to the asymptotic behavior of the risk
estimators is negligible. Simulations demonstrate the theory under both i.i.d.
and dependent data, and an application into financial data illustrates the
usefulness of the method in extracting joint sources of risk in practice.
"
"stat.TH","  Cyber security is an important concern for all individuals, organisations and
governments globally. Cyber attacks have become more sophisticated, frequent
and dangerous than ever, and traditional anomaly detection methods have been
proven to be less effective when dealing with these new classes of cyber
threats. In order to address this, both classical and Bayesian statistical
models offer a valid and innovative alternative to the traditional
signature-based methods, motivating the increasing interest in statistical
research that it has been observed in recent years. In this review we provide a
description of some typical cyber security challenges, typical types of data
and statistical methods, paying special attention to Bayesian approaches for
these problems.
"
"stat.TH","  We study minimax rates of convergence in the label shift problem. In addition
to the usual setting in which the learner only has access to unlabeled examples
from the target domain, we also consider the setting in which a small number of
labeled examples from the target domain are available to the learner. Our study
reveals a difference in the difficulty of the label shift problem in the two
settings. We attribute this difference to the availability of data from the
target domain to estimate the class conditional distributions in the latter
setting. We also show that a distributional matching approach is minimax
rate-optimal in the former setting.
"
"stat.TH","  Stochastic gradient descent (SGD) is a popular algorithm for optimization
problems arising in high-dimensional inference tasks. Here one produces an
estimator of an unknown parameter from independent samples of data by
iteratively optimizing a loss function. This loss function is random and often
non-convex. We study the performance of the simplest version of SGD, namely
online SGD, from a random start in the setting where the parameter space is
high-dimensional.
  We develop nearly sharp thresholds for the number of samples needed for
consistent estimation as one varies the dimension. Our thresholds depend only
on an intrinsic property of the population loss which we call the information
exponent. In particular, our results do not assume uniform control on the loss
itself, such as convexity or uniform derivative bounds. The thresholds we
obtain are polynomial in the dimension and the precise exponent depends
explicitly on the information exponent. As a consequence of our results, we
find that except for the simplest tasks, almost all of the data is used simply
in the initial search phase to obtain non-trivial correlation with the ground
truth. Upon attaining non-trivial correlation, the descent is rapid and
exhibits law of large numbers type behaviour.
  We illustrate our approach by applying it to a wide set of inference tasks
such as phase retrieval, parameter estimation for generalized linear models,
spiked matrix models, and spiked tensor models, as well as for supervised
learning for single-layer networks with general activation functions.
"
"stat.TH","  We consider a broad class of Approximate Message Passing (AMP) algorithms
defined as a Lipschitzian functional iteration in terms of an $n\times n$
random symmetric matrix $A$. We establish universality in noise for this AMP in
the $n$-limit and validate this behavior in a number of AMPs popularly adapted
in compressed sensing, statistical inferences, and optimizations in spin
glasses.
"
"stat.TH","  We extend the construction principle of multivariate phase-type distributions
to establish an analytically tractable class of heavy-tailed multivariate
random variables whose marginal distributions are of Mittag-Leffler type with
arbitrary index of regular variation. The construction can essentially be seen
as allowing a scalar parameter to become matrix-valued. The class of
distributions is shown to be dense among all multivariate positive random
variables and hence provides a versatile candidate for the modelling of
heavy-tailed, but tail-independent, risks in various fields of application.
"
"stat.TH","  In the context of neural network models, overparametrization refers to the
phenomena whereby these models appear to generalize well on the unseen data,
even though the number of parameters significantly exceeds the sample sizes,
and the model perfectly fits the in-training data. A conventional explanation
of this phenomena is based on self-regularization properties of algorithms used
to train the data. In this paper we prove a series of results which provide a
somewhat diverging explanation. Adopting a teacher/student model where the
teacher network is used to generate the predictions and student network is
trained on the observed labeled data, and then tested on out-of-sample data, we
show that any student network interpolating the data generated by a teacher
network generalizes well, provided that the sample size is at least an explicit
quantity controlled by data dimension and approximation guarantee alone,
regardless of the number of internal nodes of either teacher or student
network.
  Our claim is based on approximating both teacher and student networks by
polynomial (tensor) regression models with degree depending on the desired
accuracy and network depth only. Such a parametrization notably does not depend
on the number of internal nodes. Thus a message implied by our results is that
parametrizing wide neural networks by the number of hidden nodes is misleading,
and a more fitting measure of parametrization complexity is the number of
regression coefficients associated with tensorized data. In particular, this
somewhat reconciles the generalization ability of neural networks with more
classical statistical notions of data complexity and generalization bounds. Our
empirical results on MNIST and Fashion-MNIST datasets indeed confirm that
tensorized regression achieves a good out-of-sample performance, even when the
degree of the tensor is at most two.
"
"stat.TH","  Modelling a large collection of functional time series arises in a broad
spectral of real applications. Under such a scenario, not only the number of
functional variables can be diverging with, or even larger than the number of
temporally dependent functional observations, but each function itself is an
infinite-dimensional object, posing a challenging task. In this paper, a
standard three-step procedure is proposed to address such large-scale problems.
To provide theoretical guarantees for the three-step procedure, we focus on
multivariate stationary processes and propose a novel {\it functional stability
measure} based on their spectral properties. Such stability measure facilitates
the development of some useful concentration bounds on sample (auto)covariance
functions, which serve as a fundamental tool for further consistency analysis,
e.g., for deriving rates of convergence on the regularized estimates in
high-dimensional settings. As {\it functional principal component analysis}
(FPCA) is one of the key dimension reduction techniques in the first step, we
also investigate the consistency properties of the relevant estimated terms
under a FPCA framework. To illustrate with an important application, we
consider vector functional autoregressive models and develop a regularization
approach to estimate autoregressive coefficient functions under the sparsity
constraint. Using our derived convergence results, we investigate the
theoretical properties of the regularized estimate under high-dimensional
scaling. Finally, the finite-sample performance of the proposed method is
examined through both simulations and a public financial dataset.
"
"stat.TH","  The typical central limit theorems in high-frequency asymptotics for
semimartingales are results on stable convergence to a mixed normal limit with
an unknown conditional variance. Estimating this conditional variance usually
is a hard task, in particular when the underlying process contains jumps. For
this reason, several authors have recently discussed methods to automatically
estimate the conditional variance, i.e. they build a consistent estimator from
the original statistics, but computed at various different time scales. Their
methods work in several situations, but are essentially restricted to the case
of continuous paths always. The aim of this work is to present a new method to
consistently estimate the conditional variance which works regardless of
whether the underlying process is continuous or has jumps. We will discuss the
case of power variations in detail and give insight to the heuristics behind
the approach.
"
"stat.TH","  Ordinary differential equations have been used to model dynamical systems in
a broad range. Model checking for parametric ordinary differential equations is
a necessary step to check whether the assumed models are plausible. In this
paper we introduce three test statistics for their different purposes. We first
give a trajectory matching-based test for the whole system. To further identify
which component function(s) would be wrongly modelled, we introduce two test
statistics that are based on integral matching and gradient matching
respectively. We investigate the asymptotic properties of the three test
statistics under the null, global and local alternative hypothesis. To achieve
these purposes, we also investigate the asymptotic properties of nonlinear
least squares estimation and two-step collocation estimation under both the
null and alternatives. The results about the estimations are also new in the
literature. To examine the performances of the tests, we conduct several
numerical simulations. A real data example about immune cell kinetics and
trafficking for influenza infection is analyzed for illustration.
"
"stat.TH","  We study the fundamental problem of fixed design {\em multidimensional
segmented regression}: Given noisy samples from a function $f$, promised to be
piecewise linear on an unknown set of $k$ rectangles, we want to recover $f$ up
to a desired accuracy in mean-squared error. We provide the first sample and
computationally efficient algorithm for this problem in any fixed dimension.
Our algorithm relies on a simple iterative merging approach, which is novel in
the multidimensional setting. Our experimental evaluation on both synthetic and
real datasets shows that our algorithm is competitive and in some cases
outperforms state-of-the-art heuristics. Code of our implementation is
available at
\url{https://github.com/avoloshinov/multidimensional-segmented-regression}.
"
"stat.TH","  We extend the Kulkarni class of multivariate phase--type distributions in a
natural time--fractional way to construct a new class of multivariate
distributions with heavy-tailed Mittag-Leffler(ML)-distributed marginals. The
approach relies on assigning rewards to a non--Mar\-ko\-vi\-an jump process
with ML sojourn times. This new class complements an earlier multivariate ML
construction \cite{multiml} and in contrast to the former also allows for tail
dependence. We derive properties and characterizations of this class, and work
out some special cases that lead to explicit density representations.
"
"stat.TH","  One classical canon of statistics is that large models are prone to
overfitting and model selection procedures are necessary for high-dimensional
data. However, many overparameterized models such as neural networks, which are
often trained with simple online methods and regularization, perform very well
in practice. The empirical success of overparameterized models, which is often
known as benign overfitting, motivates us to have a new look at the statistical
generalization theory for online optimization. In particular, we present a
general theory on the generalization error of stochastic gradient descent (SGD)
for both convex and non-convex loss functions. We further provide the
definition of ""low effective dimension"" so that the generalization error either
does not depend on the ambient dimension $p$ or depends on $p$ via a
poly-logarithmic factor. We also demonstrate on several widely used statistical
models that the ""low effect dimension"" arises naturally in overparameterized
settings. The studied statistical applications include both convex models such
as linear regression and logistic regression, and non-convex models such as
$M$-estimator and two-layer neural networks.
"
"stat.TH","  Stochastic zeroth-order optimization concerns problems where only noisy
function evaluations are available. Such problems arises frequently in many
important applications. In this paper, we consider stochastic zeroth-order
optimization over Riemannian submanifolds embedded in an Euclidean space, an
important but less studied area, and propose four algorithms for solving this
class of problems under different settings. Our algorithms are based on
estimating the Riemannian gradient and Hessian from noisy objective function
evaluations, based on a Riemannian version of the Gaussian smoothing technique.
In particular, we consider the following settings for the objective function:
(i) stochastic and gradient-Lipschitz (in both nonconvex and geodesic convex
settings), (ii) sum of gradient-Lipschitz and non-smooth functions, and (iii)
Hessian-Lipschitz. For these settings, we characterize the oracle complexity of
our algorithms to obtain appropriately defined notions of $\epsilon$-stationary
point or $\epsilon$-approximate local minimizer. Notably, our complexities are
independent of the dimension of the ambient Euclidean space and depend only on
the intrinsic dimension of the manifold under consideration. We demonstrate the
applicability of our algorithms by simulation results.
"
"stat.TH","  Design-consistent model-assisted estimation has become the standard practice
in survey sampling. However, a general theory is lacking so far, which allows
one to incorporate modern machine-learning techniques that can lead to
potentially much more powerful assisting models. We propose a subsampling
Rao-Blackwell method, and develop a statistical learning theory for exactly
design-unbiased estimation with the help of linear or non-linear prediction
models. Our approach makes use of classic ideas from Statistical Science as
well as the rapidly growing field of Machine Learning. Provided rich auxiliary
information, it can yield considerable efficiency gains over standard linear
model-assisted methods, while ensuring valid estimation for the given target
population, which is robust against potential mis-specifications of the
assisting model at the individual level.
"
"stat.TH","  We detail an approach to develop Stein's method for bounding integral metrics
on probability measures defined on a Riemannian manifold $\mathbf{M}$. Our
approach exploits the relationship between the generator of a diffusion on
$\mathbf{M}$ with target invariant measure and its characterising Stein
operator. We consider a pair of such diffusions with different starting points,
and investigate properties of solution to the Stein equation based on analysis
of the distance process between the pair. Several examples elucidating the role
of geometry of $\mathbf{M}$ in these developments are presented.
"
"stat.TH","  Recent theoretical studies proved that deep neural network (DNN) estimators
obtained by minimizing empirical risk with a certain sparsity constraint can
attain optimal convergence rates for regression and classification problems.
However, the sparsity constraint requires to know certain properties of the
true model, which are not available in practice. Moreover, computation is
difficult due to the discrete nature of the sparsity constraint. In this paper,
we propose a novel penalized estimation method for sparse DNNs, which resolves
the aforementioned problems existing in the sparsity constraint. We establish
an oracle inequality for the excess risk of the proposed sparse-penalized DNN
estimator and derive convergence rates for several learning tasks. In
particular, we prove that the sparse-penalized estimator can adaptively attain
minimax convergence rates for various nonparametric regression problems. For
computation, we develop an efficient gradient-based optimization algorithm that
guarantees the monotonic reduction of the objective function.
"
"stat.TH","  We show that for local alternatives to uniformity which are determined by a
sequence of square integrable densities the moderate deviation (MD) theorem for
the corresponding Neyman-Pearson statistic does not hold in the full range for
all unbounded densities. We give a sufficient condition under which MD theorem
holds. The proof is based on Mogulskii's inequality.
"
"stat.TH","  We consider a variational autoencoder (VAE) for binary data. Our main
innovations are an interpretable lower bound for its training objective, a
modified initialization and architecture of such a VAE that leads to faster
training, and a decision support for finding the appropriate dimension of the
latent space via using a PCA. Numerical examples illustrate our theoretical
result and the performance of the new architecture.
"
"stat.TH","  This paper is devoted to the estimation of a partial graphical model with a
structural Bayesian penalization. Precisely, we are interested in the linear
regression setting where the estimation is made through the direct links
between potentially high-dimensional predictors and multiple responses, since
it is known that Gaussian graphical models enable to exhibit direct links only,
whereas coefficients in linear regressions contain both direct and indirect
relations (due e.g. to strong correlations among the variables). A smooth
penalty reflecting a generalized Gaussian Bayesian prior on the covariates is
added, either enforcing patterns (like row structures) in the direct links or
regulating the joint influence of predictors. We give a theoretical guarantee
for our method, taking the form of an upper bound on the estimation error
arising with high probability, provided that the model is suitably regularized.
Empirical studies on synthetic data and real datasets are conducted to compare
the efficiency of the model with well-known related procedures. Our work shows
that the flexibility induced by the additional hyperparametrization may control
the extent of structuring in the direct links and improve both predictions and
statistical interpretations.
"
"stat.TH","  The estimation of covariance operators of spatio-temporal data is in many
applications only computationally feasible under simplifying assumptions, such
as separability of the covariance into strictly temporal and spatial
factors.Powerful tests for this assumption have been proposed in the
literature. However, as real world systems, such as climate data are
notoriously inseparable, validating this assumption by statistical tests, seems
inherently questionable. In this paper we present an alternative approach: By
virtue of separability measures, we quantify how strongly the data's covariance
operator diverges from a separable approximation. Confidence intervals localize
these measures with statistical guarantees. This method provides users with a
flexible tool, to weigh the computational gains of a separable model against
the associated increase in bias. As separable approximations we consider the
established methods of partial traces and partial products, and develop weak
convergence principles for the corresponding estimators. Moreover, we also
prove such results for estimators of optimal, separable approximations, which
are arguably of most interest in applications. In particular we present for the
first time statistical inference for this object, which has been confined to
estimation previously. Besides confidence intervals, our results encompass
tests for approximate separability. All methods proposed in this paper are free
of nuisance parameters and do neither require computationally expensive
resampling procedures nor the estimation of nuisance parameters. A simulation
study underlines the advantages of our approach and its applicability is
demonstrated by the investigation of German annual temperature data.
"
"stat.TH","  In Bayesian nonparametrics there exists a rich variety of discrete priors,
including the Dirichlet process and its generalizations, which are nowadays
well-established tools. Despite the remarkable advances, few proposals are
tailored for modeling observations lying on product spaces, such as
$\mathbb{R}^p$. In this setting, most of the available priors lack of
flexibility and they do not allow for separate partition structures among the
spaces. We address these issues by introducing a discrete nonparametric prior
termed enriched Pitman-Yor process (EPY). Theoretical properties of this novel
prior are extensively investigated. Specifically, we discuss its formal link
with the enriched Dirichlet process and normalized random measures, we describe
a square-breaking representation and we obtain closed form expressions for the
posterior law and the involved urn schemes. In second place, we show that
several existing approaches, including Dirichlet processes with a spike and
slab base measure and mixture of mixtures models, implicitly rely on special
cases of the EPY, which therefore constitutes a unified probabilistic framework
for many Bayesian nonparametric priors. Interestingly, our unifying formulation
will allow to naturally extend these models, while preserving their analytical
tractability. As an illustration, we employ the EPY for a species sampling
problem in ecology.
"
"stat.TH","  The paper discusses identification conditions, representations and relations
of generalized least squares estimators of regression parameters in
multivariate linear regression models such as seemingly unrelated and fixed
effect panel models. Results are presented on identification for unrestricted
dispersion structure and general heteroskedasticity and cross-equation
dependence, considering explicit and implicit restrictions, singularity of the
dispersion and multicollinearity in the design matrix.
"
"stat.TH","  We investigate the large-sample behavior of change-point tests based on
weighted two-sample U-statistics, in the case of short-range dependent data.
Under some mild mixing conditions, we establish convergence of the test
statistic to an extreme value distribution. A simulation study shows that the
weighted tests are superior to the non-weighted versions when the change-point
occurs near the boundary of the time interval, while they loose power in the
center.
"
"stat.TH","  We consider the problem of estimating the joint distribution of $n$
independent random variables. Our approach is based on a family of candidate
probabilities that we shall call a model and which is chosen to either contain
the true distribution of the data or at least to provide a good approximation
of it with respect to some loss function. The aim of the present paper is to
describe a general estimation strategy that allows to adapt to both the
specific features of the model and the choice of the loss function in view of
designing an estimator with good estimation properties. The losses we have in
mind are based on the total variation, Hellinger, Wasserstein and
$\mathbb{L}_p$-distances to name a few. We show that the risk of the resulting
estimator with respect to the loss function can be bounded by the sum of an
approximation term accounting for the loss between the true distribution and
the model and a complexity term that corresponds to the bound we would get if
this distribution did belong to the model. Our results hold under mild
assumptions on the true distribution of the data and are based on exponential
deviation inequalities that are non-asymptotic and involve explicit constants.
When the model reduces to two distinct probabilities, we show how our
estimation strategy leads to a robust test whose errors of first and second
kinds only depend on the losses between the true distribution and the two
tested probabilities.
"
"stat.TH","  We consider the general (stochastic) contextual bandit problem under the
realizability assumption, i.e., the expected reward, as a function of contexts
and actions, belongs to a general function class $\mathcal{F}$. We design a
fast and simple algorithm that achieves the statistically optimal regret with
only ${O}(\log T)$ calls to an offline least-squares regression oracle across
all $T$ rounds. The number of oracle calls can be further reduced to
$O(\log\log T)$ if $T$ is known in advance. Our results provide the first
universal and optimal reduction from contextual bandits to offline regression,
solving an important open problem in contextual bandits. A direct consequence
of our results is that any advances in offline regression immediately translate
to contextual bandits, statistically and computationally. This leads to faster
algorithms and improved regret guarantees for broader classes of contextual
bandit problems.
"
"stat.TH","  We investigate whether in a distributed setting, adaptive estimation of a
smooth function at the optimal rate is possible under minimal communication. It
turns out that the answer depends on the risk considered and on the number of
servers over which the procedure is distributed. We show that for the
$L_\infty$-risk, adaptively obtaining optimal rates under minimal communication
is not possible. For the $L_2$-risk, it is possible over a range of
regularities that depends on the relation between the number of local servers
and the total sample size.
"
"stat.TH","  We introduce a new general modeling approach for multivariate discrete event
data with categorical interacting marks, which we refer to as marked Bernoulli
processes. In the proposed model, the probability of an event of a specific
category to occur in a location may be influenced by past events at this and
other locations. We do not restrict interactions to be positive or decaying
over time as it is commonly adopted, allowing us to capture an arbitrary shape
of influence from historical events, locations, and events of different
categories. In our modeling, prior knowledge is incorporated by allowing
general convex constraints on model parameters. We develop two parameter
estimation procedures utilizing the constrained Least Squares (LS) and Maximum
Likelihood (ML) estimation, which are solved using variational inequalities
with monotone operators. We discuss different applications of our approach and
illustrate the performance of proposed recovery routines on synthetic examples
and a real-world police dataset.
"
"stat.TH","  This paper considers a multi-armed bandit (MAB) problem in which multiple
mobile agents receive rewards by sampling from a collection of spatially
dispersed stochastic processes, called bandits. The goal is to formulate a
decentralized policy for each agent, in order to maximize the total cumulative
reward over all agents, subject to option availability and inter-agent
communication constraints. The problem formulation is motivated by applications
in which a team of autonomous mobile robots cooperates to accomplish an
exploration and exploitation task in an uncertain environment. Bandit locations
are represented by vertices of the spatial graph. At any time, an agent's
option consist of sampling the bandit at its current location, or traveling
along an edge of the spatial graph to a new bandit location. Communication
constraints are described by a directed, non-stationary, stochastic
communication graph. At any time, agents may receive data only from their
communication graph in-neighbors. For the case of a single agent on a fully
connected spatial graph, it is known that the expected regret for any optimal
policy is necessarily bounded below by a function that grows as the logarithm
of time. A class of policies called upper confidence bound (UCB) algorithms
asymptotically achieve logarithmic regret for the classical MAB problem. In
this paper, we propose a UCB-based decentralized motion and option selection
policy and a non-stationary stochastic communication protocol that guarantee
logarithmic regret. To our knowledge, this is the first such decentralized
policy for non-fully connected spatial graphs with communication constraints.
When the spatial graph is fully connected and the communication graph is
stationary, our decentralized algorithm matches or exceeds the best reported
prior results from the literature.
"
"stat.TH","  We consider a stationary linear AR($p$) model with observations subject to
gross errors (outliers). The autoregression parameters are unknown as well as
the distribution function $G$ of innovations. The distribution of outliers
$\Pi$ is unknown and arbitrary, their intensity is $\gamma n^{-1/2}$ with an
unknown $\gamma$, $n$ is the sample size. We test the hypothesis $H_0\colon
G=G_0$ with simmetric $G_0$. We find the power of the test under local
alternatives $H_{1n}(\rho)\colon G=(1-\rho n^{-1/2})G_0+\rho n^{-1/2}H$. Our
test is the special symmetrized Pearson's type test. Namely, first of all we
estimate the autoregression parameters and then using the residuals from the
estimated autoregression we construct a kind of empirical distribution function
(e.d.f.), which is a counterpart of the (inaccessible) e.d.f. of the
autoregression innovations. We obtain a stochastic expansion of this e.d.f. and
its symmetrized variant under $H_{1n}(\rho)$ , which enables us to construct
and investigate our symmetrized test of Pearson's type for $H_0$. We establish
qualitative robustness of this test in terms of uniform equicontinuity of the
limiting power (as functions of $\gamma,\rho$ and $\Pi$) with respect to
$\gamma$ in a neighborhood of $\gamma=0$.
"
"stat.TH","  Several formulations have long existed in the literature in the form of
continuous mixtures of normal variables where a mixing variable operates on the
mean or on the variance or on both the mean and the variance of a multivariate
normal variable, by changing the nature of these basic constituents from
constants to random quantities. More recently, other mixture-type constructions
have been introduced, where the core random component, on which the mixing
operation operates, is not necessarily normal. The main aim of the present work
is to show that many existing constructions can be encompassed by a formulation
where normal variables are mixed using two univariate random variables. For
this formulation, we derive various general properties. Within the proposed
framework, it is also simpler to formulate new proposals of parametric families
and we provide a few such instances. At the same time, the exposition provides
a review of the theme of normal mixtures.
"
"stat.TH","  The partial copula provides a method for describing the dependence between
two random variables $X$ and $Y$ conditional on a third random vector $Z$ in
terms of nonparametric residuals $U_1$ and $U_2$. This paper develops a
nonparametric test for conditional independence by combining the partial copula
with a quantile regression based method for estimating the nonparametric
residuals. We consider a test statistic based on generalized correlation
between $U_1$ and $U_2$ and derive its large sample properties under
consistency assumptions on the quantile regression procedure. We demonstrate
through a simulation study that the resulting test is sound under complicated
data generating distributions. Moreover, it is competitive to or better than
other state-of-the-art conditional independence tests in terms of level and
power.
"
"stat.TH","  Techniques of matrix completion aim to impute a large portion of missing
entries in a data matrix through a small portion of observed ones, with broad
machine learning applications including collaborative filtering, pairwise
ranking, etc. In practice, additional structures are usually employed in order
to improve the accuracy of matrix completion. Examples include subspace
constraints formed by side information in collaborative filtering, and skew
symmetry in pairwise ranking. This paper performs a unified analysis of
nonconvex matrix completion with linearly parameterized factorization, which
covers the aforementioned examples as special cases. Importantly, uniform upper
bounds for estimation errors are established for all local minima, provided
that the sampling rate satisfies certain conditions determined by the rank,
condition number, and incoherence parameter of the ground-truth low rank
matrix. Empirical efficiency of the proposed method is further illustrated by
numerical simulations.
"
"stat.TH","  This paper uses a probabilistic approach to analyze the converge of an
ensemble Kalman filter solution to an exact Kalman filter solution in the
simplest possible setting, the scalar case, as it allows us to build upon a
rich literature of scalar probability distributions and non-elementary
functions. To this end we introduce the bare-bones Scalar Pedagogical Ensemble
Kalman Filter (SPEnKF). We show that in the asymptotic case of ensemble size,
the expected value of both the analysis mean and variance estimate of the
SPEnKF converges to that of the true Kalman filter, and that the variances of
both tend towards zero, at each time moment. We also show that the ensemble
converges in probability in the complementary case, when the ensemble is
finite, and time is taken to infinity. Moreover, we show that in the
finite-ensemble, finite-time case, variance inflation and mean correction can
be leveraged to coerce the SPEnKF converge to its scalar Kalman filter
counterpart. We then apply this framework to analyze perturbed observations and
explain why perturbed observations ensemble Kalman filters underperform their
deterministic counterparts.
"
"stat.TH","  Permutation tests are widely used in statistics, providing a finite-sample
guarantee on the type I error rate whenever the distribution of the samples
under the null hypothesis is invariant to some rearrangement. Despite its
increasing popularity and empirical success, theoretical properties of the
permutation test, especially its power, have not been fully explored beyond
simple cases. In this paper, we attempt to fill this gap by presenting a
general non-asymptotic framework for analyzing the power of the permutation
test. The utility of our proposed framework is illustrated in the context of
two-sample and independence testing under both discrete and continuous
settings. In each setting, we introduce permutation tests based on U-statistics
and study their minimax performance. We also develop exponential concentration
bounds for permuted U-statistics based on a novel coupling idea, which may be
of independent interest. Building on these exponential bounds, we introduce
permutation tests which are adaptive to unknown smoothness parameters without
losing much power. The proposed framework is further illustrated using more
sophisticated test statistics including weighted U-statistics for multinomial
testing and Gaussian kernel-based statistics for density testing. Finally, we
provide some simulation results that further justify the permutation approach.
"
"stat.TH","  New inference methods for the multivariate coefficient of variation and its
reciprocal, the standardized mean, are presented. While there are various
testing procedures for both parameters in the univariate case, it is less known
how to do inference in the multivariate setting appropriately. There are some
existing procedures but they rely on restrictive assumptions on the underlying
distributions. We tackle this problem by applying Wald-type statistics in the
context of general, potentially heteroscedastic factorial designs. In addition
to the $k$-sample case, higher-way layouts can be incorporated into this
framework allowing the discussion of main and interaction effects. The
resulting procedures are shown to be asymptotically valid under the null
hypothesis and consistent under general alternatives. To improve the finite
sample performance, we suggest permutation versions of the tests and shown that
the tests' asymptotic properties can be transferred to them. An exhaustive
simulation study compares the new tests, their permutation counterparts and
existing methods. To further analyse the differences between the tests, we
conduct two illustrative real data examples.
"
"stat.TH","  The mixed fractional Vasicek model, which is an extended model of the
traditional Vasicek model, has been widely used in modelling volatility,
interest rate and exchange rate. Obviously, if some phenomenon are modeled by
the mixed fractional Vasicek model, statistical inference for this process is
of great interest. Based on continuous time observations, this paper considers
the problem of estimating the drift parameters in the mixed fractional Vasicek
model. We will propose the maximum likelihood estimators of the drift
parameters in the mixed fractional Vasicek model with the Radon-Nikodym
derivative for a mixed fractional Brownian motion. Using the fundamental
martingale and the Laplace transform, both the strong consistency and the
asymptotic normality of the maximum likelihood estimators have been established
for all $H\in(0,1)$, $H\neq 1/2$.
"
"stat.TH","  In this paper, we focus on the influences of the condition number of the
regression matrix upon the comparison between two hyper-parameter estimation
methods: the empirical Bayes (EB) and the Stein's unbiased estimator with
respect to the mean square error (MSE) related to output prediction (SUREy). We
firstly show that the greatest power of the condition number of the regression
matrix of SUREy cost function convergence rate upper bound is always one larger
than that of EB cost function convergence rate upper bound. Meanwhile, EB and
SUREy hyper-parameter estimators are both proved to be asymptotically normally
distributed under suitable conditions. In addition, one ridge regression case
is further investigated to show that when the condition number of the
regression matrix goes to infinity, the asymptotic variance of SUREy estimator
tends to be larger than that of EB estimator.
"
"stat.TH","  This paper introduces a high-dimensional linear IV regression for the data
sampled at mixed frequencies. We show that the high-dimensional slope parameter
of a high-frequency covariate can be identified and accurately estimated
leveraging on a low-frequency instrumental variable. The distinguishing feature
of the model is that it allows handing high-dimensional datasets without
imposing the approximate sparsity restrictions. We propose a
Tikhonov-regularized estimator and derive the convergence rate of its
mean-integrated squared error for time series data. The estimator has a
closed-form expression that is easy to compute and demonstrates excellent
performance in our Monte Carlo experiments. We estimate the real-time price
elasticity of supply on the Australian electricity spot market. Our estimates
suggest that the supply is relatively inelastic and that its elasticity is
heterogeneous throughout the day.
"
"stat.TH","  Random fields on the sphere play a fundamental role in the natural sciences.
This paper presents a simulation algorithm parenthetical to the spectral
turning bands method used in Euclidean spaces, for simulating scalar- or
vector-valued Gaussian random fields on the $d$-dimensional unit sphere. The
simulated random field is obtained by a sum of Gegenbauer waves, each of which
is variable along a randomly oriented arc and constant along the parallels
orthogonal to the arc. Convergence criteria based on the Berry-Ess\'een
inequality are proposed to choose suitable parameters for the implementation of
the algorithm, which is illustrated through numerical experiments. A by-product
of this work is a closed-form expression of the Schoenberg coefficients
associated with the Chentsov and exponential covariance models on spheres of
dimensions greater than or equal to 2.
"
"stat.TH","  We study functional central limit theorems (FCLTs) for persistent Betti
numbers obtained from networks defined on a Poisson point process. The limit is
formed in large volumes of cylindrical shape stretching only in one dimension.
Moreover, the limiting results cover two possible filtrations, namely a
directed sublevel-filtration for stabilizing networks and the Vietoris-Rips
complex on the random geometric graph. Finally, the presented FCLTs open the
door to a variety of statistical applications in topological data analysis and
we consider goodness-of-fit tests in a simulation study.
"
"stat.TH","  In this note, we provide upper bounds on the expectation of the supremum of
empirical processes indexed by H\""older classes of any smoothness and for any
distribution supported on a bounded set in $\mathbb R^d$. These results can be
alternatively seen as non-asymptotic risk bounds, when the unknown distribution
is estimated by its empirical counterpart, based on $n$ independent
observations, and the error of estimation is quantified by the integral
probability metrics (IPM). In particular, the IPM indexed by a H\""older class
is considered and the corresponding rates are derived. These results
interpolate between the two well-known extreme cases: the rate $n^{-1/d}$
corresponding to the Wassertein-1 distance (the least smooth case) and the fast
rate $n^{-1/2}$ corresponding to very smooth functions (for instance, functions
from an RKHS defined by a bounded kernel).
"
"stat.TH","  We discuss estimation problems where a polynomial is observed under Ornstein
Uhlenbeck noise over a long time interval. We prove local asymptotic normality
(LAN) and specify asymptotically efficient estimators. We apply this to the
following problem: feeding noise into the classical (deterministic) Hodgkin
Huxley model of neuroscience, we are interested in asymptotically efficient
estimation of the parameters of the noise process.
"
"stat.TH","  We show that maximum likelihood estimation in statistics is equivalent to
finding the capacity in invariant theory, in two statistical settings:
log-linear models and Gaussian transformation families.The former includes the
classical independence model while the latter includes matrix normal models and
Gaussian graphical models given by transitive directed acyclic graphs. We use
stability under group actions to characterize boundedness of the likelihood,
and existence and uniqueness of the maximum likelihood estimate. Our approach
reveals promising consequences of the interplay between invariant theory and
statistics. In particular, existing scaling algorithms from statistics can be
used in invariant theory, and vice versa.
"
"stat.TH","  We study general singular value shrinkage estimators in high-dimensional
regression and classification, when the number of features and the sample size
both grow proportionally to infinity. We allow models with general covariance
matrices that include a large class of data generating distributions. As far as
the implications of our results are concerned, we find exact asymptotic
formulas for both the training and test errors in regression models fitted by
gradient descent, which provides theoretical insights for early stopping as a
regularization method. In addition, we propose a numerical method based on the
empirical spectra of covariance matrices for the optimal eigenvalue shrinkage
classifier in linear discriminant analysis. Finally, we derive optimal
estimators for the dense mean vectors of high-dimensional distributions.
Throughout our analysis we rely on recent advances in random matrix theory and
develop further results of independent mathematical interest.
"
"stat.TH","  We obtain concentration and large deviation for the sums of independent and
identically distributed random variables with heavy-tailed distributions. Our
concentration results are concerned with random variables whose distributions
satisfy $P(X>t) \leq {\rm e}^{- I(t)}$, where $I: \mathbb{R} \rightarrow
\mathbb{R}$ is an increasing function and $I(t)/t \rightarrow \alpha \in [0,
\infty)$ as $t \rightarrow \infty$. Our main theorem can not only recover some
of the existing results, such as the concentration of the sum of subWeibull
random variables, but it can also produce new results for the sum of random
variables with heavier tails. We show that the concentration inequalities we
obtain are sharp enough to offer large deviation results for the sums of
independent random variables as well. Our analyses which are based on standard
truncation arguments simplify, unify and generalize the existing results on the
concentration and large deviation of heavy-tailed random variables.
"
"stat.TH","  This paper studies the estimation of the coefficient matrix $\Ttheta$ in
multivariate regression with hidden variables, $Y = (\Ttheta)^TX + (B^*)^TZ +
E$, where $Y$ is a $m$-dimensional response vector, $X$ is a $p$-dimensional
vector of observable features, $Z$ represents a $K$-dimensional vector of
unobserved hidden variables, possibly correlated with $X$, and $E$ is an
independent error. The number of hidden variables $K$ is unknown and both $m$
and $p$ are allowed but not required to grow with the sample size $n$. Since
only $Y$ and $X$ are observable, we provide necessary conditions for the
identifiability of $\Ttheta$. The same set of conditions are shown to be
sufficient when the error $E$ is homoscedastic. Our identifiability proof is
constructive and leads to a novel and computationally efficient estimation
algorithm, called HIVE. The first step of the algorithm is to estimate the best
linear prediction of $Y$ given $X$ in which the unknown coefficient matrix
exhibits an additive decomposition of $\Ttheta$ and a dense matrix originated
from the correlation between $X$ and the hidden variable $Z$. Under the row
sparsity assumption on $\Ttheta$, we propose to minimize a penalized least
squares loss by regularizing $\Ttheta$ via a group-lasso penalty and
regularizing the dense matrix via a multivariate ridge penalty. Non-asymptotic
deviation bounds of the in-sample prediction error are established. Our second
step is to estimate the row space of $B^*$ by leveraging the covariance
structure of the residual vector from the first step. In the last step, we
remove the effect of hidden variable by projecting $Y$ onto the complement of
the estimated row space of $B^*$. Non-asymptotic error bounds of our final
estimator are established. The model identifiability, parameter estimation and
statistical guarantees are further extended to the setting with heteroscedastic
errors.
"
"stat.TH","  In their fundamental paper on cubic variance functions (VFs), Letac and Mora
(The Annals of Statistics,1990) presented a systematic, rigorous and
comprehensive study of natural exponential families (NEFs) on the real line,
their characterization through their VFs and mean value parameterization. They
presented a section that for some reason has been left unnoticed. This section
deals with the construction of VFs associated with NEFs of counting
distributions on the set of nonnegative integers and allows to find the
corresponding generating measures. As EDMs are based on NEFs, we introduce in
this paper three new classes of EDMs based on their results. For these classes,
which are associated with simple VFs, we derive their mean value
parameterization and their associated generating measures. We also prove that
they have some desirable properties. Each of these classes is shown to be
overdispersed and zero inflated in ascending order, making them as competitive
statistical models for those in use in both, statistical and actuarial
modeling. A numerical example of real data compares the performance of one
class and demonstrates its superiority.
"
"stat.TH","  Our aim in this work is to give explicit formula of the linear processes
solution of autoregressive time series AR(2) with hint of generating functions
theory by using the Horadam numbers and polynomials.
"
"stat.TH","  Two separate and distinct sources of nonidentifiability arise naturally in
the context of latent position random graph models, though neither are unique
to this setting. In this paper we define and examine these two
nonidentifiabilities, dubbed subspace nonidentifiability and model-based
nonidentifiability, in the context of random graph inference. We give examples
where each type of nonidentifiability comes into play, and we show how in
certain settings one need worry about one or the other type of
nonidentifiability. Then, we characterize the limit for model-based
nonidentifiability both with and without subspace nonidentifiability. We
further obtain additional limiting results for covariances and $U$-statistics
of stochastic block models and generalized random dot product graphs.
"
"stat.TH","  We study the non-convex optimization landscape for maximum likelihood
estimation in the discrete orbit recovery model with Gaussian noise. This model
is motivated by applications in molecular microscopy and image processing,
where each measurement of an unknown object is subject to an independent random
rotation from a rotational group. Equivalently, it is a Gaussian mixture model
where the mixture centers belong to a group orbit.
  We show that fundamental properties of the likelihood landscape depend on the
signal-to-noise ratio and the group structure. At low noise, this landscape is
""benign"" for any discrete group, possessing no spurious local optima and only
strict saddle points. At high noise, this landscape may develop spurious local
optima, depending on the specific group. We discuss several positive and
negative examples, and provide a general condition that ensures a globally
benign landscape. For cyclic permutations of coordinates on $\mathbb{R}^d$
(multi-reference alignment), there may be spurious local optima when $d \geq
6$, and we establish a correspondence between these local optima and those of a
surrogate function of the phase variables in the Fourier domain.
  We show that the Fisher information matrix transitions from resembling that
of a single Gaussian in low noise to having a graded eigenvalue structure in
high noise, which is determined by the graded algebra of invariant polynomials
under the group action. In a local neighborhood of the true object, the
likelihood landscape is strongly convex in a reparametrized system of variables
given by a transcendence basis of this polynomial algebra. We discuss
implications for optimization algorithms, including slow convergence of
expectation-maximization, and possible advantages of momentum-based
acceleration and variable reparametrization for first- and second-order descent
methods.
"
"stat.TH","  Boosting is a well-known method for improving the accuracy of weak learners
in machine learning. However, its theoretical generalization guarantee is
missing in literature. In this paper, we propose an efficient boosting method
with theoretical generalization guarantees for binary classification. Three key
ingredients of the proposed boosting method are: a) the
\textit{fully-corrective greedy} (FCG) update in the boosting procedure, b) a
differentiable \textit{squared hinge} (also called \textit{truncated
quadratic}) function as the loss function, and c) an efficient alternating
direction method of multipliers (ADMM) algorithm for the associated FCG
optimization. The used squared hinge loss not only inherits the robustness of
the well-known hinge loss for classification with outliers, but also brings
some benefits for computational implementation and theoretical justification.
Under some sparseness assumption, we derive a fast learning rate of the order
${\cal O}((m/\log m)^{-1/4})$ for the proposed boosting method, which can be
further improved to ${\cal O}((m/\log m)^{-1/2})$ if certain additional noise
assumption is imposed, where $m$ is the size of sample set. Both derived
learning rates are the best ones among the existing generalization results of
boosting-type methods for classification. Moreover, an efficient early stopping
scheme is provided for the proposed method. A series of toy simulations and
real data experiments are conducted to verify the developed theories and
demonstrate the effectiveness of the proposed method.
"
"stat.TH","  We introduce the concept of pattern graphs--directed acyclic graphs
representing how response patterns are associated. A pattern graph represents
an identifying restriction that is nonparametrically identified/saturated and
is often a missing not at random restriction. We introduce a selection model
and a pattern mixture model formulations using the pattern graphs and show that
they are equivalent. A pattern graph leads to an inverse probability weighting
estimator as well as an imputation-based estimator. We also study the
semi-parametric efficiency theory and derive a multiply-robust estimator using
pattern graphs.
"
"stat.TH","  A distributed binary hypothesis testing (HT) problem over a noisy (discrete
and memoryless) channel studied previously by the authors is investigated from
the perspective of the strong converse property. It was shown by Ahlswede and
Csisz\'{a}r that a strong converse holds in the above setting when the channel
is rate-limited and noiseless. Motivated by this observation, we show that the
strong converse continues to hold in the noisy channel setting for a special
case of HT known as testing against independence (TAI). The proof utilizes the
blowing up lemma and the recent change of measure technique of Tyagi and
Watanabe as the key tools.
"
"stat.TH","  We consider a design problem where experimental conditions (design points
$X_i$) are presented in the form of a sequence of i.i.d.\ random variables,
generated with an unknown probability measure $\mu$, and only a given
proportion $\alpha\in(0,1)$ can be selected. The objective is to select good
candidates $X_i$ on the fly and maximize a concave function $\Phi$ of the
corresponding information matrix. The optimal solution corresponds to the
construction of an optimal bounded design measure $\xi_\alpha^*\leq
\mu/\alpha$, with the difficulty that $\mu$ is unknown and $\xi_\alpha^*$ must
be constructed online. The construction proposed relies on the definition of a
threshold $\tau$ on the directional derivative of $\Phi$ at the current
information matrix, the value of $\tau$ being fixed by a certain quantile of
the distribution of this directional derivative. Combination with recursive
quantile estimation yields a nonlinear two-time-scale stochastic approximation
method. It can be applied to very long design sequences since only the current
information matrix and estimated quantile need to be stored. Convergence to an
optimum design is proved. Various illustrative examples are presented.
"
"stat.TH","  We study plane trees as a model for RNA secondary structure, assigning energy
to each tree based on the Nearest Neighbor Thermodynamic Model, and defining a
corresponding Gibbs distribution on the trees. Through a bijection between
plane trees and 2-Motzkin paths, we design a Markov chain converging to the
Gibbs distribution, and establish fast mixing time results by estimating the
spectral gap of the chain. The spectral gap estimate is established through a
series of decompositions of the chain and also by building on known mixing time
results for other chains on Dyck paths. In addition to the mathematical aspects
of the result, the resulting algorithm can be used as a tool for exploring the
branching structure of RNA and its dependence on energy model parameters. The
pseudocode implementing the Markov chain is provided in an appendix.
"
"stat.TH","  Shrinkage estimation is a fundamental tool of modern statistics, pioneered by
Charles Stein upon the discovery of his famous paradox. Despite a large
subsequent literature, the efficiency of shrinkage, and the associated
procedure known as Stein's Unbiased Risk Estimate, or SURE, has mainly been
analysed in the Gaussian setting. Importing tools developed for use in the
probabilistic area now known as Stein's method, the present work investigates
the domain of validity of shrinkage and SURE away from the Gaussian. We show
that shrinkage is efficient away from the Gaussian under very mild conditions
on the distribution of the noise. SURE is also proved to be adaptive under
similar assumptions, and in particular in a way that retains the classical
asymptotics of Pinsker's theorem. Notably, shrinkage and SURE are shown to be
efficient under mild distributional assumptions.
"
"stat.TH","  We introduce tramp, standing for TRee Approximate Message Passing, a python
package for compositional inference in high-dimensional tree-structured models.
The package provides an unifying framework to study several approximate message
passing algorithms previously derived for a variety of machine learning tasks
such as generalized linear models, inference in multi-layer networks, matrix
factorization, and reconstruction using non-separable penalties. For some
models, the asymptotic performance of the algorithm can be theoretically
predicted by the state evolution, and the measurements entropy estimated by the
free entropy formalism. The implementation is modular by design: each module,
which implements a factor, can be composed at will with other modules to solve
complex inference tasks. The user only needs to declare the factor graph of the
model: the inference algorithm, state evolution and entropy estimation are
fully automated.
"
"stat.TH","  The classical binary hypothesis testing problem is revisited. We notice that
when one of the hypotheses is composite, there is an inherent difficulty in
defining an optimality criterion that is both informative and well-justified.
For testing in the simple normal location problem (that is, testing for the
mean of multivariate Gaussians), we overcome the difficulty as follows. In this
problem there exists a natural hardness order between parameters as for
different parameters the error-probailities curves (when the parameter is
known) are either identical, or one dominates the other. We can thus define
minimax performance as the worst-case among parameters which are below some
hardness level. Fortunately, there exists a universal minimax test, in the
sense that it is minimax for all hardness levels simultaneously. Under this
criterion we also find the optimal test for composite hypothesis testing with
training data. This criterion extends to the wide class of local asymptotic
normal models, in an asymptotic sense where the approximation of the error
probabilities is additive. Since we have the asymptotically optimal tests for
composite hypothesis testing with and without training data, we quantify the
loss of universality and gain of training data for these models.
"
"stat.TH","  We first revisit the problem of kernel estimation of spot volatility in a
general continuous It\^o semimartingale model in the absence of microstructure
noise, and prove a Central Limit Theorem with optimal convergence rate, which
is an extension of Figueroa and Li (2020) as we allow for a general two-sided
kernel function. Next, to handle the microstructure noise of ultra
high-frequency observations, we present a new type of pre-averaging/kernel
estimator for spot volatility under the presence of additive microstructure
noise. We prove Central Limit Theorems for the estimation error with an optimal
rate and study the problems of optimal bandwidth and kernel selection. As in
the case of a simple kernel estimator of spot volatility in the absence of
microstructure noise, we show that the asymptotic variance of the
pre-averaging/kernel estimator is minimal for exponential or Laplace kernels,
hence, justifying the need of working with unbounded kernels as proposed in
this work. Feasible implementation of the proposed estimators with optimal
bandwidth is also developed. Monte Carlo experiments confirm the superior
performance of the devised method.
"
"stat.TH","  Many statistical learning problems have recently been shown to be amenable to
Semi-Definite Programming (SDP), with community detection and clustering in
Gaussian mixture models as the most striking instances [javanmard et al.,
2016]. Given the growing range of applications of SDP-based techniques to
machine learning problems, and the rapid progress in the design of efficient
algorithms for solving SDPs, an intriguing question is to understand how the
recent advances from empirical process theory can be put to work in order to
provide a precise statistical analysis of SDP estimators.
  In the present paper, we borrow cutting edge techniques and concepts from the
learning theory literature, such as fixed point equations and excess risk
curvature arguments, which yield general estimation and prediction results for
a wide class of SDP estimators. From this perspective, we revisit some
classical results in community detection from [gu\'edon et al.,2016] and [chen
et al., 2016], and we obtain statistical guarantees for SDP estimators used in
signed clustering, group synchronization and MAXCUT.
"
"stat.TH","  Completely nonparametric transformation models with heteroscedastic errors
are considered. Despite their flexibility, such models have rarely been used so
far, since estimators of the model components have been missing and even
identification of such models has not been clear until very recently. The
results of Kloodt (2020) are used to construct the first two estimators of the
transformation function in these models. While the first estimator converges to
the true transformation function at a parametric rate, the second estimator can
be obtained by an explicit formula and is less computationally demanding.
Finally, a simulation study is followed by some concluding remarks. Assumptions
and proofs can be found in the appendix.
"
"stat.TH","  We consider a stationary linear AR($p$) model with observations subject to
gross errors (outliers). The autoregression parameters as well as the
distribution function (d.f.) $G$ of innovations are unknown. The distribution
of outliers $\Pi$ is unknown and arbitrary, their intensity is $\gamma
n^{-1/2}$ with an unknown $\gamma$, $n$ is the sample size. We test the
hypothesis for normality of innovations $$\mathbf{H}_\Phi \colon G \in
\{\Phi(x/\theta),\,\theta>0\},$$ $\Phi(x)$ is the d.f. $\mathbf{N}(0,1)$. Our
test is the special symmetrized Pearson's type test. We find the power of this
test under local alternatives $$\mathbf{H}_{1n}(\rho)\colon
G(x)=A_n(x):=(1-\rho n^{-1/2})\Phi(x/\theta_0)+\rho n^{-1/2}H(x), $$ $\rho\geq
0,\,\theta_0$ is the unknown (under $\mathbf{H}_\Phi$) variance of innovations.
First of all we estimate the autoregression parameters and then using the
residuals from the estimated autoregression we construct a kind of empirical
distribution function (r.e.d.f.), which is a counterpart of the (inaccessible)
e.d.f. of the autoregression innovations. After this we construct the
symmetrized variant r.e.d.f. Our test statistic is the functional from
symmetrized r.e.d.f. We obtain a stochastic expansion of this symmetrized
r.e.d.f. under $\mathbf{H}_{1n}(\rho)$ , which enables us to investigate our
test. We establish qualitative robustness of this test in terms of uniform
equicontinuity of the limiting power (as functions of $\gamma,\rho$ and $\Pi$)
with respect to $\gamma$ in a neighborhood of $\gamma=0$.
"
"stat.TH","  Sequential Monte Carlo (SMC), also known as particle filters, has been widely
accepted as a powerful computational tool for making inference with dynamical
systems. A key step in SMC is resampling, which plays the role of steering the
algorithm towards the future dynamics. Several strategies have been proposed
and used in practice, including multinomial resampling, residual resampling
(Liu and Chen 1998), optimal resampling (Fearnhead and Clifford 2003),
stratified resampling (Kitagawa 1996), and optimal transport resampling (Reich
2013). We show that, in the one dimensional case, optimal transport resampling
is equivalent to stratified resampling on the sorted particles, and they both
minimize the resampling variance as well as the expected squared energy
distance between the original and resampled empirical distributions; in the
multidimensional case, the variance of stratified resampling after sorting
particles using Hilbert curve (Gerber et al. 2019) in $\mathbb{R}^d$ is
$O(m^{-(1+2/d)})$, an improved rate compared to the original $O(m^{-(1+1/d)})$,
where $m$ is the number of particles. This improved rate is the lowest for
ordered stratified resampling schemes, as conjectured in Gerber et al. (2019).
We also present an almost sure bound on the Wasserstein distance between the
original and Hilbert-curve-resampled empirical distributions. In light of these
theoretical results, we propose the stratified multiple-descendant growth (SMG)
algorithm, which allows us to explore the sample space more efficiently
compared to the standard i.i.d. multiple-descendant sampling-resampling
approach as measured by the Wasserstein metric. Numerical evidence is provided
to demonstrate the effectiveness of our proposed method.
"
"stat.TH","  Traditional Bayesian random partition models assume that the size of each
cluster grows linearly with the number of data points. While this is appealing
for some applications, this assumption is not appropriate for other tasks such
as entity resolution, modeling of sparse networks, and DNA sequencing tasks.
Such applications require models that yield clusters whose sizes grow
sublinearly with the total number of data points -- the microclustering
property. Motivated by these issues, we propose a general class of random
partition models that satisfy the microclustering property with
well-characterized theoretical properties. Our proposed models overcome major
limitations in the existing literature on microclustering models, namely a lack
of interpretability, identifiability, and full characterization of model
asymptotic properties. Crucially, we drop the classical assumption of having an
exchangeable sequence of data points, and instead assume an exchangeable
sequence of clusters. In addition, our framework provides flexibility in terms
of the prior distribution of cluster sizes, computational tractability, and
applicability to a large number of microclustering tasks. We establish
theoretical properties of the resulting class of priors, where we characterize
the asymptotic behavior of the number of clusters and of the proportion of
clusters of a given size. Our framework allows a simple and efficient Markov
chain Monte Carlo algorithm to perform statistical inference. We illustrate our
proposed methodology on the microclustering task of entity resolution, where we
provide a simulation study and real experiments on survey panel data.
"
"stat.TH","  This paper deals with nonparametric projection estimators of the drift
function computed from independent continuous observations, on a compact time
interval, of the solution of a stochastic differential equation driven by the
fractional Brownian motion. A projection least-squares estimator is defined and
a $\mathbb L^2$-type risk bound is proved for it. The consistency and rate of
convergence are established for these estimators in the case of the compactly
supported trigonometric basis or the $\mathbb R$-supported Hermite basis.
"
"stat.TH","  The problem of testing equality of the entire second order structure of two
independent functional processes is considered. A fully functional $L^2$-type
test is developed which evaluates, over all frequencies, the Hilbert-Schmidt
distance between the estimated spectral density operators of the two processes.
Under the assumption of a linear functional process, the asymptotic behavior of
the test statistic is investigated and its limiting distribution under the null
hypothesis is derived. Furthermore, a novel frequency domain bootstrap method
is developed which leads to a more accurate approximation of the distribution
of the test statistic under the null than the large sample Gaussian
approximation derived. Asymptotic validity of the bootstrap procedure is
established under very general conditions and consistency of the
bootstrap-based test under the alternative is proved. Numerical simulations
show that, even for small samples, the bootstrap-based test has a very good
size and power behavior. An application to a bivariate real-life functional
time series is also presented.
"
"stat.TH","  We propose an estimator for the mean of random variables in separable real
Banach spaces using the empirical characteristic function. Assuming that the
covariance operator of the random variable is bounded in a precise sense, we
show that the proposed estimator achieves the optimal sub-Gaussian rate, except
for a faster decaying mean-dependent term. Under a mild condition, an iterative
refinement procedure can essentially eliminate the mean-dependent term and
provide a shift-equivariant estimate. Our results particularly suggests that a
certain Gaussian width that appears in the best known rate in the literature
might not be necessary. Furthermore, using the boundedness of the
characteristic functions, we also show that, except possibly at high
signal-to-noise ratios, the proposed estimator is gracefully robust against
adversarial ""contamination"". Our analysis is overall concise and transparent,
thanks to the tractability of the characteristic functions.
"
"stat.TH","  This paper investigates asymptotic properties of a class of algorithms that
can be viewed as robust analogues of the classical empirical risk minimization.
These strategies are based on replacing the usual empirical average by a robust
proxy of the mean, such as the (version of) the median-of-means estimator. It
is well known by now that the excess risk of resulting estimators often
converges to 0 at optimal rates under much weaker assumptions than those
required by their ""classical"" counterparts. However, much less is known about
the asymptotic properties of the estimators themselves, for instance, whether
robust analogues of the maximum likelihood estimators are asymptotically
efficient. We make a step towards answering these questions and show that for a
wide class of parametric problems, minimizers of the appropriately defined
robust proxy of the risk converge to the minimizers of the true risk at the
same rate, and often have the same asymptotic variance, as the estimators
obtained by minimizing the usual empirical risk. Moreover, our results show
that robust algorithms based on the so-called ""min-max"" type procedures in many
cases provably outperform, is the asymptotic sense, algorithms based on direct
risk minimization.
"
"stat.TH","  Multilayer and multiplex networks are becoming common network data sets in
recent times. We consider the problem of identifying the common community
structure for a special type of multilayer networks called multi-relational
networks. We consider extensions of the spectral clustering methods for
multi-relational networks and give theoretical guarantees that the spectral
clustering methods recover community structure consistently for
multi-relational networks generated from multilayer versions of both stochastic
and degree-corrected block models even with dependence between network layers.
The methods are shown to work under optimal conditions on the degree parameter
of the networks to detect both assortative and disassortative community
structures with vanishing error proportions even if individual layers of the
multi-relational network has the network structures below community
detectability threshold. We reinforce the validity of the theoretical results
via simulations too.
"
"stat.TH","  We investigate shrinkage priors on power spectral densities for
complex-valued circular-symmetric autoregressive processes. We construct
shrinkage predictive power spectral densities, which asymptotically dominate
(i) the Bayesian predictive power spectral density based on the Jeffreys prior
and (ii) the estimative power spectral density with the maximal likelihood
estimator, where the Kullback-Leibler divergence from the true power spectral
density to a predictive power spectral density is adopted as a risk.
Furthermore, we propose general constructions of objective priors for K\""ahler
parameter spaces, utilizing a positive continuous eigenfunction of the
Laplace-Beltrami operator with a negative eigenvalue. We present numerical
experiments on a complex-valued stationary autoregressive model of order $1$.
"
"stat.TH","  We introduce and study a local linear nonparametric regression estimator for
censorship model. The main goal of this paper is, to establish the uniform
almost sure consistency result with rate over a compact set for the new
estimate. To support our theoretical result, a simulation study has been done
to make comparison with the classical regression estimator.
"
"stat.TH","  In this paper, we built a new nonparametric regression estimator with the
local linear method by using the mean squared relative error as a loss function
when the data are subject to random right censoring. We establish the uniform
almost sure consistency with rate over a compact set of the proposed estimator.
Some simulations are given to show the asymptotic behavior of the estimate in
different cases.
"
"stat.TH","  We study an estimator with a convex formulation for recovery of low-rank
matrices from rank-one projections. Using initial estimates of the factors of
the target $d_1\times d_2$ matrix of rank-$r$, the estimator operates as a
standard quadratic program in a space of dimension $r(d_1+d_2)$. This property
makes the estimator significantly more scalable than the convex estimators
based on lifting and semidefinite programming. Furthermore, we present a
streamlined analysis for exact recovery under the real Gaussian measurement
model, as well as the partially derandomized measurement model by using the
spherical 2-design. We show that under both models the estimator succeeds, with
high probability, if the number of measurements exceeds $r^2 (d_1+d_2)$ up to
some logarithmic factors. This sample complexity improves on the existing
results for nonconvex iterative algorithms.
"
"stat.TH","  In this paper, we present a novel approach to geostatistical filtering which
tackles two challenges encountered when applying this method to complex spatial
datasets: modeling the non-stationarity of the data while still being able to
work with large datasets. The approach is based on a finite element
approximation of Gaussian random fields expressed as an expansion of the
eigenfunctions of a Laplace--Beltrami operator defined to account for local
anisotropies. The numerical approximation of the resulting random fields using
a finite element approach is then leveraged to solve the scalability issue
through a matrix-free approach. Finally, two cases of application of this
approach, on simulated and real seismic data are presented.
"
"stat.TH","  Pocock and Simon's marginal procedure (Pocock and Simon, 1975) is often
implemented forbalancing treatment allocation over influential covariates in
clinical trials. However, the theoretical properties of Pocock and Simion's
procedure have remained largely elusive for decades. In this paper, we propose
a general framework for covariate-adaptive designs and establish the
corresponding theory under widely satisfied conditions. As a special case, we
obtain the theoretical properties of Pocock and Simon's marginal procedure: the
marginal imbalances and overall imbalance are bounded in probability, but the
within-stratum imbalances increase with the rate of $\sqrt{n}$ as the sample
size increases. The theoretical results provide new insights about balance
properties of covariate-adaptive randomization procedures and open a door to
study the theoretical properties of statistical inference for clinical trials
based on covariate-adaptive randomization procedures.
"
"stat.TH","  A striking result of [Acharya et al. 2017] showed that to estimate symmetric
properties of discrete distributions, plugging in the distribution that
maximizes the likelihood of observed multiset of frequencies, also known as the
profile maximum likelihood (PML) distribution, is competitive compared with any
estimators regardless of the symmetric property. Specifically, given $n$
observations from the discrete distribution, if some estimator incurs an error
$\varepsilon$ with probability at most $\delta$, then plugging in the PML
distribution incurs an error $2\varepsilon$ with probability at most
$\delta\cdot \exp(3\sqrt{n})$. In this paper, we strengthen the above result
and show that using a careful chaining argument, the error probability can be
reduced to $\delta^{1-c}\cdot \exp(c'n^{1/3+c})$ for arbitrarily small
constants $c>0$ and some constant $c'>0$. In particular, we show that the PML
distribution is an optimal estimator of the sorted distribution: it is
$\varepsilon$-close in sorted $\ell_1$ distance to the true distribution with
support size $k$ for any $n=\Omega(k/(\varepsilon^2 \log k))$ and $\varepsilon
\gg n^{-1/3}$, which are the information-theoretically optimal sample
complexity and the largest error regime where the classical empirical
distribution is sub-optimal, respectively.
  In order to strengthen the analysis of the PML, a key ingredient is to employ
novel ""continuity"" properties of the PML distributions and construct a chain of
suitable quantized PMLs, or ""coverings"". We also construct a novel
approximation-based estimator for the sorted distribution with a near-optimal
concentration property without any sample splitting, where as a byproduct we
obtain better trade-offs between the polynomial approximation error and the
maximum magnitude of coefficients in the Poisson approximation of $1$-Lipschitz
functions.
"
"stat.TH","  The notion of weighted Renyi's entropy for truncated random variables has
recently been proposed in the information-theoretic literature. In this paper,
we introduce a generalized measure of it for double truncated distribution,
namely weighted generalized interval entropy (WGIE), and study it in the
context of reliability analysis. Several properties, including monotonicity,
bounds and uniqueness of WGIE are investigated. Moreover, a simulation study is
carried out to demonstrate the performance of the estimates of the proposed
measure using simulated and real data sets. The role of WGIE in reliability
modeling has also been investigated for a real-life problem.
"
"stat.TH","  We consider multivariate centred Gaussian models for the random variable
$Z=(Z_1,\ldots, Z_p)$, invariant under the action of a subgroup of the group of
permutations on $\{1,\ldots, p\}$. Using the representation theory of the
symmetric group on the field of reals, we derive the distribution of the
maximum likelihood estimate of the covariance parameter $\Sigma$ and also the
analytic expression of the normalizing constant of the Diaconis-Ylvisaker
conjugate prior for the precision parameter $K=\Sigma^{-1}$. We can thus
perform Bayesian model selection in the class of complete Gaussian models
invariant by the action of a subgroup of the symmetric group, which we could
also call complete RCOP models. We illustrate our results with a toy example of
dimension $4$ and several examples for selection within cyclic groups,
including a high dimensional example with $p=100$.
"
"stat.TH","  In many applications, it is of interest to assess the relative contribution
of features (or subsets of features) toward the goal of predicting a response
-- in other words, to gauge the variable importance of features. Most recent
work on variable importance assessment has focused on describing the importance
of features within the confines of a given prediction algorithm. However, such
assessment does not necessarily characterize the prediction potential of
features, and may provide a misleading reflection of the intrinsic value of
these features. To address this limitation, we propose a general framework for
nonparametric inference on interpretable algorithm-agnostic variable
importance. We define variable importance as a population-level contrast
between the oracle predictiveness of all available features versus all features
except those under consideration. We propose a nonparametric efficient
estimation procedure that allows the construction of valid confidence
intervals, even when machine learning techniques are used. We also outline a
valid strategy for testing the null importance hypothesis. Through simulations,
we show that our proposal has good operating characteristics, and we illustrate
its use with data from a study of an antibody against HIV-1 infection.
"
"stat.TH","  Recently, the Wasserstein loss function has been proven to be effective when
applied to deterministic full-waveform inversion (FWI) problems. We consider
the application of this loss function in Bayesian FWI so that the uncertainty
can be captured in the solution. Other loss functions that are commonly used in
practice are also considered for comparison. Existence and stability of the
resulting Gibbs posteriors are shown on function space under weak assumptions
on the prior and model. In particular, the distribution arising from the
Wasserstein loss is shown to be quite stable with respect to high-frequency
noise in the data. We then illustrate the difference between the resulting
distributions numerically, using Laplace approximations and dimension-robust
MCMC to estimate the unknown velocity field and uncertainty associated with the
estimates.
"
"stat.TH","  Inferring causal relationships or related associations from observational
data can be invalidated by the existence of hidden confounding. We focus on a
high-dimensional linear regression setting, where the measured covariates are
affected by hidden confounding and propose the {\em Doubly Debiased Lasso}
estimator for individual components of the regression coefficient vector. Our
advocated method simultaneously corrects both the bias due to estimation of
high-dimensional parameters as well as the bias caused by the hidden
confounding. We establish its asymptotic normality and also prove that it is
efficient in the Gauss-Markov sense. The validity of our methodology relies on
a dense confounding assumption, i.e. that every confounding variable affects
many covariates. The finite sample performance is illustrated with an extensive
simulation study and a genomic application.
"
"stat.TH","  We consider the problem of fitting a polynomial to a set of data points, each
data point consisting of a feature vector and a response variable. In contrast
to standard least-squares polynomial regression, we require that the polynomial
regressor satisfy shape constraints, such as monotonicity with respect to a
variable, Lipschitz-continuity, or convexity over a region. Constraints of this
type appear quite frequently in a number of areas including economics,
operations research, and pricing. We show how to use semidefinite programming
to obtain polynomial regressors that have these properties. We further show
that, under some assumptions on the generation of the data points, the
regressors obtained are consistent estimators of the underlying
shape-constrained function that maps the feature vectors to the responses. We
apply our methodology to the US KLEMS dataset to estimate production of a
sector as a function of capital, energy, labor, materials, and services. We
observe that it outperforms the more traditional approach (which consists in
modelling the production curves as Cobb-Douglas functions) on 50 out of the 65
industries listed in the KLEMS database.
"
"stat.TH","  Multivariate subordinated L\'evy processes are widely employed in finance for
modeling multivariate asset returns. We propose to exploit non-linear
dependence among financial assets through multivariate cumulants of these
processes, for which we provide a closed form formula by using the multi-index
generalized Bell polynomials. Using multivariate cumulants, we perform a
sensitivity analysis, to investigate non-linear dependence as a function of the
model parameters driving the dependence structure
"
"stat.TH","  A bootstrap procedure for constructing pointwise or simultaneous prediction
intervals for a stationary functional time series is proposed. The procedure
exploits a general vector autoregressive representation of the time-reversed
series of Fourier coefficients appearing in the Karhunen-Lo\`{e}ve
representation of the functional process. It generates backwards-in-time,
functional replicates that adequately mimic the dependence structure of the
underlying process and have the same conditionally fixed curves at the end of
each functional pseudo-time series. The bootstrap prediction error distribution
is then calculated as the difference between the model-free,
bootstrap-generated future functional observations and the functional forecasts
obtained from the model used for prediction. This allows the estimated
prediction error distribution to account for not only the innovation and
estimation errors associated with prediction but also the possible errors from
model misspecification. We show the asymptotic validity of the bootstrap in
estimating the prediction error distribution of interest. Furthermore, the
bootstrap procedure allows for the construction of prediction bands that
achieve (asymptotically) the desired coverage. These prediction bands are based
on a consistent estimation of the distribution of the studentized prediction
error process. Through a simulation study and the analysis of two data sets, we
demonstrate the capabilities and the good finite-sample performance of the
proposed method.
"
"stat.TH","  Expectiles are a least squares analogue of quantiles which have lately
received substantial attention in actuarial and financial risk management
contexts. Unlike quantiles, expectiles define coherent risk measures and are
determined by tail expectations rather than tail probabilities; unlike the
Expected Shortfall, they define elicitable risk measures. This has motivated
recent studies of the behaviour and estimation of extreme expectile-based risk
measures. The case of stationary but weakly dependent observations has,
however, been left largely untouched, even though correctly accounting for the
uncertainty present in typical financial applications requires the
consideration of dependent data. We investigate the estimation of, and
construction of accurate confidence intervals for, extreme expectiles and
expectile-based Marginal Expected Shortfall in a general $\beta-$mixing
context, containing the classes of ARMA, ARCH and GARCH models with
heavy-tailed innovations that are of interest in financial applications. The
methods are showcased in a numerical simulation study and on real financial
data.
"
"stat.TH","  Markov chain Monte Carlo (MCMC) sampling algorithms have dominated the
literature on posterior computation. However, MCMC faces substantial hurdles in
performing efficient posterior sampling for challenging Bayesian models,
particularly in high-dimensional and large data settings. Motivated in part by
such hurdles, an intriguing new class of piecewise deterministic Markov
processes (PDMPs) has recently been proposed as an alternative to MCMC. One of
the most popular types of PDMPs is known as the zig-zag (ZZ) sampler. Such
algorithms require a computational upper bound in a Poisson thinning step, with
performance improving for tighter bounds. In order to facilitate scaling to
larger classes of problems, we propose a general class of Gibbs zig-zag (GZZ)
samplers. GZZ allows parameters to be updated in blocks with ZZ applied to
certain parameters and traditional MCMC style updates to others. This provides
a flexible framework to combine PDMPs with the rich literature on MCMC
algorithms. We prove appealing theoretical properties of GZZ and demonstrate it
on posterior sampling for logistic models with shrinkage priors for
high-dimensional regression and random effects.
"
"stat.TH","  Bipartite incidence graph sampling provides a unified representation of many
sampling situations for the purpose of estimation, including the existing
unconventional sampling methods, such as indirect, network or adaptive cluster
sampling, which are not originally described as graph problems. We develop a
large class of linear estimators based on the edges in the sample bipartite
incidence graph, subjected to a general condition of design unbiasedness. The
class contains as special cases the classic Horvitz-Thompson estimator, as well
as the other unbiased estimators in the literature of unconventional sampling,
which can be traced back to Birnbaum and Sirken (1965). Our generalisation
allows one to devise other unbiased estimators, thereby providing a potential
of efficiency gains in applications. Illustrations are given for adaptive
cluster sampling, line-intercept sampling and simulated graphs.
"
"stat.TH","  The choice of the prior distribution is a key aspect of Bayesian analysis.
For the spatial regression setting a subjective prior choice for the parameters
may not be trivial, from this perspective, using the objective Bayesian
analysis framework a reference is introduced for the spatial Student-t
regression model with unknown degrees of freedom. The spatial Student-t
regression model poses two main challenges when eliciting priors: one for the
spatial dependence parameter and the other one for the degrees of freedom. It
is well-known that the propriety of the posterior distribution over objective
priors is not always guaranteed, whereas the use of proper prior distributions
may dominate and bias the posterior analysis. In this paper, we show the
conditions under which our proposed reference prior yield to a proper posterior
distribution. Simulation studies are used in order to evaluate the performance
of the reference prior to a commonly used vague proper prior.
"
"stat.TH","  The goal of this paper is to design a causal inference method accounting for
complex interactions between causal factors. The proposed method relies on a
category theoretical reformulation of the definitions of dependent variables,
independent variables and latent variables in terms of products and arrows in
the category of unlabeled partitions. Throughout the paper, we demonstrate how
the proposed method accounts for possible hidden variables, such as
environmental variables or noise, and how it can be interpreted statistically
in terms of $p$-values. This interpretation, from category theory to
statistics, is implemented through a collection of propositions highlighting
the functorial properties of ANOVA. We use these properties in combination with
our category theoretical framework to provide solutions to causal inference
problems with both sound algebraic and statistical properties. As an
application, we show how the proposed method can be used to design a
combinatorial genome-wide association algorithm for the field of genetics.
"
"stat.TH","  We tackle the community detection problem in the Stochastic Block Model (SBM)
when the communities of the nodes of the graph are assigned with a Markovian
dynamic. To recover the partition of the nodes, we adapt the relaxed K-means
SDP program presented in [11]. We identify the relevant signal-to-noise ratio
(SNR) in our framework and we prove that the misclassification error decays
exponentially fast with respect to this SNR. We provide infinity norm
consistent estimation of the parameters of our model and we discuss our results
through the prism of classical degree regimes of the SBMs' literature. MSC 2010
subject classifications: Primary 68Q32; secondary 68R10, 90C35.
"
"stat.TH","  We interpret steady linear statistical inverse problems as artificial dynamic
systems with white noise and introduce a stochastic differential equation (SDE)
system where the inverse of the ending time $T$ naturally plays the role of the
squared noise level. The time-continuous framework then allows us to apply
classical methods from data assimilation, namely the Kalman-Bucy filter and
3DVAR, and to analyze their behaviour as a regularization method for the
original problem. Such treatment offers some connections to the famous
asymptotical regularization method, which has not yet been analyzed in the
context of random noise. We derive error bounds for both methods in terms of
the mean-squared error under standard assumptions and discuss commonalities and
differences between both approaches. If an additional tuning parameter $\alpha$
for the initial covariance is chosen appropriately in terms of the ending time
$T$, one of the proposed methods gains order optimality. Our results extend
theoretical findings in the discrete setting given in the recent paper Iglesias
et al. (2017). Numerical examples confirm our theoretical results.
"
"stat.TH","  We consider the problem of the Bayesian inference of drift and diffusion
coefficient functions in a stochastic differential equation given discrete
observations of a realisation of its solution. We give conditions for the
well-posedness and stable approximations of the posterior measure. These
conditions in particular allow for priors with unbounded support. Our proof
relies on the explicit construction of transition probability densities using
the parametrix method for general parabolic equations. We then study an
application of these results in inferring the rates of Birth-and-Death
processes.
"
"stat.TH","  We study statistical properties of the k-nearest neighbors algorithm for
multiclass classification, with a focus on settings where the number of classes
may be large and/or classes may be highly imbalanced. In particular, we
consider a variant of the k-nearest neighbor classifier with non-uniform
class-weightings, for which we derive upper and minimax lower bounds on
accuracy, class-weighted risk, and uniform error. Additionally, we show that
uniform error bounds lead to bounds on the difference between empirical
confusion matrix quantities and their population counterparts across a set of
weights. As a result, we may adjust the class weights to optimize
classification metrics such as F1 score or Matthew's Correlation Coefficient
that are commonly used in practice, particularly in settings with imbalanced
classes. We additionally provide a simple example to instantiate our bounds and
numerical experiments.
"
"stat.TH","  We undertake a precise study of the asymptotic and non-asymptotic properties
of stochastic approximation procedures with Polyak-Ruppert averaging for
solving a linear system $\bar{A} \theta = \bar{b}$. When the matrix $\bar{A}$
is Hurwitz, we prove a central limit theorem (CLT) for the averaged iterates
with fixed step size and number of iterations going to infinity. The CLT
characterizes the exact asymptotic covariance matrix, which is the sum of the
classical Polyak-Ruppert covariance and a correction term that scales with the
step size. Under assumptions on the tail of the noise distribution, we prove a
non-asymptotic concentration inequality whose main term matches the covariance
in CLT in any direction, up to universal constants. When the matrix $\bar{A}$
is not Hurwitz but only has non-negative real parts in its eigenvalues, we
prove that the averaged LSA procedure actually achieves an $O(1/T)$ rate in
mean-squared error. Our results provide a more refined understanding of linear
stochastic approximation in both the asymptotic and non-asymptotic settings. We
also show various applications of the main results, including the study of
momentum-based stochastic gradient methods as well as temporal difference
algorithms in reinforcement learning.
"
"stat.TH","  Motivated by the need to statistically quantify differences between modern
(complex) data-sets which commonly result as high-resolution measurements of
stochastic processes varying over a continuum, we propose novel testing
procedures to detect relevant differences between the second order dynamics of
two functional time series. In order to take the between-function dynamics into
account that characterize this type of functional data, a frequency domain
approach is taken. Test statistics are developed to compare differences in the
spectral density operators and in the primary modes of variation as encoded in
the associated eigenelements. Under mild moment conditions, we show convergence
of the underlying statistics to Brownian motions and obtain pivotal test
statistics via a self-normalization approach. The latter is essential because
the nuisance parameters can be unwieldly and their robust estimation
infeasible, especially if the two functional time series are dependent. Besides
from these novel features, the properties of the tests are robust to any choice
of frequency band enabling also to compare energy contents at a single
frequency. The finite sample performance of the tests are verified through a
simulation study and are illustrated with an application to fMRI data.
"
"stat.TH","  We utilize a connection between compositional kernels and branching processes
via Mehler's formula to study deep neural networks. This new probabilistic
insight provides us a novel perspective on the mathematical role of activation
functions in compositional neural networks. We study the unscaled and rescaled
limits of the compositional kernels and explore the different phases of the
limiting behavior, as the compositional depth increases. We investigate the
memorization capacity of the compositional kernels and neural networks by
characterizing the interplay among compositional depth, sample size,
dimensionality, and non-linearity of the activation. Explicit formulas on the
eigenvalues of the compositional kernel are provided, which quantify the
complexity of the corresponding reproducing kernel Hilbert space. On the
methodological front, we propose a new random features algorithm, which
compresses the compositional layers by devising a new activation function.
"
"stat.TH","  Modularity is a popular metric for quantifying the degree of community
structure within a network. The distribution of the largest eigenvalue of a
network's edge weight or adjacency matrix is well studied and is frequently
used as a substitute for modularity when performing statistical inference.
However, we show that the largest eigenvalue and modularity are asymptotically
uncorrelated, which suggests the need for inference directly on modularity
itself when the network size is large. To this end, we derive the asymptotic
distributions of modularity in the case where the network's edge weight matrix
belongs to the Gaussian Orthogonal Ensemble, and study the statistical power of
the corresponding test for community structure under some alternative model. We
empirically explore universality extensions of the limiting distribution and
demonstrate the accuracy of these asymptotic distributions through type I error
simulations. We also compare the empirical powers of the modularity based tests
with some existing methods. Our method is then used to test for the presence of
community structure in two real data applications.
"
"stat.TH","  In various applications of heavy-tail modelling, the assumed Pareto behavior
is tempered ultimately in the range of the largest data. In insurance
applications, claim payments are influenced by claim management and claims may
for instance be subject to a higher level of inspection at highest damage
levels leading to weaker tails than apparent from modal claims. Generalizing
earlier results of Meerschaert et al. (2012) and Raschke (2019), in this paper
we consider tempering of a Pareto-type distribution with a general Weibull
distribution in a peaks-over-threshold approach. This requires to modulate the
tempering parameters as a function of the chosen threshold. Modelling such a
tempering effect is important in order to avoid overestimation of risk measures
such as the Value-at-Risk (VaR) at high quantiles. We use a pseudo maximum
likelihood approach to estimate the model parameters, and consider the
estimation of extreme quantiles. We derive basic asymptotic results for the
estimators, give illustrations with simulation experiments and apply the
developed techniques to fire and liability insurance data, providing insight
into the relevance of the tempering component in heavy-tail modelling.
"
"stat.TH","  Let the Ornstein-Uhlenbeck process $(X_t)_{t\ge0}$ driven by a fractional
Brownian motion $B^{H }$, described by $dX_t = -\theta X_t dt + \sigma dB_t^{H
}$ be observed at discrete time instants
  $t_k=kh$, $k=0, 1, 2, \cdots, 2n+2 $. We propose ergodic type statistical
estimators $\hat \theta_n $, $\hat H_n $ and $\hat \sigma_n $ to estimate all
the parameters $\theta $, $H $ and $\sigma $ in the above Ornstein-Uhlenbeck
model simultaneously. We prove the strong consistence and the rate of
convergence of the estimators. The step size $h$ can be arbitrarily fixed and
will not be forced to go zero, which is usually a reality. The tools to use are
the generalized moment approach (via ergodic theorem) and the Malliavin
calculus.
"
"stat.TH","  We define a multivariate medial correlation coefficient that extends the
probabilistic interpretation and properties of Blomqvist's $\beta$ coefficient,
incorporates multivariate marginal dependencies and it preserves a stronger
multivariate concordance relation. We determine the maximum and minimum values
attainable and illustrate the results in some models. We end with an
application on real datasets.
"
"stat.TH","  Shrinkage prior are becoming more and more popular in Bayesian modeling for
high dimensional sparse problems due to its computational efficiency. Recent
works show that a polynomially decaying prior leads to satisfactory posterior
asymptotics under regression models. In the literature, statisticians have
investigated how the global shrinkage parameter, i.e., the scale parameter, in
a heavy tail prior affects the posterior contraction. In this work, we explore
how the shape of the prior, or more specifically, the polynomial order of the
prior tail affects the posterior. We discover that, under the sparse normal
means models, the polynomial order does affect the multiplicative constant of
the posterior contraction rate. More importantly, if the polynomial order is
sufficiently close to 1, it will induce the optimal Bayesian posterior
convergence, in the sense that the Bayesian contraction rate is sharply
minimax, i.e., not only the order, but also the multiplicative constant of the
posterior contraction rate are optimal. The above Bayesian sharp minimaxity
holds when the global shrinkage parameter follows a deterministic choice which
depends on the unknown sparsity $s$. Therefore, a Beta-prior modeling is
further proposed, such that our sharply minimax Bayesian procedure is adaptive
to unknown $s$. Our theoretical discoveries are justified by simulation
studies.
"
"stat.TH","  This article describes the inferential procedures and Bayesian optimal
life-testing issues under Type-II unified hybrid censoring scheme. First, the
explicit expressions of expected number of failures, expected duration of
testing and Fisher information matrix for the unknown parameters of the
underlying lifetime model are derived. Then, using these quantities, the
Bayesian optimal life-testing plans are computed in subsequent section. A cost
constraint D-optimal optimization problem has been formulated and the
corresponding solution algorithm is provided to obtain optimal plans.
Computational procedures are illustrated through numerical examples.
"
"stat.TH","  This work performs a non-asymptotic analysis of the generalized Lasso under
the assumption of sub-exponential data. Our main results continue recent
research on the benchmark case of (sub-)Gaussian sample distributions and
thereby explore what conclusions are still valid when going beyond. While many
statistical features of the generalized Lasso remain unaffected (e.g.,
consistency), the key difference becomes manifested in the way how the
complexity of the hypothesis set is measured. It turns out that the estimation
error can be controlled by means of two complexity parameters that arise
naturally from a generic-chaining-based proof strategy. The output model can be
non-realizable, while the only requirement for the input vector is a generic
concentration inequality of Bernstein-type, which can be implemented for a
variety of sub-exponential distributions. This abstract approach allows us to
reproduce, unify, and extend previously known guarantees for the generalized
Lasso. In particular, we present applications to semi-parametric output models
and phase retrieval via the lifted Lasso. Moreover, our findings are discussed
in the context of sparse recovery and high-dimensional estimation problems.
"
"stat.TH","  Psychologists developed Multiple Factor Analysis to decompose multivariate
data into a small number of interpretable factors without any a priori
knowledge about those factors. In this form of factor analysis, the Varimax
""factor rotation"" is a key step to make the factors interpretable. Charles
Spearman and many others objected to factor rotations because the factors seem
to be rotationally invariant. These objections are still reported in all
contemporary multivariate statistics textbooks. This is an engima because this
vintage form of factor analysis has survived and is widely popular because,
empirically, the factor rotation often makes the factors easier to interpret.
We argue that the rotation makes the factors easier to interpret because, in
fact, the Varimax factor rotation performs statistical inference. We show that
Principal Components Analysis (PCA) with the Varimax rotation provides a
unified spectral estimation strategy for a broad class of modern factor models,
including the Stochastic Blockmodel and a natural variation of Latent Dirichlet
Allocation (i.e., ""topic modeling""). In addition, we show that Thurstone's
widely employed sparsity diagnostics implicitly assess a key ""leptokurtic""
condition that makes the rotation statistically identifiable in these models.
Taken together, this shows that the know-how of Vintage Factor Analysis
performs statistical inference, reversing nearly a century of statistical
thinking on the topic. With a sparse eigensolver, PCA with Varimax is both fast
and stable. Combined with Thurstone's straightforward diagnostics, this vintage
approach is suitable for a wide array of modern applications.
"
"stat.TH","  We consider the problem of simultaneous variable selection and estimation of
the corresponding regression coefficients in an ultra-high dimensional linear
regression models, an extremely important problem in the recent era. The
adaptive penalty functions are used in this regard to achieve the oracle
variable selection property along with easier computational burden. However,
the usual adaptive procedures (e.g., adaptive LASSO) based on the squared error
loss function is extremely non-robust in the presence of data contamination
which are quite common with large-scale data (e.g., noisy gene expression data,
spectra and spectral data). In this paper, we present a regularization
procedure for the ultra-high dimensional data using a robust loss function
based on the popular density power divergence (DPD) measure along with the
adaptive LASSO penalty. We theoretically study the robustness and the
large-sample properties of the proposed adaptive robust estimators for a
general class of error distributions; in particular, we show that the proposed
adaptive DPD-LASSO estimator is highly robust, satisfies the oracle variable
selection property, and the corresponding estimators of the regression
coefficients are consistent and asymptotically normal under easily verifiable
set of assumptions. Numerical illustrations are provided for the mostly used
normal error density. Finally, the proposal is applied to analyze an
interesting spectral dataset, in the field of chemometrics, regarding the
electron-probe X-ray microanalysis (EPXMA) of archaeological glass vessels from
the 16th and 17th centuries.
"
"stat.TH","  Mixtures of product distributions are a powerful device for learning about
heterogeneity within data populations. In this class of latent structure
models, de Finetti's mixing measure plays the central role for describing the
uncertainty about the latent parameters representing heterogeneity. In this
paper posterior contraction theorems for de Finetti's mixing measure arising
from finite mixtures of product distributions will be established, under the
setting the number of exchangeable sequences of observed variables increases
while sequence length(s) may be either fixed or varied. The role of both the
number of sequences and the sequence lengths will be carefully examined. In
order to obtain concrete rates of convergence, a first-order identifiability
theory for finite mixture models and a family of sharp inverse bounds for
mixtures of product distributions will be developed via a harmonic analysis of
such latent structure models. This theory is applicable to broad classes of
probability kernels composing the mixture model of product distributions for
both continuous and discrete domain. Examples of interest include the case the
probability kernel is only weakly identifiable in the sense of Ho and Nguyen
(2016), the case where the kernel is itself a mixture distribution as in
hierarchical models, and the case the kernel may not have a density with
respect to a dominating measure on an abstract domain such as Dirichlet
processes.
"
"stat.TH","  Data uniformity is a concept associated with several semantic data
characteristics such as lack of features, correlation and sample bias. This
article introduces a novel measure to assess data uniformity and detect uniform
pointsets on high-dimensional Euclidean spaces. Spatial uniformity measure
builds upon the isomorphism between hyperspherical chords and L2-normalised
data Euclidean distances, which is implied by the fact that, in Euclidean
spaces, L2-normalised data can be geometrically defined as points on a
hypersphere. The imposed connection between the distance distribution of
uniformly selected points and the hyperspherical chord length distribution is
employed to quantify uniformity. More specifically,, the closed-form expression
of hypersphere chord length distribution is revisited extended, before
examining a few qualitative and quantitative characteristics of this
distribution that can be rather straightforwardly linked to data uniformity.
The experimental section includes validation in four distinct setups, thus
substantiating the potential of the new uniformity measure on practical
data-science applications.
"
"stat.TH","  We consider robust estimation when outputs are adversarially contaminated.
Nguyen and Tran (2012) proposed an extended Lasso for robust parameter
estimation and then they showed the convergence rate of the estimation error.
Recently, Dalalyan and Thompson (2019) gave some useful inequalities and then
they showed a faster convergence rate than Nguyen and Tran (2012). They focused
on the fact that the minimization problem of the extended Lasso can become that
of the penalized Huber loss function with $L_1$ penalty. The distinguishing
point is that the Huber loss function includes an extra tuning parameter, which
is different from the conventional method. We give the proof, which is
different from Dalalyan and Thompson (2019) and then we give the same
convergence rate as Dalalyan and Thompson (2019). The significance of our proof
is to use some specific properties of the Huber function. Such techniques have
not been used in the past proofs.
"
"stat.TH","  Based on a Gaussian mixture type model , we derive an eigen selection
procedure that improves the usual spectral clustering in high-dimensional
settings. Concretely, we derive the asymptotic expansion of the spiked
eigenvalues under eigenvalue multiplicity and eigenvalue ratio concentration
results, giving rise to the first theory-backed eigen selection procedure in
spectral clustering. The resulting eigen-selected spectral clustering (ESSC)
algorithm enjoys better stability and compares favorably against canonical
alternatives. We demonstrate the advantages of ESSC using extensive simulation
and multiple real data studies.
"
"stat.TH","  It is well known that any continuous probability density function on
$\mathbb{R}^m$ can be approximated arbitrarily well by a finite mixture of
normal distributions, provided that the number of mixture components is
sufficiently large. The von-Mises-Fisher distribution, defined on the unit
hypersphere $S^m$ in $\mathbb{R}^{m+1}$, has properties that are analogous to
those of the multivariate normal on $\mathbb{R}^{m+1}$. We prove that any
continuous probability density function on $S^m$ can be approximated to
arbitrary degrees of accuracy by a finite mixture of von-Mises-Fisher
distributions.
"
"stat.TH","  Nearly all statistical inference methods were developed for the regime where
the number $N$ of data samples is much larger than the data dimension $p$.
Inference protocols such as maximum likelihood (ML) or maximum a posteriori
probability (MAP) are unreliable if $p=O(N)$, due to overfitting. This
limitation has for many disciplines with increasingly high-dimensional data
become a serious bottleneck. We recently showed that in Cox regression for
time-to-event data the overfitting errors are not just noise but take mostly
the form of a bias, and how with the replica method from statistical physics
once can model and predict this bias and the noise statistics. Here we extend
our approach to arbitrary generalized linear regression models (GLM), with
possibly correlated covariates. We analyse overfitting in ML/MAP inference
without having to specify data types or regression models, relying only on the
GLM form, and derive generic order parameter equations for the case of $L2$
priors. Second, we derive the probabilistic relationship between true and
inferred regression coefficients in GLMs, and show that, for the relevant
hyperparameter scaling and correlated covariates, the $L2$ regularization
causes a predictable direction change of the coefficient vector. Our results,
illustrated by application to linear, logistic, and Cox regression, enable one
to correct ML and MAP inferences in GLMs systematically for overfitting bias,
and thus extend their applicability into the hitherto forbidden regime
$p=O(N)$.
"
"stat.TH","  A few matrix-vector multiplications with random vectors are often sufficient
to obtain reasonably good estimates for the norm of a general matrix or the
trace of a symmetric positive semi-definite matrix. Several such probabilistic
estimators have been proposed and analyzed for standard Gaussian and Rademacher
random vectors. In this work, we consider the use of rank-one random vectors,
that is, Kronecker products of (smaller) Gaussian or Rademacher vectors. It is
not only cheaper to sample such vectors but it can sometimes also be much
cheaper to multiply a matrix with a rank-one vector instead of a general
vector. In this work, theoretical and numerical evidence is given that the use
of rank-one instead of unstructured random vectors still leads to good
estimates. In particular, it is shown that our rank-one estimators multiplied
with a modest constant constitute, with high probability, upper bounds of the
quantity of interest. Partial results are provided for the case of lower
bounds. The application of our techniques to condition number estimation for
matrix functions is illustrated.
"
"stat.TH","  We consider change-point tests based on rank statistics to test for
structural changes in long-range dependent observations. Under the hypothesis
of stationary time series and under the assumption of a change with decreasing
change-point height, the asymptotic distributions of corresponding test
statistics are derived. For this, a uniform reduction principle for the
sequential empirical process in a two-parameter Skorohod space equipped with a
weighted supremum norm is proved. Moreover, we compare the efficiency of rank
tests resulting from the consideration of different score functions. Under
Gaussianity, the asymptotic relative efficiency of rank-based tests with
respect to the CuSum test is 1, irrespective of the score function. Regarding
the practical implementation of rank-based change-point tests, we suggest to
combine self-normalized rank statistics with subsampling. The theoretical
results are accompanied by simulation studies that, in particular, allow for a
comparison of rank tests resulting from different score functions. With respect
to the finite sample performance of rank-based change-point tests, the Van der
Waerden rank test proves to be favorable in a broad range of situations.
Finally, we analyze data sets from economy, hydrology, and network traffic
monitoring in view of structural changes and compare our results to previous
analysis of the data.
"
"stat.TH","  Modelling multivariate systems is important for many applications in
engineering and operational research. The multivariate distributions under
scrutiny usually have no analytic or closed form. Therefore their modelling
employs a numerical technique, typically multivariate simulations, which can
have very high dimensions. Random Orthogonal Matrix (ROM) simulation is a
method that has gained some popularity because of the absence of certain
simulation errors. Specifically, it exactly matches a target mean, covariance
matrix and certain higher moments with every simulation. This paper extends the
ROM simulation algorithm presented by Hanke et al. (2017), hereafter referred
to as HPSW, which matches the target mean, covariance matrix and Kollo skewness
vector exactly. Our first contribution is to establish necessary and sufficient
conditions for the HPSW algorithm to work. Our second contribution is to
develop a general approach for constructing admissible values in the HPSW. Our
third theoretical contribution is to analyse the effect of multivariate sample
concatenation on the target Kollo skewness. Finally, we illustrate the
extensions we develop here using a simulation study.
"
"stat.TH","  Network method of moments arXiv:1202.5101 is an important tool for
nonparametric network inferences. However, there has been little investigation
on accurate descriptions of the sampling distributions of network moment
statistics. In this paper, we present the first higher-order accurate
approximation to the sampling CDF of a studentized network moment by Edgeworth
expansion. In sharp contrast to classical literature on noiseless U-statistics,
we showed that the Edgeworth expansion of a network moment statistic as a noisy
U-statistic can achieve higher-order accuracy without non-lattice or smoothness
assumptions but just requiring weak regularity conditions. Behind this result
is our surprising discovery that the two typically-hated factors in network
analysis, namely, sparsity and edge-wise observational errors, jointly play a
blessing role, contributing a crucial self-smoothing effect in the network
moment statistic and making it analytically tractable. Our assumptions match
the minimum requirements in related literature.
  For practitioners, our empirical Edgeworth expansion is highly accurate and
computationally efficient. It is also easy to implement. These were
demonstrated by comprehensive simulation studies.
  We showcase three applications of our results in network inference. We
proved, to our knowledge, for the first time that some network bootstraps enjoy
higher-order accuracy, and provided theoretical guidance for tuning network
sub-sampling. We also derived a one-sample test and Cornish-Fisher confidence
interval for any given moment, both with analytical formulation and explicit
error rates.
"
"stat.TH","  For Kolmogorov test we find necessary and sufficient conditions of uniform
consistency of sets of alternatives approaching to hypothesis. Sets of
alternatives can be defined both in terms of distribution functions and in
terms of densities.
"
"stat.TH","  Doubly-intractable distributions appear naturally as posterior distributions
in Bayesian inference frameworks whenever the likelihood contains a normalizing
function $Z$. Having two such functions $Z$ and $\widetilde Z$ we provide
estimates of the total variation and Wasserstein distance of the resulting
posterior probability measures. As a consequence this leads to local Lipschitz
continuity w.r.t. $Z$. In the more general framework of a random function
$\widetilde Z$ we derive bounds on the expected total variation and expected
Wasserstein distance. The applicability of the estimates is illustrated within
the setting of two representative Monte Carlo recovery scenarios.
"
"stat.TH","  This article gives a synopsis on new developments in affine invariant tests
for multivariate normality in an i.i.d.-setting, with special emphasis on
asymptotic properties of several classes of weighted $L^2$-statistics. Since
weighted $L^2$-statistics typically have limit normal distributions under fixed
alternatives to normality, they open ground for a neighborhood of model
validation for normality. The paper also reviews several other invariant tests
for this problem, notably the energy test, and it presents the results of a
large-scale simulation study. All tests under study are implemented in the
accompanying R-package mnt.
"
"stat.TH","  Besides the classical distinction of correlation and dependence, many
dependence measures bear further pitfalls in their application and
interpretation. The aim of this paper is to raise and recall awareness of some
of these limitations by explicitly discussing Pearson's correlation and the
multivariate dependence measures: distance correlation, distance
multicorrelations and their copula versions. The discussed aspects include
types of dependence, bias of empirical measures, influence of marginal
distributions and dimensions.
  In general it is recommended to use a proper dependence measure instead of
Pearson's correlation. Moreover, a measure which is distribution-free (at least
in some sense) can help to avoid certain systematic errors. Nevertheless, in a
truly multivariate setting only the p-values of the corresponding independence
tests provide always values with indubitable interpretation.
"
"stat.TH","  For more than a century and a half it has been widely-believed (but was never
rigorously shown) that the physics of diffraction imposes certain fundamental
limits on the resolution of an optical system. However our understanding of
what exactly can and cannot be resolved has never risen above heuristic
arguments which, even worse, appear contradictory. In this work we remedy this
gap by studying the diffraction limit as a statistical inverse problem and,
based on connections to provable algorithms for learning mixture models, we
rigorously prove upper and lower bounds on how many photons we need (and how
precisely we need to record their locations) to resolve closely-spaced point
sources.
  We give the first provable algorithms that work regardless of the separation,
but only for a constant number of point sources. Surprisingly, we show that
when the number of point sources becomes large, there is a phase transition
where the sample complexity goes from polynomial to exponential, and we pin
down its location to within a universal constant that is independent of the
number of point sources. Thus it is rigorously possible both to break the
diffraction limit and yet to prove strong impossibility results depending on
the setup. This is the first non-asymptotic statistical foundation for
resolution in a model arising from first principles in physics, and helps
clarify many omnipresent debates in the optics literature.
"
"stat.TH","  We study the maximum likelihood (ML) degree of linear concentration models in
algebraic statistics. We relate it to an intersection problem on the variety of
complete quadrics. This allows us to provide an explicit, basic, albeit of high
computational complexity, formula for the ML-degree. The variety of complete
quadrics is an exact analog for symmetric matrices of the permutohedron variety
for the diagonal matrices.
"
"stat.TH","  Statistical analysis of high-dimensional functional times series arises in
various applications. Under this scenario, in addition to the intrinsic
infinite-dimensionality of functional data, the number of functional variables
can grow with the number of serially dependent functional observations. In this
paper, we focus on the theoretical analysis of relevant estimated
cross-(auto)covariance terms between two multivariate functional time series or
a mixture of multivariate functional and scalar time series beyond the
Gaussianity assumption. We introduce a new perspective on dependence by
proposing functional cross-spectral stability measure to characterize the
effect of dependence on these estimated cross terms, which are essential in the
estimates for additive functional linear regressions. With the proposed
functional cross-spectral stability measure, we develop useful concentration
inequalities for estimated cross-(auto)covariance matrix functions to
accommodate more general sub-Gaussian functional linear processes and,
furthermore, establish finite sample theory for relevant estimated terms under
a commonly adopted functional principal component analysis framework. Using our
derived non-asymptotic results, we investigate the convergence properties of
the regularized estimates for two additive functional linear regression
applications under sparsity assumptions including functional linear lagged
regression and partially functional linear regression in the context of
high-dimensional functional/scalar time series.
"
"stat.TH","  We establish nonparametric identification in a class of so-called index
models using a novel approach that relies on general topological results. Our
proof strategy requires substantially weaker conditions on the functions and
distributions characterizing the model compared to existing strategies; in
particular, it does not require any large support conditions on the regressors
of our model. We apply the general identification result to additive random
utility and competing risk models.
"
"stat.TH","  We provide statistical learning guarantees for two unsupervised learning
tasks in the context of compressive statistical learning, a general framework
for resource-efficient large-scale learning that we introduced in a companion
paper. The principle of compressive statistical learning is to compress a
training collection, in one pass, into a low-dimensional sketch (a vector of
random empirical generalized moments) that captures the information relevant to
the considered learning task. We explicit random feature functions which
empirical averages preserve the needed information for compressive clustering
and compressive Gaussian mixture modeling with fixed known variance, and
establish sufficient sketch sizes given the problem dimensions.
"
"stat.TH","  In this paper, we consider high-dimensional Gaussian graphical models where
the true underlying graph is decomposable. A hierarchical $G$-Wishart prior is
proposed to conduct a Bayesian inference for the precision matrix and its graph
structure. Although the posterior asymptotics using the $G$-Wishart prior has
received increasing attention in recent years, most of results assume moderate
high-dimensional settings, where the number of variables $p$ is smaller than
the sample size $n$. However, this assumption might not hold in many real
applications such as genomics, speech recognition and climatology. Motivated by
this gap, we investigate asymptotic properties of posteriors under the
high-dimensional setting where $p$ can be much larger than $n$. The pairwise
Bayes factor consistency, posterior ratio consistency and graph selection
consistency are obtained in this high-dimensional setting. Furthermore, the
posterior convergence rate for precision matrices under the matrix
$\ell_1$-norm is derived, which turns out to coincide with the minimax
convergence rate for sparse precision matrices. A simulation study confirms
that the proposed Bayesian procedure outperforms competitors.
"
"stat.TH","  We study the maximum score statistic to detect and estimate local signals in
the form of change-points in the level, slope, or other property of a sequence
of observations, and to segment the sequence when there appear to be multiple
changes. We find that when observations are serially dependent, the
change-points can lead to upwardly biased estimates of autocorrelations,
resulting in a sometimes serious loss of power. Examples involving temperature
variations, the level of atmospheric greenhouse gases, suicide rates and daily
incidence of COVID-19 illustrate the general theory.
"
"stat.TH","  We investigate the construction of early stopping rules in the nonparametric
regression problem where iterative learning algorithms are used and the optimal
iteration number is unknown. More precisely, we study the discrepancy
principle, as well as modifications based on smoothed residuals, for kernelized
spectral filter learning algorithms including gradient descent. Our main
theoretical bounds are oracle inequalities established for the empirical
estimation error (fixed design), and for the prediction error (random design).
From these finite-sample bounds it follows that the classical discrepancy
principle is statistically adaptive for slow rates occurring in the hard
learning scenario, while the smoothed discrepancy principles are adaptive over
ranges of faster rates (resp. higher smoothness parameters). Our approach
relies on deviation inequalities for the stopping rules in the fixed design
setting, combined with change-of-norm arguments to deal with the random design
setting.
"
"stat.TH","  The flexibility and wide applicability of the Fisher randomization test (FRT)
makes it an attractive tool for assessment of causal effects of interventions
from modern-day randomized experiments that are increasing in size and
complexity. This paper provides a theoretical inferential framework for FRT by
establishing its connection with confidence distributions Such a connection
leads to development of (i) an unambiguous procedure for inversion of FRTs to
generate confidence intervals with guaranteed coverage, (ii) generic and
specific methods to combine FRTs from multiple independent experiments with
theoretical guarantees and (iii) new insights on the effect of size of the
Monte Carlo sample on the results of FRT. Our developments pertain to finite
sample settings but have direct extensions to large samples. Simulations and a
case example demonstrate the benefit of these new developments.
"
"stat.TH","  In this paper, we propose a new statistical inference method for massive data
sets, which is very simple and efficient by combining divide-and-conquer method
and empirical likelihood. Compared with two popular methods (the bag of little
bootstrap and the subsampled double bootstrap), we make full use of data sets,
and reduce the computation burden. Extensive numerical studies and real data
analysis demonstrate the effectiveness and flexibility of our proposed method.
Furthermore, the asymptotic property of our method is derived.
"
"stat.TH","  We study minimax convergence rates of nonparametric density estimation in the
Huber contamination model, in which a proportion of the data comes from an
unknown outlier distribution. We provide the first results for this problem
under a large family of losses, called Besov integral probability metrics
(IPMs), that includes $\mathcal{L}^p$, Wasserstein, Kolmogorov-Smirnov, and
other common distances between probability distributions. Specifically, under a
range of smoothness assumptions on the population and outlier distributions, we
show that a re-scaled thresholding wavelet series estimator achieves minimax
optimal convergence rates under a wide variety of losses. Finally, based on
connections that have recently been shown between nonparametric density
estimation under IPM losses and generative adversarial networks (GANs), we show
that certain GAN architectures also achieve these minimax rates.
"
"stat.TH","  We construct a zig-zag process targeting posterior distributions arising in
genetics from the Kingman coalescent and several popular models of mutation. We
show that the zig-zag process can lead to efficiency gains of up to several
orders of magnitude over classical Metropolis-Hastings, and argue that it is
also well suited to parallel computation for coalescent models. Our
construction is based on embedding discrete variables into continuous space; a
technique previously exploited in the construction of Hamiltonian Monte Carlo
algorithms, where it can lead to implementationally and analytically complex
boundary crossings. We demonstrate that the continuous-time zig-zag process can
largely avoid these complications.
"
"stat.TH","  This paper studies the universal approximation property of deep neural
networks for representing probability distributions. Given a target
distribution $\pi$ and a source distribution $p_z$ both defined on
$\mathbb{R}^d$, we prove under some assumptions that there exists a deep neural
network $g:\mathbb{R}^d\rightarrow \mathbb{R}$ with ReLU activation such that
the push-forward measure $(\nabla g)_\# p_z$ of $p_z$ under the map $\nabla g$
is arbitrarily close to the target measure $\pi$. The closeness are measured by
three classes of integral probability metrics between probability
distributions: $1$-Wasserstein distance, maximum mean distance (MMD) and
kernelized Stein discrepancy (KSD). We prove upper bounds for the size (width
and depth) of the deep neural network in terms of the dimension $d$ and the
approximation error $\varepsilon$ with respect to the three discrepancies. In
particular, the size of neural network can grow exponentially in $d$ when
$1$-Wasserstein distance is used as the discrepancy, whereas for both MMD and
KSD the size of neural network only depends on $d$ at most polynomially. Our
proof relies on convergence estimates of empirical measures under
aforementioned discrepancies and semi-discrete optimal transport.
"
"stat.TH","  Exponential families comprise a broad class of statistical models and
parametric families like normal distributions, binomial distributions, gamma
distributions or exponential distributions. Thereby the formal representation
of its probability distributions induces a confined intrinsic structure, which
appears to be that of a dually flat statistical manifold. Conversely it can be
shown, that any dually flat statistical manifold, which is given by a regular
Bregman divergence uniquely induced a regular exponential family, such that
exponential families may - with some restrictions - be regarded as a universal
representation of dually flat statistical manifolds. This article reviews the
pioneering work of Shun'ichi Amari about the intrinsic structure of exponential
families in terms of structural stratistics.
"
"stat.TH","  We study the properties of a leave-node-out jackknife procedure for network
data. Under the sparse graphon model, we prove an Efron-Stein-type inequality,
showing that the network jackknife leads to conservative estimates of the
variance (in expectation) for any network functional that is invariant to node
permutation. For a general class of count functionals, we also establish
consistency of the network jackknife. We complement our theoretical analysis
with a range of simulated and real-data examples and show that the network
jackknife offers competitive performance in cases where other resampling
methods are known to be valid. In fact, for several network statistics, we see
that the jackknife provides more accurate inferences compared to related
methods such as subsampling.
"
"stat.TH","  We provide a necessary and sufficient condition for the uniqueness of
penalized least-squares estimators whose penalty term is given by a norm with a
polytope unit ball, covering a wide range of methods including SLOPE and LASSO,
as well as the related method of basis pursuit. We consider a strong type of
uniqueness that is relevant for statistical problems. The uniqueness condition
is geometric and involves how the row span of the design matrix intersects the
faces of the dual norm unit ball, which for SLOPE is given by the sign
permutahedron. Further considerations based this condition also allow to derive
results on sparsity and clustering features. In particular, we define the
notion of a SLOPE model to describe both sparsity and clustering properties of
this method and also provide a geometric characterization of accessible SLOPE
models.
"
"stat.TH","  Adjusting for covariates is a well established method to estimate the total
causal effect of an exposure variable on an outcome of interest. Depending on
the causal structure of the mechanism under study there may be different
adjustment sets, equally valid from a theoretical perspective, leading to
identical causal effects. However, in practice, with finite data, estimators
built on different sets may display different precision. To investigate the
extent of this variability we consider the simplest non-trivial non-linear
model of a v-structure on three nodes for binary data. We explicitly compute
and compare the variance of the two possible different causal estimators.
Further, by going beyond leading order asymptotics we show that there are
parameter regimes where the set with the asymptotically optimal variance does
depend on the edge coefficients, a result which is not captured by the recent
leading order developments for general causal models.
"
"stat.TH","  We consider the joint sparse estimation of regression coefficients and the
covariance matrix for covariates in a high-dimensional regression model, where
the predictors are both relevant to a response variable of interest and
functionally related to one another via a Gaussian directed acyclic graph (DAG)
model. Gaussian DAG models introduce sparsity in the Cholesky factor of the
inverse covariance matrix, and the sparsity pattern in turn corresponds to
specific conditional independence assumptions on the underlying predictors. A
variety of methods have been developed in recent years for Bayesian inference
in identifying such network-structured predictors in regression setting, yet
crucial sparsity selection properties for these models have not been thoroughly
investigated. In this paper, we consider a hierarchical model with spike and
slab priors on the regression coefficients and a flexible and general class of
DAG-Wishart distributions with multiple shape parameters on the Cholesky
factors of the inverse covariance matrix. Under mild regularity assumptions, we
establish the joint selection consistency for both the variable and the
underlying DAG of the covariates when the dimension of predictors is allowed to
grow much larger than the sample size. We demonstrate that our method
outperforms existing methods in selecting network-structured predictors in
several simulation settings.
"
"stat.TH","  Given one sample $X \in \{\pm 1\}^n$ from an Ising model $\Pr[X=x]\propto
\exp(x^\top J x/2)$, whose interaction matrix satisfies $J:= \sum_{i=1}^k
\beta_i J_i$ for some known matrices $J_i$ and some unknown parameters
$\beta_i$, we study whether $J$ can be estimated to high accuracy. Assuming
that each node of the Ising model has bounded total interaction with the other
nodes, i.e. $\|J\|_{\infty} \le O(1)$, we provide a computationally efficient
estimator $\hat{J}$ with the high probability guarantee $\|\hat{J} -J\|_F \le
\widetilde O(\sqrt{k})$, where $\|J\|_F$ can be as high as $\Omega(\sqrt{n})$.
Our guarantee is tight when the interaction strengths are sufficiently low. An
example application of our result is in social networks, wherein nodes make
binary choices, $x_1,\ldots,x_n$, which may be influenced at varying strengths
$\beta_i$ by different networks $J_i$ in which these nodes belong. By observing
a single snapshot of the nodes' behaviors the goal is to learn the combined
correlation structure.
  When $k=1$ and a single parameter is to be inferred, we further show
$|\hat{\beta}_1 - \beta_1| \le \widetilde O(F(\beta_1J_1)^{-1/2})$, where
$F(\beta_1J_1)$ is the log-partition function of the model. This was proved in
prior work under additional assumptions. We generalize these results to any
setting.
  While our guarantees aim both high and low temperature regimes, our proof
relies on sparsifying the correlation network by conditioning on subsets of the
variables, such that the unconditioned variables satisfy Dobrushin's condition,
i.e. a high temperature condition which allows us to apply stronger
concentration inequalities. We use this to prove concentration and
anti-concentration properties of the Ising model, and we believe this
sparsification result has applications beyond the scope of this paper as well.
"
"stat.TH","  For a regression problem with a binary label response, we examine the problem
of constructing confidence intervals for the label probability conditional on
the features. In a setting where we do not have any information about the
underlying distribution, we would ideally like to provide confidence intervals
that are distribution-free---that is, valid with no assumptions on the
distribution of the data. Our results establish an explicit lower bound on the
length of any distribution-free confidence interval, and construct a procedure
that can approximately achieve this length. In particular, this lower bound is
independent of the sample size and holds for all distributions with no point
masses, meaning that it is not possible for any distribution-free procedure to
be adaptive with respect to any type of special structure in the distribution.
"
"stat.TH","  How many statistical inference tools we have for inference from massive data?
A huge number, but only when we are ready to assume the given database is
homogenous, consisting of a large cohort of ""similar"" cases. Why we need the
homogeneity assumption? To make `learning from the experience of others' or
`borrowing strength' possible. But, what if, we are dealing with a massive
database of heterogeneous cases (which is a norm in almost all modern
data-science applications including neuroscience, genomics, healthcare, and
astronomy)? How many methods we have in this situation? Not much, if not ZERO.
Why? It's not obvious how to go about gathering strength when each piece of
information is fuzzy. The danger is that, if we include irrelevant cases,
borrowing information might heavily damage the quality of the inference! This
raises some fundamental questions for big data inference: When (not) to borrow?
Whom (not) to borrow? How (not) to borrow? These questions are at the heart of
the ""Problem of Relevance"" in statistical inference -- a puzzle that has
remained too little addressed since its inception nearly half a century ago.
  Here we offer the first practical theory of relevance with precisely
describable statistical formulation and algorithm. Through examples, we
demonstrate how our new statistical perspective answers previously unanswerable
questions in a realistic and feasible way.
"
"stat.TH","  Assessing sampling uncertainty in extremum estimation can be challenging when
the asymptotic variance is not analytically tractable. Bootstrap inference
offers a feasible solution but can be computationally costly especially when
the model is complex. This paper uses iterates of a specially designed
stochastic optimization algorithm as draws from which both point estimates and
bootstrap standard errors can be computed in a single run. The draws are
generated by the gradient and Hessian computed from batches of data that are
resampled at each iteration. We show that these draws yield consistent
estimates and asymptotically valid frequentist inference for a large class of
regular problems. The algorithm provides accurate standard errors in simulation
examples and empirical applications at low computational costs. The draws from
the algorithm also provide a convenient way to detect data irregularities.
"
"stat.TH","  Statistical network analysis primarily focuses on inferring the parameters of
an observed network. In many applications, especially in the social sciences,
the observed data is the groups formed by individual subjects. In these
applications, the network is itself a parameter of a statistical model. Zhao
and Weko (2019) propose a model-based approach, called the hub model, to infer
implicit networks from grouping behavior. The hub model assumes that each
member of the group is brought together by a member of the group called the
hub. The hub model belongs to the family of Bernoulli mixture models.
Identifiability of parameters is a notoriously difficult problem for Bernoulli
mixture models. This paper proves identifiability of the hub model parameters
and estimation consistency under mild conditions. Furthermore, this paper
generalizes the hub model by introducing a model component that allows hubless
groups in which individual nodes spontaneously appear independent of any other
individual. We refer to this additional component as the null component. The
new model bridges the gap between the hub model and the degenerate case of the
mixture model -- the Bernoulli product. Identifiability and consistency are
also proved for the new model. Numerical studies are provided to demonstrate
the theoretical results.
"
"stat.TH","  The problem of quickly diagnosing an unknown change in a stochastic process
is studied. We establish novel bounds on the performance of misspecified
diagnosis algorithms designed for changes that differ from those of the
process, and pose and solve a new robust quickest change diagnosis problem in
the asymptotic regime of few false alarms and false isolations. Simulations
suggest that our asymptotically robust solution offers a computationally
efficient alternative to generalised likelihood ratio algorithms.
"
"stat.TH","  In this paper, the exact distribution of the largest eigenvalue of a singular
random matrix for Roy's test is discussed. The key to developing the
distribution theory of eigenvalues of a singular random matrix is to use
heterogeneous hypergeometric functions with two matrix arguments. In this
study, we define the singular beta F-matrix and extend the distributions of a
nonsingular beta F-matrix to the singular case. We also give the joint density
function of eigenvalues and the exact distribution of the largest eigenvalue in
terms of heterogeneous hypergeometric functions.
"
"stat.TH","  In this work, we consider a nonparametric regression model with one-sided
errors, multivariate covariates and regression function in a general H\""older
class. We work under the assumption of regularly varying independent and
identically distributed errors that are also independent of the design points.
Following Drees, Neumeyer and Selk (2019), we estimate the regression function,
i.e, the upper boundary curve, via minimization of the local integral of a
polynomial approximation lying above the data points. The main purpose of this
paper is to show the uniform consistency and to provide the rates of
convergence of such estimators for both multivariate random covariates and
multivariate deterministic design points. To demonstrate the performance of the
estimators, the small sample behavior is investigated in a simulation study in
dimension two and three.
"
"stat.TH","  The main objective of this paper consists in creating a new class of copulae
from various joint distributions occurring in connection with certain Brownian
motion processes. We focus our attention on the distributions of univariate
Brownian motions having a drift parameter and their maxima and on correlated
bivariate Brownian motions by considering the maximum value of one of them. The
copulae generated therefrom and their associated density functions are
explicitly given as well as graphically represented.
"
"stat.TH","  We extend the theoretical results for any FOU(p) processes for the case in
which the Hurst parameter is less than 1/2 and we show theoretically and by
simulations that under some conditions on T and the sample size n it is
possible to obtain consistent estimators of the parameters when the process is
observed in a discretized and equispaced interval [0, T ]. Also we will show
that the FOU(p) processes can be used to model a wide range of time series
varying from short range dependence to large range dependence with similar
results as the ARMA or ARFIMA models, and in several cases outperforms those.
Lastly, we give a way to obtain explicit formulas for the auto-covariance
function for any FOU(p) and we present an application for FOU(2) and FOU(3).
"
"stat.TH","  We study the selection of covariate adjustment sets for estimating the value
of point exposure dynamic policies, also known as dynamic treatment regimes,
assuming a non-parametric causal graphical model with hidden variables, in
which at least one adjustment set is fully observable. We show that recently
developed criteria, for graphs without hidden variables, to compare the
asymptotic variance of non-parametric estimators of static policy values that
control for certain adjustment sets, are also valid under dynamic policies and
graphs with hidden variables. We show that there exist adjustment sets that are
optimal minimal (minimum), in the sense of yielding estimators with the
smallest variance among those that control for adjustment sets that are minimal
(of minimum cardinality). Moreover, we show that if either no variables are
hidden or if all the observable variables are ancestors of either treatment,
outcome, or the variables that are used to decide treatment, a globally optimal
adjustment set exists. We provide polynomial time algorithms to compute the
globally optimal (when it exists), optimal minimal, and optimal minimum
adjustment sets. Our results are based on the construction of an undirected
graph in which vertex cuts between the treatment and outcome variables
correspond to adjustment sets. In this undirected graph, a partial order
between minimal vertex cuts can be defined that makes the set of minimal cuts a
lattice. This partial order corresponds directly to the ordering of the
asymptotic variances of the corresponding non-parametrically adjusted
estimators.
"
"stat.TH","  PAC-Bayesian algorithms and Gibbs posteriors are gaining popularity due to
their robustness against model misspecification even when Bayesian inference is
inconsistent. The PAC-Bayesian alpha-posterior is a generalization of the
standard Bayes posterior which can be tempered with a parameter alpha to handle
inconsistency. Data driven methods for tuning alpha have been proposed but are
still few, and are often computationally heavy. Additionally, the adequacy of
these methods in cases where we use variational approximations instead of exact
alpha-posteriors is not clear. This narrows their usage to simple models and
prevents their application to large-scale problems. We hence need fast methods
to tune alpha that work with both exact and variational alpha-posteriors.
First, we propose two data driven methods for tuning alpha, based on
sample-splitting and bootstrapping respectively. Second, we formulate the
(exact or variational) posteriors of three popular statistical models, and
modify them into alpha-posteriors. For each model, we test our strategies and
compare them with standard Bayes and Grunwald's SafeBayes. While bootstrapping
achieves mixed results, sample-splitting and SafeBayes perform well on the
exact and variational alpha-posteriors we describe, and achieve better results
than standard Bayes in misspecified or complex models. Additionally,
sample-splitting outperforms SafeBayes in terms of speed. Sample-splitting
offers a fast and easy solution to inconsistency and typically performs
similarly or better than Bayesian inference. Our results provide hints on the
calibration of alpha in PAC-Bayesian and Gibbs posteriors, and may facilitate
using these methods in large and complex models.
"
"stat.TH","  The notion of memory capacity, originally introduced for echo state and
linear networks with independent inputs, is generalized to nonlinear recurrent
networks with stationary but dependent inputs. The presence of dependence in
the inputs makes natural the introduction of the network forecasting capacity,
that measures the possibility of forecasting time series values using network
states. Generic bounds for memory and forecasting capacities are formulated in
terms of the number of neurons of the nonlinear recurrent network and the
autocovariance function or the spectral density of the input. These bounds
generalize well-known estimates in the literature to a dependent inputs setup.
Finally, for the particular case of linear recurrent networks with independent
inputs it is proved that the memory capacity is given by the rank of the
associated controllability matrix, a fact that has been for a long time assumed
to be true without proof by the community.
"
"stat.TH","  Quantum channel estimation and discrimination are fundamentally related
information processing tasks of interest in quantum information science. In
this paper, we analyze these tasks by employing the right logarithmic
derivative Fisher information and the geometric R\'enyi relative entropy,
respectively, and we also identify connections between these distinguishability
measures. A key result of our paper is that a chain-rule property holds for the
right logarithmic derivative Fisher information and the geometric R\'enyi
relative entropy for the interval $\alpha\in(0,1) $ of the R\'enyi parameter
$\alpha$. In channel estimation, these results imply a condition for the
unattainability of Heisenberg scaling, while in channel discrimination, they
lead to improved bounds on error rates in the Chernoff and Hoeffding error
exponent settings. More generally, we introduce the amortized quantum Fisher
information as a conceptual framework for analyzing general sequential
protocols that estimate a parameter encoded in a quantum channel, and we use
this framework, beyond the aforementioned application, to show that Heisenberg
scaling is not possible when a parameter is encoded in a classical-quantum
channel. We then identify a number of other conceptual and technical
connections between the tasks of estimation and discrimination and the
distinguishability measures involved in analyzing each. As part of this work,
we present a detailed overview of the geometric R\'enyi relative entropy of
quantum states and channels, as well as its properties, which may be of
independent interest.
"
"stat.TH","  Zero inflation is a common nuisance while monitoring disease progression over
time. This article proposes a new observation driven model for zero inflated
and over-dispersed count time series. The counts given the past history of the
process and available information on covariates is assumed to be distributed as
a mixture of a Poisson distribution and a distribution degenerate at zero, with
a time dependent mixing probability, $\pi_t$. Since, count data usually suffers
from overdispersion, a Gamma distribution is used to model the excess
variation, resulting in a zero inflated Negative Binomial (NB) regression model
with mean parameter $\lambda_t$. Linear predictors with auto regressive and
moving average (ARMA) type terms, covariates, seasonality and trend are fitted
to $\lambda_t$ and $\pi_t$ through canonical link generalized linear models.
Estimation is done using maximum likelihood aided by iterative algorithms, such
as Newton Raphson (NR) and Expectation and Maximization (EM). Theoretical
results on the consistency and asymptotic normality of the estimators are
given. %In-depth simulation studies provide an understanding about the
properties of the estimators. The proposed model is illustrated using in-depth
simulation studies and a dengue data set.
"
"stat.TH","  We propose inference procedures for general nonparametric factorial survival
designs with possibly right-censored data. Similar to additive Aalen models,
null hypotheses are formulated in terms of cumulative hazards. Thereby,
deviations are measured in terms of quadratic forms in Nelson-Aalen-type
integrals. Different to existing approaches this allows to work without
restrictive model assumptions as proportional hazards. In particular, crossing
survival or hazard curves can be detected without a significant loss of power.
For a distribution-free application of the method, a permutation strategy is
suggested. The resulting procedures' asymptotic validity as well as their
consistency are proven and their small sample performances are analyzed in
extensive simulations. Their applicability is finally illustrated by analyzing
an oncology data set.
"
"stat.TH","  This paper provides general expression for Bartlett and Bartlett-type
correction factors for the likelihood ratio and gradient statistics to test the
dispersion parameter in heteroscedastic symmetric nonlinear models. This class
of regression models is potentially useful for modeling data containing
outlying observations. We consider a partition on the dispersion parameter
vector in order to test the parameters of interest. Furthermore, we develop
Monte Carlo simulations to compare the finite sample performances of the
corrected tests proposed with the usual and modified score tests, likelihood
and gradient tests, the Bartlett-type corrected score test and bootstrap
corrected tests. Our simulation results favor the score and gradient corrected
tests as well as the bootstrap tests. An empirical application is presented for
illustrative purposes.
"
"stat.TH","  In the Gaussian sequence model $Y= \theta_0 + \varepsilon$ in $\mathbb{R}^n$,
we study the fundamental limit of approximating the signal $\theta_0$ by a
class $\Theta(d,d_0,k)$ of (generalized) splines with free knots. Here $d$ is
the degree of the spline, $d_0$ is the order of differentiability at each inner
knot, and $k$ is the maximal number of pieces. We show that, given any integer
$d\geq 0$ and $d_0\in\{-1,0,\ldots,d-1\}$, the minimax rate of estimation over
$\Theta(d,d_0,k)$ exhibits the following phase transition: \begin{equation*}
\begin{aligned} \inf_{\widetilde{\theta}}\sup_{\theta\in\Theta(d,d_0,
k)}\mathbb{E}_\theta\|\widetilde{\theta} - \theta\|^2 \asymp_d \begin{cases}
k\log\log(16n/k), & 2\leq k\leq k_0,\\ k\log(en/k), & k \geq k_0+1. \end{cases}
\end{aligned} \end{equation*} The transition boundary $k_0$, which takes the
form $\lfloor{(d+1)/(d-d_0)\rfloor} + 1$, demonstrates the critical role of the
regularity parameter $d_0$ in the separation between a faster $\log \log(16n)$
and a slower $\log(en)$ rate. We further show that, once encouraging an
additional '$d$-monotonicity' shape constraint (including monotonicity for $d =
0$ and convexity for $d=1$), the above phase transition is eliminated and the
faster $k\log\log(16n/k)$ rate can be achieved for all $k$. These results
provide theoretical support for developing $\ell_0$-penalized
(shape-constrained) spline regression procedures as useful alternatives to
$\ell_1$- and $\ell_2$-penalized ones.
"
"stat.TH","  Gaussian processes provide a framework for nonlinear nonparametric Bayesian
inference widely applicable across science and engineering. Unfortunately,
their computational burden scales cubically with the training sample size,
which in the case that samples arrive in perpetuity, approaches infinity. This
issue necessitates approximations for use with streaming data, which to date
mostly lack convergence guarantees. Thus, we develop the first online Gaussian
process approximation that preserves convergence to the population posterior,
i.e., asymptotic posterior consistency, while ameliorating its intractable
complexity growth with the sample size. We propose an online compression scheme
that, following each a posteriori update, fixes an error neighborhood with
respect to the Hellinger metric centered at the current posterior, and greedily
tosses out past kernel dictionary elements until its boundary is hit. We call
the resulting method Parsimonious Online Gaussian Processes (POG). For
diminishing error radius, exact asymptotic consistency is preserved (Theorem
1(i)) at the cost of unbounded memory in the limit. On the other hand, for
constant error radius, POG converges to a neighborhood of the population
posterior (Theorem 1(ii))but with finite memory at-worst determined by the
metric entropy of the feature space (Theorem 2). Experimental results are
presented on several nonlinear regression problems which illuminates the merits
of this approach as compared with alternatives that fix the subspace dimension
defining the history of past points.
"
"stat.TH","  In this paper, we present a new Marshall-Olkin exponential shock model. The
new construction method gives the proposed model further ability to allocate
the common joint shock on each of the components, making it suitable for
application in fields like reliability and credit risk. The given model has a
singular part and supports both positive and negative dependence structure.
Main dependence properties of the model is given and an analysis of
stress-strength is presented. After a performance analysis on the estimator of
parameters, a real data is studied. Finally, we give the multivariate version
of the proposed model and its main properties.
"
"stat.TH","  We develop large sample theory including nonparametric confidence regions for
$r$-dimensional ridges of probability density functions on $\mathbb{R}^d$,
where $1\leq r<d$. We view ridges as the intersections of level sets of some
special functions. The vertical variation of the plug-in kernel estimators for
these functions constrained on the ridges is used as the measure of maximal
deviation for ridge estimation. Our confidence regions for the ridges are
determined by the asymptotic distribution of this maximal deviation, which is
established by utilizing the extreme value distribution of nonstationary
$\chi$-fields indexed by manifolds.
"
"stat.TH","  After performing a randomized experiment, researchers often use
ordinary-least squares (OLS) regression to adjust for baseline covariates when
estimating the average treatment effect. It is widely known that the resulting
confidence interval is valid even if the linear model is misspecified. In this
paper, we generalize that conclusion to covariate adjustment with nonlinear
models. We introduce an intuitive way to use any ""simple"" nonlinear model to
construct a covariate-adjusted confidence interval for the average treatment
effect. The confidence interval derives its validity from randomization alone,
and when nonlinear models fit the data better than linear models, it is
narrower than the usual interval from OLS adjustment.
"
"stat.TH","  We revisit Zadeh's notion of ""evidence of the second kind"" and show that it
provides the foundation for a general theory of epistemic random fuzzy sets,
which generalizes both the Dempster-Shafer theory of belief functions and
possibility theory. In this perspective, Dempster-Shafer theory deals with
belief functions generated by random sets, while possibility theory deals with
belief functions induced by fuzzy sets. The more general theory allows us to
represent and combine evidence that is both uncertain and fuzzy. We demonstrate
the application of this formalism to statistical inference, and show that it
makes it possible to reconcile the possibilistic interpretation of likelihood
with Bayesian inference.
"
"stat.TH","  Median-of-means (MOM) based procedures provide non-asymptotic and strong
deviation bounds even when data are heavy-tailed and/or corrupted. This work
proposes a new general way to bound the excess risk for MOM estimators. The
core technique is the use of VC-dimension (instead of Rademacher complexity) to
measure the statistical complexity. In particular, this allows to give the
first robust estimators for sparse estimation which achieves the so-called
subgaussian rate only assuming a finite second moment for the uncorrupted data.
By comparison, previous works using Rademacher complexities required a number
of finite moments that grows logarithmically with the dimension. With this
technique, we derive new robust sugaussian bounds for mean estimation in any
norm. We also derive a new robust estimator for covariance estimation that is
the first to achieve subgaussian bounds without $L_4-L_2$ norm equivalence.
"
"stat.TH","  The energy distance and energy scores became important tools in multivariate
statistics and multivariate probabilistic forecasting in recent years. They are
both based on the expected distance of two independent samples. In this paper
we study dependence uncertainty bounds for these quantities under the
assumption that we know the marginals but do not know the dependence structure.
We find some interesting sharp analytic bounds, where one of them is obtained
for an unusual spherically symmetric copula. These results should help to
better understand the sensitivity of these measures to misspecifications in the
copula.
"
"stat.TH","  Symmetry is key in classical and modern physics. A striking example is
conservation of energy as a consequence of time-shift invariance from Noether's
theorem. Symmetry is likewise a key element in statistics, which, as also
physics, provide models for real world phenomena. Sufficiency, conditionality,
and invariance are examples of basic principles. Galili and Meilijson (2016)
and Mandel (2020) illustrate the first two principles very nicely by
considering the scaled uniform model. We illustrate the third principle by
providing further results which give optimal inference for the scaled uniform
by symmetry considerations. The proofs are simplified by relying on fiducial
arguments as initiated by Fisher (1930).
  Keywords: Data generating equation; Optimal equivariant estimate; Scale
family; Conditionality principle; Minimal sufficient; Uniform distribution;
"
"stat.TH","  We prove bounds on the population risk of the maximum margin algorithm for
two-class linear classification. For linearly separable training data, the
maximum margin algorithm has been shown in previous work to be equivalent to a
limit of training with logistic loss using gradient descent, as the training
error is driven to zero. We analyze this algorithm applied to random data
including misclassification noise. Our assumptions on the clean data include
the case in which the class-conditional distributions are standard normal
distributions. The misclassification noise may be chosen by an adversary,
subject to a limit on the fraction of corrupted labels. Our bounds show that,
with sufficient over-parameterization, the maximum margin algorithm trained on
noisy data can achieve nearly optimal population risk.
"
"stat.TH","  Classification algorithms face difficulties when one or more classes have
limited training data. We are particularly interested in classification trees,
due to their interpretability and flexibility. When data are limited in one or
more of the classes, the estimated decision boundaries are often irregularly
shaped due to the limited sample size, leading to poor generalization error. We
propose a novel approach that penalizes the Surface-to-Volume Ratio (SVR) of
the decision set, obtaining a new class of SVR-Tree algorithms. We develop a
simple and computationally efficient implementation while proving estimation
and feature selection consistency for SVR-Tree. SVR-Tree is compared with
multiple algorithms that are designed to deal with imbalance through real data
applications.
"
"stat.TH","  We study the problem of testing the equivalence of functional parameters
(such as the mean or variance function) in the two sample functional data
problem. In contrast to previous work, which reduces the functional problem to
a multiple testing problem for the equivalence of scalar data by comparing the
functions at each point, our approach is based on an estimate of a distance
measuring the maximum deviation between the two functional parameters.
Equivalence is claimed if the estimate for the maximum deviation does not
exceed a given threshold. A bootstrap procedure is proposed to obtain quantiles
for the distribution of the test statistic and consistency of the corresponding
test is proved in the large sample scenario. As the methods proposed here avoid
the use of the intersection-union principle they are less conservative and more
powerful than the currently available methodology.
"
"stat.TH","  In this paper we propose a bimodal gamma distribution using a quadratic
transformation based on the alpha-skew-normal model. We discuss several
properties of this distribution such as mean, variance, moments, hazard rate
and entropy measures. Further, we propose a new regression model with censored
data based on the bimodal gamma distribution. This regression model can be very
useful to the analysis of real data and could give more realistic fits than
other special regression models. Monte Carlo simulations were performed to
check the bias in the maximum likelihood estimation. The proposed models are
applied to two real data sets found in literature.
"
"stat.TH","  We develop a mathematical and interpretative foundation for the enterprise of
decision-theoretic statistical causality (DT), which is a straightforward way
of representing and addressing causal questions. DT reframes causal inference
as ""assisted decision-making"", and aims to understand when, and how, I can make
use of external data, typically observational, to help me solve a decision
problem by taking advantage of assumed relationships between the data and my
problem.
  The relationships embodied in any representation of a causal problem require
deeper justification, which is necessarily context-dependent. Here we clarify
the considerations needed to support applications of the DT methodology.
Exchangeability considerations are used to structure the required
relationships, and a distinction drawn between intention to treat and
intervention to treat forms the basis for the enabling condition of
""ignorability"". We also show how the DT perspective unifies and sheds light on
other popular formalisations of statistical causality, including potential
responses and directed acyclic graphs.
"
"stat.TH","  We study the problems of learning and testing junta distributions on
$\{-1,1\}^n$ with respect to the uniform distribution, where a distribution $p$
is a $k$-junta if its probability mass function $p(x)$ depends on a subset of
at most $k$ variables. The main contribution is an algorithm for finding
relevant coordinates in a $k$-junta distribution with subcube conditioning
[BC18, CCKLW20]. We give two applications:
  1. An algorithm for learning $k$-junta distributions with
$\tilde{O}(k/\epsilon^2) \log n + O(2^k/\epsilon^2)$ subcube conditioning
queries, and
  2. An algorithm for testing $k$-junta distributions with $\tilde{O}((k +
\sqrt{n})/\epsilon^2)$ subcube conditioning queries.
  All our algorithms are optimal up to poly-logarithmic factors.
  Our results show that subcube conditioning, as a natural model for accessing
high-dimensional distributions, enables significant savings in learning and
testing junta distributions compared to the standard sampling model. This
addresses an open question posed by Aliakbarpour, Blais, and Rubinfeld [ABR17].
"
"stat.TH","  We show that the full-sample bootstrap is asymptotically valid for
constructing confidence intervals for high-quantiles, tail probabilities, and
other tail parameters of a univariate distribution. This resolves the doubts
that have been raised about the validity of such bootstrap methods. In our
extensive simulation study, the overall performance of the bootstrap method was
better than that of the standard asymptotic method, indicating that the
bootstrap method is at least as good, if not better than, the asymptotic method
for inference. This paper also lays the foundation for developing bootstrap
methods for inference about tail events in multivariate statistics; this is
particularly important because some of the non-bootstrap methods are complex.
"
"stat.TH","  We present a novel approach to estimating discrete distributions with
(potentially) infinite support in the total variation metric. In a departure
from the established paradigm, we make no structural assumptions whatsoever on
the sampling distribution. In such a setting, distribution-free risk bounds are
impossible, and the best one could hope for is a fully empirical data-dependent
bound. We derive precisely such bounds, and demonstrate that these are, in a
well-defined sense, the best possible. Our main discovery is that the half-norm
of the empirical distribution provides tight upper and lower estimates on the
empirical risk. Furthermore, this quantity decays at a nearly optimal rate as a
function of the true distribution. The optimality follows from a minimax
result, of possible independent interest. Additional structural results are
provided, including an exact Rademacher complexity calculation and apparently a
first connection between the total variation risk and the missing mass.
"
"stat.TH","  In a circular convolution model, we aim to infer on the density of a circular
random variable using observations contaminated by an additive measurement
error. We highlight the interplay of the two problems: optimal testing and
quadratic functional estimation. Under general regularity assumptions, we
determine an upper bound for the minimax risk of estimation for the quadratic
functional. The upper bound consists of two terms, one that mimics a classical
bias-variance trade-off and a second that causes the typical elbow effect in
quadratic functional estimation. Using a minimax optimal estimator of the
quadratic functional as a test statistic, we derive an upper bound for the
nonasymptotic minimax radius of testing for nonparametric alternatives.
Interestingly, the term causing the elbow effect in the estimation case
vanishes in the radius of testing. We provide a matching lower bound for the
testing problem. By showing that any lower bound for the testing problem also
yields a lower bound for the quadratic functional estimation problem, we obtain
a lower bound for the risk of estimation. Lastly, we prove a matching lower
bound for the term causing the elbow effect in the estimation problem. The
results are illustrated considering Sobolev spaces and ordinary or super smooth
error densities.
"
"stat.TH","  The classical regular and partial autocorrelation functions are powerful
tools for stationary time series modelling and analysis. However, it is
increasingly recognized that many time series are not stationary and the use of
classical global autocorrelations can give misleading answers. This article
introduces two estimators of the local partial autocorrelation function and
establishes their asymptotic properties. The article then illustrates the use
of these new estimators on both simulated and real time series. The examples
clearly demonstrate the strong practical benefits of local estimators for time
series that exhibit nonstationarities.
"
"stat.TH","  Let denote $S_n(p) = k_n^{-1} \sum_{i=1}^{k_n} \left( \log (X_{n+1-i,n} /
X_{n-k_n, n}) \right)^p$, where $p > 0$, $k_n \leq n$ is a sequence of integers
such that $k_n \to \infty$ and $k_n / n \to 0$, and $X_{1,n} \leq \ldots \leq
X_{n,n}$ is the order statistics of iid random variables with regularly varying
upper tail. The estimator $\widehat \gamma(n) = (S_n(p)/\Gamma(p+1))^{1/p}$ is
an extension of the Hill estimator. We investigate the asymptotic properties of
$S_n(p)$ and $\widehat \gamma(n)$ both for fixed $p > 0$ and for $p = p_n \to
\infty$. We prove strong consistency and asymptotic normality under appropriate
assumptions. Applied to real data we find that for larger $p$ the estimator is
less sensitive to the change in $k_n$ than the Hill estimator.
"
"stat.TH","  In the recent information-theoretic literature, the concept of extropy has
been studied for order statistics. In the present communication we consider a
cumulative analogue of extropy in the same vein of cumulative residual (past)
entropy and study it in context with extreme order statistics. A dynamic
version of cumulative residual (past) extropy for smallest (largest) order
statistic is also studied here. It is shown that the proposed measures (and
their dynamic versions) of extreme order statistics determine the distribution
uniquely. Some characterizations of the generalized Pareto and power
distributions, which are commonly used in reliability modeling, are given.
"
"stat.TH","  We give a characterization of positive definite integrable functions on a
product of two Gelfand pairs as an integral of positive definite functions on
one of the Gelfand pairs with respect to the Plancherel measure on the dual of
the other Gelfand pair.
  In the very special case where the Gelfand pairs are Euclidean groups and the
compact subgroups are reduced to the identity, the characterization is a much
cited result in spatio-temporal statistics due to Cressie, Huang and Gneiting.
  When one of the Gelfand pairs is compact the characterization leads to
results about expansions in spherical functions with positive definite
expansion functions, thereby recovering recent results of the author in
collaboration with Peron and Porcu. In the special case when the compact
Gelfand pair consists of orthogonal groups, the characterization is important
in geostatistics and covers a recent result of Porcu and White.
"
"stat.TH","  A reflexive generalized inverse and the Moore-Penrose inverse are often
confused in statistical literature but in fact they have completely different
behaviour in case the population covariance matrix is not a multiple of
identity. In this paper, we study the spectral properties of a reflexive
generalized inverse and of the Moore-Penrose inverse of the sample covariance
matrix. The obtained results are used to assess the difference in the
asymptotic behaviour of their eigenvalues.
"
"stat.TH","  We construct new testing procedures for spherical and elliptical symmetry
based on the characterization that a random vector $X$ with finite mean has a
spherical distribution if and only if $\Ex[u^\top X | v^\top X] = 0$ holds for
any two perpendicular vectors $u$ and $v$. Our test is based on the
Kolmogorov-Smirnov statistic, and its rejection region is found via the
spherically symmetric bootstrap. We show the consistency of the spherically
symmetric bootstrap test using a general Donsker theorem which is of some
independent interest. For the case of testing for elliptical symmetry, the
Kolmogorov-Smirnov statistic has an asymptotic drift term due to the estimated
location and scale parameters. Therefore, an additional standardization is
required in the bootstrap procedure. In a simulation study, the size and the
power properties of our tests are assessed for several distributions and the
performance is compared to that of several competing procedures.
"
"stat.TH","  We introduce a new model of correlated randomly growing graphs and study the
fundamental questions of detecting correlation and estimating aspects of the
correlated structure. The model is simple and starts with any model of randomly
growing graphs, such as uniform attachment (UA) or preferential attachment
(PA). Given such a model, a pair of graphs $(G_1, G_2)$ is grown in two stages:
until time $t_{\star}$ they are grown together (i.e., $G_1 = G_2$), after which
they grow independently according to the underlying growth model.
  We show that whenever the seed graph has an influence in the underlying graph
growth model---this has been shown for PA and UA trees and is conjectured to
hold broadly---then correlation can be detected in this model, even if the
graphs are grown together for just a single time step. We also give a general
sufficient condition (which holds for PA and UA trees) under which detection is
possible with probability going to $1$ as $t_{\star} \to \infty$. Finally, we
show for PA and UA trees that the amount of correlation, measured by
$t_{\star}$, can be estimated with vanishing relative error as $t_{\star} \to
\infty$.
"
"stat.TH","  The Gaussian model equips strong properties that facilitate studying and
interpreting graphical models. Specifically it reduces conditional independence
and the study of positive association to determining partial correlations and
their signs. When Gaussianity does not hold partial correlation graphs are a
useful relaxation of graphical models, but it is not clear what information
they contain (besides the obvious lack of linear association). We study
elliptical and transelliptical distributions as middle-ground between the
Gaussian and other families that are more flexible but either do not embed
strong properties or do not lead to simple interpretation. We characterize the
meaning of zero partial correlations in the elliptical family and
transelliptical copula models and show that it retains much of the dependence
structure from the Gaussian case. Regarding positive dependence, we prove
impossibility results to learn (trans)elliptical graphical models, including
that an elliptical distribution that is multivariate totally positive of order
two for all dimensions must be essentially Gaussian. We then show how to
interpret positive partial correlations as a relaxation, and obtain important
properties related to faithfulness and Simpson's paradox. We illustrate the
transelliptical model potential to study tail dependence in S&P500 data, and of
positivity to improve regularized inference.
"
"stat.TH","  As a flexible nonparametric learning tool, random forest has been widely
applied to various real applications with appealing empirical performance, even
in the presence of high-dimensional feature space. Unveiling the underlying
mechanisms has led to some important recent theoretical results on consistency
under the classical setting of fixed dimensionality or for some modified
version of the random forest algorithm. Yet the consistency rates of the
original version of the random forest algorithm in a general high-dimensional
nonparametric regression setting remain largely unexplored. In this paper, we
fill such a gap and build a high-dimensional consistency theory for random
forest. Our new theoretical results show that random forest can indeed adapt to
high dimensions and also provide some insights into the role of sparsity from
the perspective of feature relevance.
"
"stat.TH","  This article focuses on the estimation and design aspects of a bivariate
collocated cokriging experiment. For a large class of covariance matrices a
linear dependency criterion is identified, which allows the best linear
unbiased estimator of the primary variable in a bivariate collocated cokriging
setup to reduce to a univariate kriging estimator. Exact optimal designs for
efficient prediction for such simple and ordinary cokriging models, with one
dimensional inputs are determined. Designs are found by minimizing the maximum
and integrated prediction variance. For simple and ordinary cokriging models
with known covariance parameters, the equispaced design is shown to be optimal
for both criterion functions. The more realistic scenario of unknown covariance
parameters is addressed by assuming prior distributions on the parameter
vector, thus adopting a Bayesian approach to the design problem. The equispaced
design is proved to be the Bayesian optimal design for both criteria. The work
is motivated by designing an optimal water monitoring system for an Indian
river.
"
"stat.TH","  We consider the adaptive test for the parameter change in discretely observed
ergodic diffusion processes based on the cusum test. Using two test statistics
based on the two quasi-log likelihood functions of the diffusion parameter and
the drift parameter, we perform the change point tests for both diffusion and
drift parameters of the diffusion process. It is shown that the test statistics
have the limiting distribution of the sup of the norm of a Brownian bridge.
Simulation results are illustrated for the 1-dimensional Ornstein-Uhlenbeck
process.
"
"stat.TH","  Vertex nomination is a lightly-supervised network information retrieval (IR)
task in which vertices of interest in one graph are used to query a second
graph to discover vertices of interest in the second graph. Similar to other IR
tasks, the output of a vertex nomination scheme is a ranked list of the
vertices in the second graph, with the heretofore unknown vertices of interest
ideally concentrating at the top of the list. Vertex nomination schemes provide
a useful suite of tools for efficiently mining complex networks for pertinent
information. In this paper, we explore, both theoretically and practically, the
dual roles of content (i.e., edge and vertex attributes) and context (i.e.,
network topology) in vertex nomination. We provide necessary and sufficient
conditions under which vertex nomination schemes that leverage both content and
context outperform schemes that leverage only content or context separately.
While the joint utility of both content and context has been demonstrated
empirically in the literature, the framework presented in this paper provides a
novel theoretical basis for understanding the potential complementary roles of
network features and topology.
"
"stat.TH","  The emergence of big data has led to so-called convergence complexity
analysis, which is the study of how Markov chain Monte Carlo (MCMC) algorithms
behave as the sample size, $n$, and/or the number of parameters, $p$, in the
underlying data set increase. This type of analysis is often quite challenging,
in part because existing results for fixed $n$ and $p$ are simply not sharp
enough to yield good asymptotic results. One of the first convergence
complexity results for an MCMC algorithm on a continuous state space is due to
Yang and Rosenthal (2019), who established a mixing time result for a Gibbs
sampler (for a simple Bayesian random effects model) that was introduced and
studied by Rosenthal (1996). The asymptotic behavior of the spectral gap of
this Gibbs sampler is, however, still unknown. We use a recently developed
simulation technique (Qin et. al., 2019) to provide substantial numerical
evidence that the gap is bounded away from 0 as $n \rightarrow \infty$. We also
establish a pair of rigorous convergence complexity results for two different
Gibbs samplers associated with a generalization of the random effects model
considered by Rosenthal (1996). Our results show that, under strong regularity
conditions, the spectral gaps of these Gibbs samplers converge to 1 as the
sample size increases.
"
"stat.TH","  Bayesian network (BN) structure learning from complete data has been
extensively studied in the literature. However, fewer theoretical results are
available for incomplete data, and most are based on the use of the
Expectation-Maximisation (EM) algorithm. Balov (2013) proposed an alternative
approach called Node-Average Likelihood (NAL) that is competitive with EM but
computationally more efficient; and proved its consistency and model
identifiability for discrete BNs.
  In this paper, we give general sufficient conditions for the consistency of
NAL; and we prove consistency and identifiability for conditional Gaussian BNs,
which include discrete and Gaussian BNs as special cases. Hence NAL has a wider
applicability than originally stated in Balov (2013).
"
"stat.TH","  We propose to compute a sparse approximate inverse Cholesky factor $L$ of a
dense covariance matrix $\Theta$ by minimizing the Kullback-Leibler divergence
between the Gaussian distributions $\mathcal{N}(0, \Theta)$ and $\mathcal{N}(0,
L^{-\top} L^{-1})$, subject to a sparsity constraint. Surprisingly, this
problem has a closed-form solution that can be computed efficiently, recovering
the popular Vecchia approximation in spatial statistics. Based on recent
results on the approximate sparsity of inverse Cholesky factors of $\Theta$
obtained from pairwise evaluation of Green's functions of elliptic
boundary-value problems at points $\{x_{i}\}_{1 \leq i \leq N} \subset
\mathbb{R}^{d}$, we propose an elimination ordering and sparsity pattern that
allows us to compute $\epsilon$-approximate inverse Cholesky factors of such
$\Theta$ in computational complexity $\mathcal{O}(N \log(N/\epsilon)^d)$ in
space and $\mathcal{O}(N \log(N/\epsilon)^{2d})$ in time. To the best of our
knowledge, this is the best asymptotic complexity for this class of problems.
Furthermore, our method is embarrassingly parallel, automatically exploits
low-dimensional structure in the data, and can perform Gaussian-process
regression in linear (in $N$) space complexity. Motivated by the optimality
properties of our methods, we propose methods for applying it to the joint
covariance of training and prediction points in Gaussian-process regression,
greatly improving stability and computational cost. Finally, we show how to
apply our method to the important setting of Gaussian processes with additive
noise, sacrificing neither accuracy nor computational complexity.
"
"stat.TH","  Heterogeneous effect estimation plays a crucial role in causal inference,
with applications across medicine and social science. Many methods for
estimating conditional average treatment effects (CATEs) have been proposed in
recent years, but there are important theoretical gaps in understanding if and
when such methods are optimal. This is especially true when the CATE has
nontrivial structure (e.g., smoothness or sparsity). Our work contributes in
several main ways. First, we study a two-stage doubly robust CATE estimator and
give a generic model-free error bound, which, despite its generality, yields
sharper results than those in the current literature. We apply the bound to
derive error rates in nonparametric models with smoothness or sparsity, and
give sufficient conditions for oracle efficiency. Underlying our error bound is
a general oracle inequality for regression with estimated or imputed outcomes,
which is of independent interest; this is the second main contribution. The
third contribution is aimed at understanding the fundamental statistical limits
of CATE estimation. To that end, we propose and study a local polynomial
adaptation of double-residual regression. We show that this estimator can be
oracle efficient under even weaker conditions, if used with a specialized form
of sample splitting and careful choices of tuning parameters. These are the
weakest conditions currently found in the literature, and we conjecture that
they are minimal in a minimax sense. We go on to give error bounds in the
non-trivial regime where oracle rates cannot be achieved. Some finite-sample
properties are explored with simulations.
"
"stat.TH","  In this paper, we introduce a wavelet-based method for estimating the EDR
space in Li's semiparametric regression model for achieving dimension
reduction. This method is obtained by using linear wavelet estimators of the
density and regression functions that are involved in the covariance matrix of
conditional expectation whose spectral analysis gives the EDR directions. Then,
consistency of the proposed estimators is proved. A simulation study that allow
one to evaluate the performance of the proposal with comparison to existing
methods is presented.
"
"stat.TH","  We study statistical estimators computed using iterative optimization methods
that are not run until completion. Classical results on maximum likelihood
estimators (MLEs) assert that a one-step estimator (OSE), in which a single
Newton-Raphson iteration is performed from a starting point with certain
properties, is asymptotically equivalent to the MLE. We further develop these
early-stopping results by deriving properties of one-step estimators defined by
a single iteration of scaled proximal methods. Our main results show the
asymptotic equivalence of the likelihood-based estimator and various one-step
estimators defined by scaled proximal methods. By interpreting OSEs as the last
of a sequence of iterates, our results provide insight on scaling numerical
tolerance with sample size. Our setting contains scaled proximal gradient
descent applied to certain composite models as a special case, making our
results applicable to many problems of practical interest. Additionally, we
provide support for the utility of the scaled Moreau envelope as a statistical
smoother by interpreting scaled proximal descent as a quasi-Newton method
applied to the scaled Moreau envelope.
"
"stat.TH","  We propose a generic network model, based on the Stochastic Block Model, to
study the hierarchy of communities in real-world networks, under which the
connection probabilities are structured in a binary tree. Under the network
model, we show that the eigenstructure of the expected unnormalized graph
Laplacian reveals the community structure of the network as well as the
hierarchy of communities in a recursive fashion. Inspired by the nice property
of the population eigenstructure, we develop a recursive bi-partitioning
algorithm that divides the network into two communities based on the Fiedler
vector of the unnormalized graph Laplacian and repeats the split until a
stopping rule indicates no further community structures. We prove the weak and
strong consistency of our algorithm for sparse networks with the expected node
degree in $O(\log n)$ order, based on newly developed theory on
$\ell_{2\rightarrow\infty}$ eigenspace perturbation, without knowing the total
number of communities in advance. Unlike most of existing work, our theory
covers multi-scale networks where the connection probabilities may differ in
order of magnitude, which comprise an important class of models that are
practically relevant but technically challenging to deal with. Finally we
demonstrate the performance of our algorithm on synthetic data and real-world
examples.
"
"stat.TH","  We introduce algorithms for learning nonlinear dynamical systems of the form
$x_{t+1}=\sigma(\Theta^{\star}x_t)+\varepsilon_t$, where $\Theta^{\star}$ is a
weight matrix, $\sigma$ is a nonlinear link function, and $\varepsilon_t$ is a
mean-zero noise process. We give an algorithm that recovers the weight matrix
$\Theta^{\star}$ from a single trajectory with optimal sample complexity and
linear running time. The algorithm succeeds under weaker statistical
assumptions than in previous work, and in particular i) does not require a
bound on the spectral norm of the weight matrix $\Theta^{\star}$ (rather, it
depends on a generalization of the spectral radius) and ii) enjoys guarantees
for non-strictly-increasing link functions such as the ReLU. Our analysis has
two key components: i) we give a general recipe whereby global stability for
nonlinear dynamical systems can be used to certify that the state-vector
covariance is well-conditioned, and ii) using these tools, we extend well-known
algorithms for efficiently learning generalized linear models to the dependent
setting.
"
"stat.TH","  This work contributes to the limited literature on estimating the diffusivity
or drift coefficient of nonlinear SPDEs driven by additive noise. Assuming that
the solution is measured locally in space and over a finite time interval, we
show that the augmented maximum likelihood estimator introduced in Altmeyer,
Reiss (2020) retains its asymptotic properties when used for semilinear SPDEs
that satisfy some abstract, and verifiable, conditions. The proofs of
asymptotic results are based on splitting the solution in linear and nonlinear
parts and fine regularity properties in $L^p$-spaces. The obtained general
results are applied to particular classes of equations, including stochastic
reaction-diffusion equations. The stochastic Burgers equation, as an example
with first order nonlinearity, is an interesting borderline case of the general
results, and is treated by a Wiener chaos expansion. We conclude with numerical
examples that validate the theoretical results.
"
"stat.TH","  In competing event settings, a counterfactual contrast of cause-specific
cumulative incidences quantifies the total causal effect of a treatment on the
event of interest. However, effects of treatment on the competing event may
indirectly contribute to this total effect, complicating its interpretation. We
previously proposed the separable effects (Stensrud et al, 2019) to define
direct and indirect effects of the treatment on the event of interest. This
definition presupposes a treatment decomposition into two components acting
along two separate causal pathways, one exclusively outside of the competing
event and the other exclusively through it. Unlike previous definitions of
direct and indirect effects, the separable effects can be subject to empirical
scrutiny in a study where separate interventions on the treatment components
are available. Here we extend and generalize the notion of the separable
effects in several ways, allowing for interpretation, identification and
estimation under considerably weaker assumptions. We propose and discuss a
definition of separable effects that is applicable to general time-varying
structures, where the separable effects can still be meaningfully interpreted,
even when they cannot be regarded as direct and indirect effects. We further
derive weaker conditions for identification of separable effects in
observational studies where decomposed treatments are not yet available; in
particular, these conditions allow for time-varying common causes of the event
of interest, the competing events and loss to follow-up. For these general
settings, we propose semi-parametric weighted estimators that are
straightforward to implement. As an illustration, we apply the estimators to
study the separable effects of intensive blood pressure therapy on acute kidney
injury, using data from a randomized clinical trial.
"
"stat.TH","  The endogeneity issue is fundamentally important as many empirical
applications may suffer from the omission of explanatory variables, measurement
error, or simultaneous causality. Recently, \cite{hllt17} propose a ""Deep
Instrumental Variable (IV)"" framework based on deep neural networks to address
endogeneity, demonstrating superior performances than existing approaches. The
aim of this paper is to theoretically understand the empirical success of the
Deep IV. Specifically, we consider a two-stage estimator using deep neural
networks in the linear instrumental variables model. By imposing a latent
structural assumption on the reduced form equation between endogenous variables
and instrumental variables, the first-stage estimator can automatically capture
this latent structure and converge to the optimal instruments at the minimax
optimal rate, which is free of the dimension of instrumental variables and thus
mitigates the curse of dimensionality. Additionally, in comparison with
classical methods, due to the faster convergence rate of the first-stage
estimator, the second-stage estimator has {a smaller (second order) estimation
error} and requires a weaker condition on the smoothness of the optimal
instruments. Given that the depth and width of the employed deep neural network
are well chosen, we further show that the second-stage estimator achieves the
semiparametric efficiency bound. Simulation studies on synthetic data and
application to automobile market data confirm our theory.
"
"stat.TH","  In this article, we primarily propose a novel Bayesian characterization of
stationary and nonstationary stochastic processes. In practice, this theory
aims to distinguish between global stationarity and nonstationarity for both
parametric and nonparametric stochastic processes. Interestingly, our theory
builds on our previous work on Bayesian characterization of infinite series,
which was applied to verification of the (in)famous Riemann Hypothesis. Thus,
there seems to be interesting and important connections between pure
mathematics and Bayesian statistics, with respect to our proposed ideas. We
validate our proposed method with simulation and real data experiments
associated with different setups. In particular, applications of our method
include stationarity and nonstationarity determination in various time series
models, spatial and spatio-temporal setups, and convergence diagnostics of
Markov Chain Monte Carlo. Our results demonstrate very encouraging performance,
even in very subtle situations. Using similar principles, we also provide a
novel Bayesian characterization of mutual independence among any number of
random variables, using which we characterize the properties of point
processes, including characterizations of Poisson point processes, complete
spatial randomness, stationarity and nonstationarity. Applications to
simulation experiments with ample Poisson and non-Poisson point process models
again indicate quite encouraging performance of our proposed ideas. We further
propose a novel recursive Bayesian method for determination of frequencies of
oscillatory stochastic processes, based on our general principle. Simulation
studies and real data experiments with varieties of time series models
consisting of single and multiple frequencies bring out the worth of our
method.
"
"stat.TH","  In this article, we investigate the asymptotic properties of Bayesian
multiple testing procedures under general dependent setup, when the sample size
and the number of hypotheses both tend to infinity. Specifically, we
investigate strong consistency of the procedures and asymptotic properties of
different versions of false discovery and false non-discovery rates under the
high dimensional setup. We particularly focus on a novel Bayesian non-marginal
multiple testing procedure and its associated error rates in this regard. Our
results show that the asymptotic convergence rates of the error rates are
directly associated with the Kullback-Leibler divergence from the true model,
and the results hold even when the postulated class of models is misspecified.
For illustration of our high-dimensional asymptotic theory, we consider a
Bayesian variable selection problem in a time-varying covariate selection
framework, with autoregressive response variables. We particularly focus on the
setup where the number of hypotheses increases at a faster rate compared to the
sample size, which is the so-called ultra-high dimensional situation.
"
"stat.TH","  We prove the reduction principle for asymptotics of functionals of vector
random fields with weakly and strongly dependent components. These functionals
can be used to construct new classes of random fields with skewed and
heavy-tailed distributions. Contrary to the case of scalar long-range dependent
random fields, it is shown that the asymptotic behaviour of such functionals is
not necessarily determined by the terms at their Hermite rank. The results are
illustrated by an application to the first Minkowski functional of the Student
random fields. Some simulation studies based on the theoretical findings are
also presented.
"
"stat.TH","  In this article, we investigate posterior convergence of nonparametric binary
and Poisson regression under possible model misspecification, assuming general
stochastic process prior with appropriate properties. Our model setup and
objective for binary regression is similar to that of Ghosal and Roy (2006)
where the authors have used the approach of entropy bound and exponentially
consistent tests with the sieve method to achieve consistency with respect to
their Gaussian process prior. In contrast, for both binary and Poisson
regression, using general stochastic process prior, our approach involves
verification of asymptotic equipartition property along with the method of
sieve, which is a manoeuvre of the general results of Shalizi (2009), useful
even for misspecified models. Moreover, we will establish not only posterior
consistency but also the rates at which the posterior probabilities converge,
which turns out to be the Kullback-Leibler divergence rate. We also investgate
the traditional posterior convergence rates. Interestingly, from subjective
Bayesian viewpoint we will show that the posterior predictive distribution can
accurately approximate the best possible predictive distribution in the sense
that the Hellinger distance, as well as the total variation distance between
the two distributions can tend to zero, in spite of misspecifications.
"
"stat.TH","  We consider Bayesian inference in inverse regression problems where the
objective is to infer about unobserved covariates from observed responses and
covariates. We establish posterior consistency of such unobserved covariates in
Bayesian inverse regression problemsunder appropriate priors in a leave-one-out
cross-validation setup. We relate this to posterior consistency of inverse
reference distributions (Bhattacharya (2013)) for assessing model adequacy. We
illustrate our theory and methods with various examples of Bayesian inverse
regression, along with adequate simulation experiments.
"
"stat.TH","  In our ""big data"" age, the size and complexity of data is steadily
increasing. Methods for dimension reduction are ever more popular and useful.
Two distinct types of dimension reduction are ""data-oblivious"" methods such as
random projections and sketching, and ""data-aware"" methods such as principal
component analysis (PCA). Both have their strengths, such as speed for random
projections, and data-adaptivity for PCA. In this work, we study how to combine
them to get the best of both. We study ""sketch and solve"" methods that take a
random projection (or sketch) first, and compute PCA after. We compute the
performance of several popular sketching methods (random iid projections,
random sampling, subsampled Hadamard transform, count sketch, etc) in a general
""signal-plus-noise"" (or spiked) data model. Compared to well-known works, our
results (1) give asymptotically exact results, and (2) apply when the signal
components are only slightly above the noise, but the projection dimension is
non-negligible. We also study stronger signals allowing more general covariance
structures. We find that (a) signal strength decreases under projection in a
delicate way depending on the structure of the data and the sketching method,
(b) orthogonal projections are more accurate, (c) randomization does not hurt
too much, due to concentration of measure, (d) count sketch can be improved by
a normalization method. Our results have implications for statistical learning
and data analysis. We also illustrate that the results are highly accurate in
simulations and in analyzing empirical data.
"
"stat.TH","  While feedback loops are known to play important roles in many complex
systems, their existence is ignored in a large part of the causal discovery
literature, as systems are typically assumed to be acyclic from the outset.
When applying causal discovery algorithms designed for the acyclic setting on
data generated by a system that involves feedback, one would not expect to
obtain correct results. In this work, we show that---surprisingly---the output
of the Fast Causal Inference (FCI) algorithm is correct if it is applied to
observational data generated by a system that involves feedback. More
specifically, we prove that for observational data generated by a simple and
$\sigma$-faithful Structural Causal Model (SCM), FCI is sound and complete, and
can be used to consistently estimate (i) the presence and absence of causal
relations, (ii) the presence and absence of direct causal relations, (iii) the
absence of confounders, and (iv) the absence of specific cycles in the causal
graph of the SCM. We extend these results to constraint-based causal discovery
algorithms that exploit certain forms of background knowledge, including the
causally sufficient setting (e.g., the PC algorithm) and the Joint Causal
Inference setting (e.g., the FCI-JCI algorithm).
"
"stat.TH","  We consider an integer-valued time series $Y=(Y_t)_{t\in\Z}$ where the models
after a time $k^*$ is Poisson autoregressive with the conditional mean that
depends on a parameter $\theta^*\in\Theta\subset\R^d$. The structure of the
process before $k^*$ is unknown;? it could be any other integer-valued time
series, that is, the process $Y$ could be nonstationary.? It is established
that the maximum likelihood estimator of $\theta^*$ computed on the
nonstationary observations is consistent and asymptotically normal. Next, we
carry out the sequential change-point detection in a large class of Poisson
autoregressive models. We propose a monitoring scheme for detecting change in
the model. The procedure is based on an updated estimator which is computed
without the historical observations. The asymptotic behavior of the detector is
studied, in particular, the above result on the inference in a nonstationary
setting are applied to prove that the proposed procedure is consistent. A
simulation study as well as a real data application are provided.
"
"stat.TH","  A new portmanteau test statistic is proposed for detecting nonlinearity in
time series data. In this paper, we elaborate on the Toeplitz autocorrelation
matrix to the autocorrelation and cross-correlation of residuals and squared
residuals block matrix. We derive a new portmanteau test statistic using the
log of the determinant of the mth autocorrelations and cross-correlations block
matrix. The asymptotic distribution of the proposed test statistic is derived
as a linear combination of chi-squared distributions and can be approximated by
a gamma distribution. This test is applied to identify the linearity and
nonlinearity dependency of some stationary time series models. It is shown that
the convergence of the new test to its asymptotic distribution is reasonable
with higher power than other tests in many situations. We demonstrate the
efficiency of the proposed test by investigating linear and nonlinear effects
in Vodafone Qatar and Nikkei-300 daily returns.
"
"stat.TH","  Consider sample covariance matrices of the form $Q:=\Sigma^{1/2} X X^*
\Sigma^{1/2}$, where $X=(x_{ij})$ is an $n\times N$ random matrix whose entries
are independent random variables with mean zero and variance $N^{-1}$, and
$\Sigma$ is a deterministic positive-definite matrix. We study the limiting
behavior of the eigenvectors of $Q$ through the so-called eigenvector empirical
spectral distribution (VESD) $F_{\mathbf u}$, which is an alternate form of
empirical spectral distribution with weights given by $|\mathbf u^\top
\xi_k|^2$, where $\mathbf u$ is any deterministic unit vector and $\xi_k$ are
the eigenvectors of $Q$. We prove a functional central limit theorem for the
linear spectral statistics of $F_{\mathbf u}$, indexed by functions with
H{\""o}lder continuous derivatives. We show that the linear spectral statistics
converge to universal Gaussian processes both on global scales of order 1, and
on local scales that are much smaller than 1 and much larger than the typical
eigenvalues spacing $N^{-1}$. Moreover, we give explicit expressions for the
means and covariance functions of the Gaussian processes, where the exact
dependence on $\Sigma$ and $\mathbf u$ allows for more flexibility in the
applications of VESD in statistical estimations of sample covariance matrices.
"
"stat.TH","  In this paper, we study the nonparametric linear model, when the error
process is a dependent Gaussian process. We focus on the estimation of the mean
vector via a model selection approach. We first give the general theoretical
form of the penalty function, ensuring that the penalized estimator among a
collection of models satisfies an oracle inequality. Then we derive a penalty
shape involving the spectral radius of the covariance matrix of the errors,
which can be chosen proportional to the dimension when the error process is
stationary and short range dependent. However, this penalty can be too rough in
some cases, in particular when the error process is long range dependent. In a
second part, we focus on the fixed-design regression model assuming that the
error process is a stationary Gaussian process. We propose a model selection
procedure in order to estimate the mean function via piecewise polynomials on a
regular partition, when the error process is either short range dependent, long
range dependent or anti-persistent. We present different kinds of penalties,
depending on the memory of the process. For each case, an adaptive estimator is
built, and the rates of convergence are computed. Thanks to several sets of
simulations, we study the performance of these different penalties for all
types of errors (short memory, long memory and anti-persistent errors).
Finally, we give an application of our method to the well-known Nile data,
which clearly shows that the type of dependence of the error process must be
taken into account.
"
"stat.TH","  Discriminating patients with Alzheimer's disease (AD) from healthy subjects
is a crucial task in the research of Alzheimer's disease. The task can be
potentially achieved by linear discriminant analysis (LDA), which is one of the
most classical and popular classification techniques. However, the
classification problem becomes challenging for LDA because of the
high-dimensionally and the spatial dependency of the brain imaging data. To
address the challenges, researchers have proposed various ways to generalize
LDA into high-dimensional context in recent years. However, these existing
methods did not reach any consensus on how to incorporate spatially dependent
structure. In light of the current needs and limitations, we propose a new
classification method, named as Penalized Maximum Likelihood Estimation LDA
(PMLE-LDA). The proposed method uses $Mat\acute{e}rn$ covariance function to
describe the spatial correlation of brain regions. Additionally, PMLE is
designed to model the sparsity of high-dimensional features. The spatial
location information is used to address the singularity of the covariance.
Tapering technique is introduced to reduce computational burden. We show in
theory that the proposed method can not only provide consistent results of
parameter estimation and feature selection, but also generate an asymptotically
optimal classifier driven by high dimensional data with specific spatially
dependent structure. Finally, the method is validated through simulations and
an application into ADNI data for classifying Alzheimer's patients.
"
"stat.TH","  This study derives a new property of the Wishart distribution when the
degree-of-freedom and the size of the matrix parameter of the distribution grow
simultaneoulsy. Particularly, the asymptotic normality of the product of four
independent Wishart matrices is shown under a high dimensional asymptotic
regime. As an application of the result, a statistical test procedure for the
common principal components hypothesis is proposed. For this problem, the
proposed test statistic is asymptotically normal under the null hypothesis. In
addition, the proposed test statistic diverges to positive infinity in
probability under the alternative hypothesis.
"
"stat.TH","  We study the frequentist properties of Bayesian statistical inference for the
stochastic block model, with an unknown number of classes of varying sizes. We
equip the space of vertex labellings with a prior on the number of classes and,
conditionally, a prior on the labels. The number of classes may grow to
infinity as a function of the number of vertices, depending on the sparsity of
the graph. We derive non-asymptotic posterior contraction rates of the form
$P_{\theta_{0,n}}\Pi_n(B_n\mid X^n)\le \epsilon_n$, where $X^n$ is the observed
graph, generated according to $P_{\theta_{0,n}}$, $B_n$ is either $\{\theta_{0,
n}\}$ or, in the very sparse case, a ball around $\theta_{0,n}$ of known
extent, and $\epsilon_n$ is an explicit rate of convergence.
  These results enable conversion of credible sets to confidence sets. In the
sparse case, credible tests are shown to be confidence sets. In the very sparse
case, credible sets are enlarged to form confidence sets. Confidence levels are
explicit, for each $n$, as a function of the credible level and the rate of
convergence.
  Hypothesis testing between the number of classes is considered with the help
of posterior odds, and is shown to be consistent. Explicit upper bounds on
errors of the first and second type and an explicit lower bound on the power of
the tests are given.
"
"stat.TH","  We study the problem of high-dimensional robust mean estimation in the
presence of a constant fraction of adversarial outliers. A recent line of work
has provided sophisticated polynomial-time algorithms for this problem with
dimension-independent error guarantees for a range of natural distribution
families.
  In this work, we show that a natural non-convex formulation of the problem
can be solved directly by gradient descent. Our approach leverages a novel
structural lemma, roughly showing that any approximate stationary point of our
non-convex objective gives a near-optimal solution to the underlying robust
estimation task. Our work establishes an intriguing connection between
algorithmic high-dimensional robust statistics and non-convex optimization,
which may have broader applications to other robust estimation tasks.
"
"stat.TH","  The present contribution investigates multivariate bootstrap procedures for
general stabilizing statistics, with specific application to topological data
analysis. Existing limit theorems for topological statistics prove difficult to
use in practice for the construction of confidence intervals, motivating the
use of the bootstrap in this capacity. However, the standard nonparametric
bootstrap does not provide for asymptotically valid confidence intervals in
some situations. The smoothed bootstrap, instead, is shown to give consistent
estimation where the standard bootstrap fails. The present work relates to
other general results in the area of stabilizing statistics, including central
limit theorems for functionals of Poisson and Binomial processes in the
critical regime. Specific statistics considered include the persistent Betti
numbers of \^Cech and Vietoris-Rips complexes over point sets in $\mathbf R^d$,
along with Euler characteristics, and the total edge length of the $k$-nearest
neighbor graph. We further define a new type of $B$-bounded persistent
homology, and investigate its fundamental properties. Specific emphasis is made
to weakening the necessary conditions needed to establish bootstrap
consistency. In particular, the assumption of a continuous underlying density
is not required. A simulation study is provided to assess the performance of
the bootstrap for finite sample sizes. Data application is made to a cosmic web
dataset from the Sloan Digital Sky Survey (SDSS).
"
"stat.TH","  In the multivariate regression, also referred to as multi-task learning in
machine learning, the goal is to recover a vector-valued function based on
noisy observations. The vector-valued function is often assumed to be of low
rank. Although the multivariate linear regression is extensively studied in the
literature, a theoretical study on the multivariate nonlinear regression is
lacking. In this paper, we study reduced rank multivariate kernel ridge
regression, proposed by \cite{mukherjee2011reduced}. We prove the consistency
of the function predictor and provide the convergence rate. An algorithm based
on nuclear norm relaxation is proposed. A few numerical examples are presented
to show the smaller mean squared prediction error comparing with the
elementwise univariate kernel ridge regression.
"
"stat.TH","  We show, by an explicit construction, that a mixture of univariate Gaussian
densities with variance $1$ and means in $[-A,A]$ can have $\Omega(A^2)$ modes.
This disproves a recent conjecture of Dytso, Yagli, Poor and Shamai
\cite{DYPS20} who showed that such a mixture can have at most $O(A^{2})$ modes
and surmised that the upper bound could be improved to $O(A)$. Our result holds
even if an additional variance constraint is imposed on the mixing
distribution. Extending the result to higher dimensions, we exhibit a mixture
of Gaussians in $\mathbb{R}^{d}$, with identity covariances and means inside
$[-A,A]^{d}$, that has $\Omega(A^{2d})$ modes.
"
"stat.TH","  We consider inference problems for high-dimensional (HD) functional data with
a dense number (T) of repeated measurements taken for a large number of p
variables from a small number of n experimental units. The spatial and temporal
dependence, high dimensionality, and the dense number of repeated measurements
all make theoretical studies and computation challenging. This paper has two
aims; our first aim is to solve the theoretical and computational challenges in
detecting and identifying change points among covariance matrices from HD
functional data. The second aim is to provide computationally efficient and
tuning-free tools with a guaranteed stochastic error control. The change point
detection procedure is developed in the form of testing the homogeneity of
covariance matrices. The weak convergence of the stochastic process formed by
the test statistics is established under the ""large p, large T and small n""
setting. Under a mild set of conditions, our change point identification
estimator is proven to be consistent for change points in any location of a
sequence. Its rate of convergence depends on the data dimension, sample size,
number of repeated measurements, and signal-to-noise ratio. We also show that
our proposed computation algorithms can significantly reduce the computation
time and are applicable to real-world data such as fMRI data with a large
number of HD repeated measurements. Simulation results demonstrate both finite
sample performance and computational effectiveness of our proposed procedures.
We observe that the empirical size of the test is well controlled at the
nominal level, and the locations of multiple change points can accurately be
identified. An application to fMRI data demonstrates that our proposed methods
can identify event boundaries in the preface of the movie Sherlock. Our
proposed procedures are implemented in an R package TechPhD.
"
"stat.TH","  We consider detecting the evolutionary oscillatory pattern of a signal when
it is contaminated by non-stationary noises with complexly time-varying data
generating mechanism. A high-dimensional dense progressive periodogram test is
proposed to accurately detect all oscillatory frequencies. A further
phase-adjusted local change point detection algorithm is applied in the
frequency domain to detect the locations at which the oscillatory pattern
changes. Our method is shown to be able to detect all oscillatory frequencies
and the corresponding change points within an accurate range with a prescribed
probability asymptotically. This study is motivated by oscillatory frequency
estimation and change point detection problems encountered in physiological
time series analysis. An application to spindle detection and estimation in
sleep EEG data is used to illustrate the usefulness of the proposed
methodology. A Gaussian approximation scheme and an overlapping-block
multiplier bootstrap methodology for sums of complex-valued high dimensional
non-stationary time series without variance lower bounds are established, which
could be of independent interest.
"
"stat.TH","  Latent position models and their corresponding estimation procedures offer a
statistically principled paradigm for multiple network inference by translating
multiple network analysis problems to familiar task in multivariate statistics.
Latent position estimation is a fundamental task in this framework yet most
work focus only on unbiased estimation procedures. We consider the
ramifications of utilizing biased latent position estimates in subsequent
statistical analysis in exchange for sizable variance reductions in finite
networks. We establish an explicit bias-variance tradeoff for latent position
estimates produced by the omnibus embedding of arXiv:1705.09355 in the presence
of heterogeneous network data. We reveal an analytic bias expression, derive a
uniform concentration bound on the residual term, and prove a central limit
theorem characterizing the distributional properties of these estimates. These
explicit bias and variance expressions enable us to show that the omnibus
embedding estimates are often preferable to comparable estimators with respect
to mean square error, state sufficient conditions for exact recovery in
community detection tasks, and develop a test statistic to determine whether
two graphs share the same set of latent positions. These results are
demonstrated in several experimental settings where community detection
algorithms and hypothesis testing procedures utilizing the biased latent
position estimates are competitive, and oftentimes preferable, to unbiased
latent position estimates.
"
"stat.TH","  We consider an estimation problem of expected functionals of a general random
element that values in a metric space. If the functional forms an explicit
function of some unknown parameters, we can estimate it by plugging-in a
suitable estimator in to the function, and we can find the asymptotic
distribution. However, if the functional is implicit in the parameters, it
causes a problem of specifying asymptotic distribution. This paper gives a
general condition to specify the asymptotic distribution even if the functional
is implicit in the parameters, and further investigates it in detail when the
random elements are semimartingales.
"
"stat.TH","  For two vast families of mixture distributions and a given prior, we provide
unified representations of posterior and predictive distributions. Model
applications presented include bivariate mixtures of Gamma distributions
labelled as Kibble-type, non-central Chi-square and F distributions, the
distribution of $R^2$ in multiple regression, variance mixture of normal
distributions, and mixtures of location-scale exponential distributions
including the multivariate Lomax distribution. An emphasis is also placed on
analytical representations and the relationships with a host of existing
distributions and several hypergeomtric functions of one or two variables.
"
"stat.TH","  We consider Bayesian nonparametric inference in the right-censoring survival
model, where modeling is made at the level of the hazard rate. We derive
posterior limiting distributions for linear functionals of the hazard, and then
for 'many' functionals simultaneously in appropriate multiscale spaces. As an
application, we derive Bernstein-von Mises theorems for the cumulative hazard
and survival functions, which lead to asymptotically efficient confidence bands
for these quantities. Further, we show optimal posterior contraction rates for
the hazard in terms of the supremum norm. In medical studies, a popular
approach is to model hazards a priori as random histograms with possibly
dependent heights. This and more general classes of arbitrarily smooth prior
distributions are considered as applications of our theory. A sampler is
provided for possibly dependent histogram posteriors. Its finite sample
properties are investigated on both simulated and real data experiments.
"
"stat.TH","  We construct the Copula Recursive Tree (CORT) estimator: a flexible,
consistent, piecewise linear estimator of a copula, leveraging the patchwork
copula formalization and various piecewise constant density estimators. While
the patchwork structure imposes a grid, the CORT estimator is data-driven and
constructs the (possibly irregular) grid recursively from the data, minimizing
a chosen distance on the copula space. The addition of the copula constraints
makes usual denisty estimators unusable, whereas the CORT estimator is only
concerned with dependence and guarantees the uniformity of margins. Refinements
such as localized dimension reduction and bagging are developed, analyzed, and
tested through applications on simulated data.
"
"stat.TH","  This paper is an extension of the work about the exponential increase of the
power of two non-parametric tests: the $ Z $-test and the chi-square
goodness-of-fit test. Subject to having auxiliary information, it is possible
to improve exponentially relative to the size of the sample the power of the
famous chi-square tests of independence and homogeneity. Improving the power of
these statistical tests by using auxiliary information makes it possible either
to reduce the probability of accepting the null hypothesis under the
alternative hypothesis, or to reduce the size of the sample necessary to reach
a predefined power. The suggested method is computational and some simple
statistical applications are presented to illustrate these results. The
framework of this work is non-parametric, so it can be applied to any kind of
data and any area using statistics.
"
"stat.TH","  In this paper we obtain the limit distribution for partial sums with a random
number of terms following a class of mixed Poisson distributions. The resulting
weak limit is a mixing between a normal distribution and an exponential family,
which we call by normal exponential family (NEF) laws. A new stability concept
is introduced and a relationship between {\alpha}-stable distributions and NEF
laws is established. We propose estimation of the parameters of the NEF models
through the method of moments and also by the maximum likelihood method, which
is performed via an Expectation-Maximization algorithm. Monte Carlo simulation
studies are addressed to check the performance of the proposed estimators and
an empirical illustration on financial market is presented.
"
"stat.TH","  Hyperbolic balance laws with uncertain (random) parameters and inputs are
ubiquitous in science and engineering. Quantification of uncertainty in
predictions derived from such laws, and reduction of predictive uncertainty via
data assimilation, remain an open challenge. That is due to nonlinearity of
governing equations, whose solutions are highly non-Gaussian and often
discontinuous. To ameliorate these issues in a computationally efficient way,
we use the method of distributions, which here takes the form of a
deterministic equation for spatiotemporal evolution of the cumulative
distribution function (CDF) of the random system state, as a means of forward
uncertainty propagation. Uncertainty reduction is achieved by recasting the
standard loss function, i.e., discrepancy between observations and model
predictions, in distributional terms. This step exploits the equivalence
between minimization of the square error discrepancy and the Kullback-Leibler
divergence. The loss function is regularized by adding a Lagrangian constraint
enforcing fulfillment of the CDF equation. Minimization is performed
sequentially, progressively updating the parameters of the CDF equation as more
measurements are assimilated.
"
"stat.TH","  This paper studies a service system in which arriving customers are provided
with information about the delay they will experience. Based on this
information they decide to wait for service or to leave the system. The main
objective is to estimate the customers' patience-level distribution and the
corresponding potential arrival rate, using knowledge of the actual workload
process only. We cast the system as a queueing model, so as to evaluate the
corresponding likelihood function. Estimating the unknown parameters relying on
a maximum likelihood procedure, we prove strong consistency and derive the
asymptotic distribution of the estimation error. Several applications and
extensions of the method are discussed. In particular, we indicate how our
method generalizes to a multi-server setting. The performance of our approach
is assessed through a series of numerical experiments. By fitting parameters of
hyperexponential and generalized-hyperexponential distributions our method
provides a robust estimation framework for any continuous patience-level
distribution.
"
"stat.TH","  Stable subordinators, and more general subordinators possessing power law
probability tails, have been widely used in the context of subdiffusions, where
particles get trapped or immobile in a number of time periods, called constant
periods. The lengths of the constant periods follow a one-sided distribution
which involves a parameter between 0 and 1 and whose first moment does not
exist. This paper constructs an estimator for the parameter, applying the
method of moments to the number of observed constant periods in a fixed time
interval. The resulting estimator is asymptotically unbiased and consistent,
and it is well-suited for situations where multiple observations of the same
subdiffusion process are available. We present supporting numerical examples
and an application to market price data for a low-volume stock.
"
"stat.TH","  This paper considers the problem of estimation of the Fisher information for
location from a random sample of size $n$. First, an estimator proposed by
Bhattacharya is revisited and improved convergence rates are derived. Second, a
new estimator, termed a clipped estimator, is proposed. Superior upper bounds
on the rates of convergence can be shown for the new estimator compared to the
Bhattacharya estimator, albeit with different regularity conditions. Third,
both of the estimators are evaluated for the practically relevant case of a
random variable contaminated by Gaussian noise. Moreover, using Brown's
identity, which relates the Fisher information and the minimum mean squared
error (MMSE) in Gaussian noise, two corresponding consistent estimators for the
MMSE are proposed. Simulation examples for the Bhattacharya estimator and the
clipped estimator as well as the MMSE estimators are presented. The examples
demonstrate that the clipped estimator can significantly reduce the required
sample size to guarantee a specific confidence interval compared to the
Bhattacharya estimator.
"
"stat.TH","  In this paper we consider the problem of parameter estimation in the $p$-spin
Curie-Weiss model, for $p \geq 3$. We provide a complete description of the
limiting properties of the maximum likelihood (ML) estimates of the inverse
temperature and the magnetic field given a single realization from the $p$-spin
Curie-Weiss model, complementing the well-known results in the 2-spin case
(Comets and Gidas (1991)). Our results unearth various new phase transitions
and surprising limit theorems, such as the existence of a 'critical' curve in
the parameter space, where the limiting distribution of the ML estimates is a
mixture with both continuous and discrete components. The number of mixture
components is either two or three, depending on, among other things, the sign
of one of the parameters and the parity of $p$. Another interesting revelation
is the existence of certain 'special' points in the parameter space where the
ML estimates exhibit a superefficiency phenomenon, converging to a non-Gaussian
limiting distribution at rate $N^{\frac{3}{4}}$. As a consequence of our
analysis, we obtain limit theorems for the average magnetization, results which
are of independent interest, that provide key insights into the thermodynamic
properties of the model.
"
"stat.TH","  The large bulk of work in multiple testing has focused on specifying
procedures that control the false discovery rate (FDR), with relatively less
attention being paid to the corresponding Type II error known as the false
non-discovery rate (FNR). A line of more recent work in multiple testing has
begun to investigate the tradeoffs between the FDR and FNR and to provide lower
bounds on the performance of procedures that depend on the model structure.
Lacking thus far, however, has been a general approach to obtaining lower
bounds for a broad class of models. This paper introduces an analysis strategy
based on derandomization, illustrated by applications to various concrete
models. Our main result is meta-theorem that gives a general recipe for
obtaining lower bounds on the combination of FDR and FNR. We illustrate this
meta-theorem by deriving explicit bounds for several models, including
instances with dependence, scale-transformed alternatives, and
non-Gaussian-like distributions. We provide numerical simulations of some of
these lower bounds, and show a close relation to the actual performance of the
Benjamini-Hochberg (BH) algorithm.
"
"stat.TH","  The use of heuristics to assess the convergence and compress the output of
Markov chain Monte Carlo can be sub-optimal in terms of the empirical
approximations that are produced. Typically a number of the initial states are
attributed to ""burn in"" and removed, whilst the remainder of the chain is
""thinned"" if compression is also required. In this paper we consider the
problem of retrospectively selecting a subset of states, of fixed cardinality,
from the sample path such that the approximation provided by their empirical
distribution is close to optimal. A novel method is proposed, based on greedy
minimisation of a kernel Stein discrepancy, that is suitable for problems where
heavy compression is required. Theoretical results guarantee consistency of the
method and its effectiveness is demonstrated in the challenging context of
parameter inference for ordinary differential equations. Software is available
in the Stein Thinning package in both Python and MATLAB.
"
"stat.TH","  We develop theoretical finite-sample results concerning the size of wild
bootstrap-based heteroskedasticity robust tests in linear regression models. In
particular, these results provide an efficient diagnostic check, which can be
used to weed out tests that are unreliable for a given testing problem in the
sense that they overreject substantially. This allows us to assess the
reliability of a large variety of wild bootstrap-based tests in an extensive
numerical study.
"
"stat.TH","  The Youden index is a popular summary statistic for receiver operating
characteristic curve. It gives the optimal cutoff point of a biomarker to
distinguish the diseased and healthy individuals. In this paper, we propose to
model the distributions of a biomarker for individuals in the healthy and
diseased groups via a semiparametric density ratio model. Based on this model,
we use the maximum empirical likelihood method to estimate the Youden index and
the optimal cutoff point. We further establish the asymptotic normality of the
proposed estimators and construct valid confidence intervals for the Youden
index and the corresponding optimal cutoff point. The proposed method
automatically covers both cases when there is no lower limit of detection
(LLOD) and when there is a fixed and finite LLOD for the biomarker. Extensive
simulation studies and a real data example are used to illustrate the
effectiveness of the proposed method and its advantages over the existing
methods.
"
"stat.TH","  In this paper introduces a new family of continuous distributions namely the
Poison transmuted-G family of distribution is proposed by inducing two addition
parameter on the base line G distribution. Some of its mathematical properties
including explicit expressions for the moments generating function, order
statistics, Probability weighted moments, stress-strength reliability, residual
life, reversed residual life, R\'enyi entropy and mean deviation are derived.
Some special models of the new family are listed. Estimation of the model
parameters by the maximum likelihood method is discussed. The advantage of the
proposed family in data fitting is illustrated by means of two applications to
failure time data set.
"
"stat.TH","  We propose an empirical Bayes method to estimate high-dimensional covariance
matrices. Our procedure centers on vectorizing the covariance matrix and
treating matrix estimation as a vector estimation problem. Drawing from the
compound decision theory literature, we introduce a new class of decision rules
that generalizes several existing procedures. We then use a nonparametric
empirical Bayes g-modeling approach to estimate the oracle optimal rule in that
class. This allows us to let the data itself determine how best to shrink the
estimator, rather than shrinking in a pre-determined direction such as toward a
diagonal matrix. Simulation results and a gene expression network analysis
shows that our approach can outperform a number of state-of-the-art proposals
in a wide range of settings, sometimes substantially.
"
"stat.TH","  In this paper, using the shrinkage-based approach for portfolio weights and
modern results from random matrix theory we construct an effective procedure
for testing the efficiency of the expected utility (EU) portfolio and discuss
the asymptotic behavior of the proposed test statistic under the
high-dimensional asymptotic regime, namely when the number of assets $p$
increases at the same rate as the sample size $n$ such that their ratio $p/n$
approaches a positive constant $c\in(0,1)$ as $n\to\infty$. We provide an
extensive simulation study where the power function and receiver operating
characteristic curves of the test are analyzed. In the empirical study, the
methodology is applied to the returns of S\&P 500 constituents.
"
"stat.TH","  Corrupted data sets containing noisy or missing observations are prevalent in
various contemporary applications such as economics, finance and
bioinformatics. Despite the recent methodological and algorithmic advances in
high-dimensional multi-response regression, how to achieve scalable and
interpretable estimation under contaminated covariates is unclear. In this
paper, we develop a new methodology called convex conditioned sequential sparse
learning (COSS) for error-in-variables multi-response regression under both
additive measurement errors and random missing data. It combines the strengths
of the recently developed sequential sparse factor regression and the nearest
positive semi-definite matrix projection, thus enjoying stepwise convexity and
scalability in large-scale association analyses. Comprehensive theoretical
guarantees are provided and we demonstrate the effectiveness of the proposed
methodology through numerical studies.
"
"stat.TH","  For an empirical signed measure $\mu = \frac{1}{N} \left(\sum_{i=1}^P
\delta_{x_i} - \sum_{i=1}^M \delta_{y_i}\right)$, particle annihilation (PA)
removes $N_A$ particles from both $\{x_i\}_{i=1}^P$ and $\{y_i\}_{i=1}^M$
simultaneously, yielding another empirical signed measure $\nu$ such that $\int
f d \nu$ approximates to $\int f d \mu$ within an acceptable accuracy for
suitable test functions $f$. Such annihilation of particles carrying opposite
importance weights has been extensively utilized for alleviating the numerical
sign problem in particle simulations. In this paper, we propose an algorithm
for PA in high-dimensional Euclidean space based on hybrid of clustering and
matching, dubbed the Sequential-clustering Particle Annihilation via
Discrepancy Estimation (SPADE). It consists of two steps: Adaptive clustering
of particles via controlling their number-theoretic discrepancies, and
independent random matching among positive and negative particles in each
cluster. Both deterministic error bounds by the Koksma-Hlawka inequality and
non-asymptotic random error bounds by concentration inequalities are proved to
be affected by two factors. One factor measures the irregularity of point
distributions and reflects their discrete nature. The other relies on the
variation of test function and is influenced by the continuity. Only the latter
implicitly depends on dimensionality $d$, implying that SPADE can be immune to
the curse of dimensionality for a wide class of test functions. Numerical
experiments up to $d=1080$ validate our theoretical discoveries.
"
"stat.TH","  Sparse principal component analysis (PCA) is a popular dimensionality
reduction technique for obtaining principal components which are linear
combinations of a small subset of the original features. Existing approaches
cannot supply certifiably optimal principal components with more than $p=100s$
covariates. By reformulating sparse PCA as a convex mixed-integer semidefinite
optimization problem, we design a cutting-plane method which solves the problem
to certifiable optimality at the scale of selecting k=10s covariates from p=300
variables, and provides small bound gaps at a larger scale. We also propose two
convex relaxations and randomized rounding schemes that provide certifiably
near-exact solutions within minutes for p=100s or hours for p=1,000s. Using
real-world financial and medical datasets, we illustrate our approach's ability
to derive interpretable principal components tractably at scale.
"
"stat.TH","  We obtain explicit Wasserstein distance error bounds between the distribution
of the multi-parameter MLE and the multivariate normal distribution. Our
general bounds are given for possibly high-dimensional, independent and
identically distributed random vectors. Our general bounds are of the optimal
$\mathcal{O}(n^{-1/2})$ order. We apply our general bounds to derive
Wasserstein distance error bounds for the multivariate normal approximation of
the MLE in several settings; these being single-parameter exponential families,
the normal distribution under canonical parametrisation, and the multivariate
normal distribution under non-canonical parametrisation.
"
"stat.TH","  Consider a Markov process with state space $[k]$, which jumps continuously to
a new state chosen uniformly at random and regardless of the previous state.
The collection of transition kernels (indexed by time $t\ge 0$) is the Potts
semigroup. Diaconis and Saloff-Coste computed the maximum of the ratio of the
relative entropy and the Dirichlet form obtaining the constant $\alpha_2$ in
the $2$-log-Sobolev inequality ($2$-LSI). In this paper, we obtain the best
possible non-linear inequality relating entropy and the Dirichlet form (i.e.,
$p$-NLSI, $p\ge1$). As an example, we show $\alpha_1 = 1+\frac{1+o(1)}{\log
k}$. The more precise NLSIs have been shown by Polyanskiy and Samorodnitsky to
imply various geometric and Fourier-analytic results.
  Beyond the Potts semigroup, we also analyze Potts channels -- Markov
transition matrices $[k]\times [k]$ constant on and off diagonal. (Potts
semigroup corresponds to a (ferromagnetic) subset of matrices with positive
second eigenvalue). By integrating the $1$-NLSI we obtain the new strong data
processing inequality (SDPI), which in turn allows us to improve results on
reconstruction thresholds for Potts models on trees. A special case is the
problem of reconstructing color of the root of a $k$-colored tree given
knowledge of colors of all the leaves. We show that to have a non-trivial
reconstruction probability the branching number of the tree should be at least
$$\frac{\log k}{\log k - \log(k-1)} = (1-o(1))k\log k.$$ This extends previous
results (of Sly and Bhatnagar et al.) to general trees, and avoids the need for
any specialized arguments. Similarly, we improve the state-of-the-art on
reconstruction threshold for the stochastic block model with $k$ balanced
groups, for all $k\ge 3$. These improvements advocate information-theoretic
methods as a useful complement to the conventional techniques originating from
the statistical physics.
"
"stat.TH","  For testing conditional independence (CI) of a response $Y$ and a predictor
$X$ given covariates $Z$, the recently introduced model-X (MX) framework has
been the subject of active methodological research, especially in the context
of MX knockoffs and their successful application to genome-wide association
studies. In this paper, we build a theoretical foundation for the MX CI
problem, yielding quantitative explanations for empirically observed phenomena
and novel insights to guide the design of MX methodology. We focus our analysis
on the conditional randomization test (CRT), whose validity conditional on
$Y,Z$ allows us to view it as a test of a point null hypothesis involving the
conditional distribution of $X$. We use the Neyman-Pearson lemma to derive the
most powerful CRT statistic against a point alternative as well as an analogous
result for MX knockoffs. We define CRT-style analogs of $t$- and $F$-tests with
explicit critical values, and show that they have uniform asymptotic Type-I
error control under the assumption that only the first two moments of $X$ given
$Z$ are known, a significant relaxation of MX. We derive expressions for the
power of these tests against local semiparametric alternatives using Le Cam's
local asymptotic normality theory, explicitly capturing the prediction error of
the underlying learning algorithm. Finally, we pave the way for estimation in
the MX setting by drawing connections to semiparametric statistics and causal
inference. Thus, this work forms explicit bridges from MX to both classical
statistics (testing) and modern causal inference (estimation).
"
"stat.TH","  In this paper, we study the geometry induced by the Fisher-Rao metric on the
parameter space of Dirichlet distributions. We show that this space is
geodesically complete and has everywhere negative sectional curvature. An
important consequence of this negative curvature for applications is that the
Fr{\'e}chet mean of a set of Dirichlet distributions is uniquely defined in
this geometry.
"
"stat.TH","  The statistics and machine learning communities have recently seen a growing
interest in classification-based approaches to two-sample testing (e.g. Kim et
al. [2016]; Rosenblatt et al. [2016]; Lopez-Paz and Oquab [2017]; Hediger et
al. [2019]). The outcome of a classification-based two-sample test remains a
rejection decision, which is not always informative since the null hypothesis
is seldom strictly true. Therefore, when a test rejects, it would be beneficial
to provide an additional quantity serving as a refined measure of
distributional difference. In this work, we introduce a framework for the
construction of high-probability lower bounds on the total variation distance.
These bounds are based on a one-dimensional projection, such as a
classification or regression method, and can be interpreted as the minimal
fraction of samples pointing towards a distributional difference. We further
derive asymptotic power and detection rates of two proposed estimators and
discuss potential uses through an application to a reanalysis climate dataset.
"
"stat.TH","  Let $A$ be a rectangular matrix of size $m\times n$ and $A_1$ be the random
matrix where each entry of $A$ is multiplied by an independent
$\{0,1\}$-Bernoulli random variable with parameter $1/2$. This paper is about
when, how and why the non-Hermitian eigen-spectra of the randomly induced
asymmetric matrices $A_1 (A - A_1)^*$ and $(A-A_1)^*A_1$ captures more of the
relevant information about the principal component structure of $A$ than via
its SVD or the eigen-spectra of $A A^*$ and $A^* A$, respectively. Hint: the
asymmetry inducing randomness breaks the echo-chamber effect that cripples the
SVD.
  We illustrate the application of this striking phenomenon on the low-rank
matrix completion problem for the setting where each entry is observed with
probability $d/n$, including the very sparse regime where $d$ is of order $1$,
where matrix completion via the SVD of $A$ fails or produces unreliable
recovery. We determine an asymptotically exact, matrix-dependent, non-universal
detection threshold above which reliable, statistically optimal matrix recovery
using a new, universal data-driven matrix-completion algorithm is possible.
Averaging the left and right eigenvectors provably improves the recovered
matrix but not the detection threshold. We define another variant of this
asymmetric procedure that bypasses the randomization step and has a detection
threshold that is smaller by a constant factor but with a computational cost
that is larger by a polynomial factor of the number of observed entries. Both
detection thresholds shatter the seeming barrier due to the well-known
information theoretical limit $d \asymp \log n$ for matrix completion found in
the literature.
"
"stat.TH","  In cluster-specific studies, ordinary logistic regression and conditional
logistic regression for binary outcomes provide maximum likelihood estimator
(MLE) and conditional maximum likelihood estimator (CMLE), respectively. In
this paper, we show that CMLE is approaching to MLE asymptotically when each
individual data point is replicated infinitely many times. Our theoretical
derivation is based on the observation that a term appearing in the conditional
average log-likelihood function is the coefficient of a polynomial, and hence
can be transformed to a complex integral by Cauchy's differentiation formula.
The asymptotic analysis of the complex integral can then be performed using the
classical method of steepest descent. Our result implies that CMLE can be
biased if individual weights are multiplied with a constant, and that we should
be cautious when assigning weights to cluster-specific studies.
"
"stat.TH","  In this study, we develop an asymptotic theory of nonparametric regression
for locally stationary random fields (LSRFs) $\{{\bf X}_{{\bf s}, A_{n}}: {\bf
s} \in R_{n} \}$ in $\mathbb{R}^{p}$ observed at irregularly spaced locations
in $R_{n} =[0,A_{n}]^{d} \subset \mathbb{R}^{d}$. We first derive the uniform
convergence rate of general kernel estimators, followed by the asymptotic
normality of an estimator for the mean function of the model. Moreover, we
consider additive models to avoid the curse of dimensionality arising from the
dependence of the convergence rate of estimators on the number of covariates.
Subsequently, we derive the uniform convergence rate and joint asymptotic
normality of the estimators for additive functions. We also introduce
approximately $m_{n}$-dependent RFs to provide examples of LSRFs. We find that
these RFs include a wide class of L\'evy-driven moving average RFs.
"
"stat.TH","  We study the efficient learnability of high-dimensional Gaussian mixtures in
the outlier-robust setting, where a small constant fraction of the data is
adversarially corrupted. We resolve the polynomial learnability of this problem
when the components are pairwise separated in total variation distance.
Specifically, we provide an algorithm that, for any constant number of
components $k$, runs in polynomial time and learns the components of an
$\epsilon$-corrupted $k$-mixture within information theoretically near-optimal
error of $\tilde{O}(\epsilon)$, under the assumption that the overlap between
any pair of components $P_i, P_j$ (i.e., the quantity $1-TV(P_i, P_j)$) is
bounded by $\mathrm{poly}(\epsilon)$.
  Our separation condition is the qualitatively weakest assumption under which
accurate clustering of the samples is possible. In particular, it allows for
components with arbitrary covariances and for components with identical means,
as long as their covariances differ sufficiently. Ours is the first polynomial
time algorithm for this problem, even for $k=2$.
  Our algorithm follows the Sum-of-Squares based proofs to algorithms approach.
Our main technical contribution is a new robust identifiability proof of
clusters from a Gaussian mixture, which can be captured by the constant-degree
Sum of Squares proof system. The key ingredients of this proof are a novel use
of SoS-certifiable anti-concentration and a new characterization of pairs of
Gaussians with small (dimension-independent) overlap in terms of their
parameter distance.
"
"stat.TH","  The Hilbert--Schmidt Independence Criterion (HSIC) is a popular measure of
the dependency between two random variables. The statistic dHSIC is an
extension of HSIC that can be used to test joint independence of $d$ random
variables. Such hypothesis testing for (joint) independence is often done using
a permutation test, which compares the observed data with randomly permuted
datasets. The main contribution of this work is proving that the power of such
independence tests converges to 1 as the sample size converges to infinity.
This answers a question that was asked in (Pfister, 2018) Additionally this
work proves correct type 1 error rate of HSIC and dHSIC permutation tests and
provides guidance on how to select the number of permutations one uses in
practice. While correct type 1 error rate was already proved in (Pfister,
2018), we provide a modified proof following (Berrett, 2019), which extends to
the case of non-continuous data. The number of permutations to use was studied
e.g. by (Marozzi, 2004) but not in the context of HSIC and with a slight
difference in the estimate of the $p$-value and for permutations rather than
vectors of permutations. While the last two points have limited novelty we
include these to give a complete overview of permutation testing in the context
of HSIC and dHSIC.
"
"stat.TH","  Data-driven prediction is becoming increasingly widespread as the volume of
data available grows and as algorithmic development matches this growth. The
nature of the predictions made, and the manner in which they should be
interpreted, depends crucially on the extent to which the variables chosen for
prediction are Markovian, or approximately Markovian. Multiscale systems
provide a framework in which this issue can be analyzed. In this work kernel
analog forecasting methods are studied from the perspective of data generated
by multiscale dynamical systems. The problems chosen exhibit a variety of
different Markovian closures, using both averaging and homogenization;
furthermore, settings where scale-separation is not present and the predicted
variables are non-Markovian, are also considered. The studies provide guidance
for the interpretation of data-driven prediction methods when used in practice.
"
"stat.TH","  In statistical decision theory involving a single decision-maker, one says
that an information structure is better than another one if for any cost
function involving a hidden state variable and an action variable which is
restricted to be only a function of some measurement, the solution value under
the former is not worse than the value under the latter. For finite probability
spaces, Blackwell's celebrated theorem on comparison of information structures
leads to a complete characterization on when one information structure is
better than another. For stochastic games with incomplete information, due to
the presence of competition among decision makers, in general such an ordering
is not possible since additional information can lead to equilibria
perturbations with positive or negative values to a player. However, for
zero-sum games in a finite probability space, P\k{e}ski introduced a complete
characterization of ordering of information structures. In this paper, we
obtain an infinite dimensional (standard Borel) generalization of P\k{e}ski's
result. A corollary of our analysis is that more information cannot hurt a
decision maker taking part in a zero-sum game in standard Borel spaces. During
our analysis, we establish two novel supporting results: (i) a partial converse
to Blackwell's ordering of information structures in the standard Borel space
setup and (ii) a refined existence result for equilibria in zero-sum games with
incomplete information when compared with the prior literature.
"
"stat.TH","  Accelerated life-testing (ALT) is a very useful technique for examining the
reliability of highly reliable products. It allows testing the products at
higher than usual stress conditions to induce failures more quickly and
economically than under typical conditions. A special case of ALT are
step-stress tests that allow experimenter to increase the stress levels at
fixed times. This paper deals with the multiple step step-stress model under
the cumulative exposure model with lognormally distributed lifetimes in the
presence of Type-II and Progressive Type-II censoring. For this model, the
maximum likelihood estimates (MLE) of its parameters, as well as the
corresponding observed Fisher Information Matrix (FI), are derived. The
likelihood equations do not lead to closed-form expressions for the MLE, and
they need to be solved by means of an iterative procedure, such as the
Newton-Raphson method. We then evaluate the bias and mean square error of the
estimates and provide asymptotic and bootstrap confidence intervals. Finally,
in order to asses the performance of the confidence intervals, a Monte Carlo
simulation study is conducted.
"
"stat.TH","  We provide lower bounds for the estimation of the eigenspaces of a covariance
operator. These information inequalities are non-asymptotic and can be applied
to any sequence of eigenvalues. In the important case of the eigenspace of the
$d$ leading eigenvalues, the lower bounds match non-asymptotic upper bounds
based on the empirical covariance operator. Our approach relies on a van Trees
inequality for equivariant models, with the reference measure being the Haar
measure on the orthogonal group, combined with large deviations techniques to
design optimal prior densities.
"
"stat.TH","  Parametric distributions are an important part of statistics. There is now a
voluminous literature on different fascinating formulations of flexible
distributions. We present a selective and brief overview of a small subset of
these distributions, focusing on those that are obtained by scaling the mean
and/or covariance matrix of the (multivariate) normal distribution with some
scaling variable(s). Namely, we consider the families of mean mixture, variance
mixture, and mean-variance mixture of normal distributions. Its basic
properties, some notable special/limiting cases, and parameter estimation
methods are also described.
"
"stat.TH","  Let $T>0,\alpha>\frac12$. In the present paper we consider the
$\alpha$-Brownian bridge defined as $dX_t=-\alpha\frac{X_t}{T-t}dt+dW_t,~ 0\leq
t< T$, where $W$ is a standard Brownian motion. We investigate the optimal rate
of convergence to normality of the maximum likelihood estimator (MLE) for the
parameter $ \alpha $ based on the continuous observation $\{X_s,0\leq s\leq
t\}$ as $t\uparrow T$. We prove that an optimal rate of Kolmogorov distance for
central limit theorem on the MLE is given by $\frac{1}{\sqrt{|\log(T-t)|}}$, as
$t\uparrow T$. First we compute an upper bound and then find a lower bound with
the same speed using Corollary 1 and Corollary 2 of \cite{kp-JVA},
respectively.
"
"stat.TH","  This paper aims at providing statistical guarantees for a kernel based
estimation of time varying parameters driving the dynamic of local stationary
processes. We extend the results of Dahlhaus et al. (2018) considering the
local stationary version of the infinite memory processes of Doukhan and
Wintenberger (2008). The estimators are computed as localized M-estimators of
any contrast satisfying appropriate contraction conditions. We prove the
uniform consistency and pointwise asymptotic normality of such kernel based
estimators. We apply our result to usual contrasts such as least-square, least
absolute value, or quasi-maximum likelihood contrasts. Various local-stationary
processes as ARMA, AR(infty), GARCH, ARCH(infty), ARMA-GARCH,
LARCH(\infty),..., and integer valued processes are also considered. Numerical
experiments demonstrate the efficiency of the estimators on both simulated and
real data sets.
"
"stat.TH","  As a general rule of thumb the resolution of a light microscope (i.e. the
ability to discern objects) is predominantly described by the full width at
half maximum (FWHM) of its point spread function (psf)---the diameter of the
blurring density at half of its maximum. Classical wave optics suggests a
linear relationship between FWHM and resolution also manifested in the well
known Abbe and Rayleigh criteria, dating back to the end of 19th century.
However, during the last two decades conventional light microscopy has
undergone a shift from microscopic scales to nanoscales. This increase in
resolution comes with the need to incorporate the random nature of observations
(light photons) and challenges the classical view of discernability, as we
argue in this paper. Instead, we suggest a statistical description of
resolution obtained from such random data. Our notion of discernability is
based on statistical testing whether one or two objects with the same total
intensity are present. For Poisson measurements we get linear dependence of the
(minimax) detection boundary on the FWHM, whereas for a homogeneous Gaussian
model the dependence of resolution is nonlinear. Hence, at small physical
scales modeling by homogeneous gaussians is inadequate, although often
implicitly assumed in many reconstruction algorithms. In contrast, the Poisson
model and its variance stabilized Gaussian approximation seem to provide a
statistically sound description of resolution at the nanoscale. Our theory is
also applicable to other imaging setups, such as telescopes.
"
"stat.TH","  We study approximation theorems for the Euler characteristic of the
Vietoris-Rips and Cech filtration. The filtration is obtained from a Poisson or
binomial sampling scheme in the critical regime. We apply our results to the
smooth bootstrap of the Euler characteristic and determine its rate of
convergence in the Kantorovich-Wasserstein distance and in the Kolmogorov
distance.
"
"stat.TH","  We study a class of graphs that represent local independence structures in
stochastic processes allowing for correlated error processes. Several graphs
may encode the same local independencies and we characterize such equivalence
classes of graphs. In the worst case, the number of conditions in our
characterizations grows superpolynomially as a function of the size of the node
set in the graph. We show that deciding Markov equivalence is coNP-complete
which suggests that our characterizations cannot be improved upon
substantially. We prove a global Markov property in the case of a multivariate
Ornstein-Uhlenbeck process which is driven by correlated Brownian motions.
"
"stat.TH","  In this paper, we propose a cone projected power iteration algorithm to
recover the principal eigenvector from a noisy positive semidefinite matrix.
When the true principal eigenvector is assumed to belong to a convex cone, the
proposed algorithm is fast and has a tractable error. Specifically, the method
achieves polynomial time complexity for certain convex cones equipped with fast
projection such as the monotone cone. It attains a small error when the noisy
matrix has a small cone-restricted operator norm. We supplement the above
results with a minimax lower bound of the error under the spiked covariance
model. Our numerical experiments on simulated and real data, show that our
method achieves shorter run time and smaller error in comparison to the
ordinary power iteration and some sparse principal component analysis
algorithms if the principal eigenvector is in a convex cone.
"
"stat.TH","  The use of objective prior in Bayesian applications has become a common
practice to analyze data without subjective information. Formal rules usually
obtain these priors distributions, and the data provide the dominant
information in the posterior distribution. However, these priors are typically
improper and may lead to improper posterior. Here, we show, for a general
family of distributions, that the obtained objective priors for the parameters
either follow a power-law distribution or has an asymptotic power-law behavior.
As a result, we observed that the exponents of the model are between 0.5 and 1.
Understand these behaviors allow us to easily verify if such priors lead to
proper or improper posteriors directly from the exponent of the power-law. The
general family considered in our study includes essential models such as
Exponential, Gamma, Weibull, Nakagami-m, Haf-Normal, Rayleigh, Erlang, and
Maxwell Boltzmann distributions, to list a few. In summary, we show that
comprehending the mechanisms describing the shapes of the priors provides
essential information that can be used in situations where additional
complexity is presented.
"
"stat.TH","  We present non-asymptotic two-sided bounds to the log-marginal likelihood in
Bayesian inference. The classical Laplace approximation is recovered as the
leading term. Our derivation permits model misspecification and allows the
parameter dimension to grow with the sample size. We do not make any
assumptions about the asymptotic shape of the posterior, and instead require
certain regularity conditions on the likelihood ratio and that the posterior to
be sufficiently concentrated.
"
"stat.TH","  The effectiveness of Bayesian Additive Regression Trees (BART) has been
demonstrated in a variety of contexts including non parametric regression and
classification. Here we introduce a BART scheme for estimating the intensity of
inhomogeneous Poisson Processes. Poisson intensity estimation is a vital task
in various applications including medical imaging, astrophysics and network
traffic analysis. Our approach enables full posterior inference of the
intensity in a nonparametric regression setting. We demonstrate the performance
of our scheme through simulation studies on synthetic and real datasets in one
and two dimensions, and compare our approach to alternative approaches.
"
"stat.TH","  Let $\mathbb{Z}_n = \{Z_1, \ldots, Z_n\}$ be a design; that is, a collection
of $n$ points $Z_j \in [-1,1]^d$. We study the quality of quantization of
$[-1,1]^d$ by the points of $\mathbb{Z}_n$ and the problem of quality of
covering of $[-1,1]^d$ by ${\cal B}_d(\mathbb{Z}_n,r)$, the union of balls
centred at $Z_j \in \mathbb{Z}_n$. We concentrate on the cases where the
dimension $d$ is not small ($d\geq 5$) and $n$ is not too large, $n \leq 2^d$.
We define the design ${\mathbb{D}_{n,\delta}}$ as the maximum-resolution
$2^{d-1}$ design defined on vertices of the cube $[-\delta,\delta]^d$, $0\leq
\delta\leq 1$. For this design, we derive a closed-form expression for the
quantization error and very accurate approximations for {the coverage area}
vol$([-1,1]^d \cap {\cal B}_d(\mathbb{Z}_n,r)) $. We conjecture that the design
${\mathbb{D}_{n,\delta}}$ with optimal $\delta$ is the most efficient quantizer
of $[-1,1]^d$ under the assumption $n \leq 2^d$ and it is also makes a very
efficient $(1-\gamma)$-covering. We provide results of a large-scale numerical
investigation confirming the accuracy of the developed approximations and the
efficiency of the
  designs ${\mathbb{D}_{n,\delta}}$.
"
"stat.TH","  Inference problems with conjectured statistical-computational gaps are
ubiquitous throughout modern statistics, computer science and statistical
physics. While there has been success evidencing these gaps from the failure of
restricted classes of algorithms, progress towards a more traditional
reduction-based approach to computational complexity in statistical inference
has been limited. Existing reductions have largely been limited to inference
problems with similar structure -- primarily mapping among problems
representable as a sparse submatrix signal plus a noise matrix, which are
similar to the common hardness assumption of planted clique.
  The insight in this work is that a slight generalization of the planted
clique conjecture -- secret leakage planted clique -- gives rise to a variety
of new average-case reduction techniques, yielding a web of reductions among
problems with very different structure. Using variants of the planted clique
conjecture for specific forms of secret leakage planted clique, we deduce tight
statistical-computational tradeoffs for a diverse range of problems including
robust sparse mean estimation, mixtures of sparse linear regressions, robust
sparse linear regression, tensor PCA, variants of dense $k$-block stochastic
block models, negatively correlated sparse PCA, semirandom planted dense
subgraph, detection in hidden partition models and a universality principle for
learning sparse mixtures. In particular, a $k$-partite hypergraph variant of
the planted clique conjecture is sufficient to establish all of our
computational lower bounds. Our techniques also reveal novel connections to
combinatorial designs and to random matrix theory. This work gives the first
evidence that an expanded set of hardness assumptions, such as for secret
leakage planted clique, may be a key first step towards a more complete theory
of reductions among statistical problems.
"
"stat.TH","  Edge-exchangeable probabilistic network models generate edges as an i.i.d.
sequence from a discrete measure, providing a simple means for statistical
inference of latent network properties. The measure is often constructed using
the self-product of a realization from a Bayesian nonparametric (BNP) discrete
prior; but unlike in standard BNP models, the self-product measure prior is not
conjugate the likelihood, hindering the development of exact inference
algorithms. Approximate inference via finite truncation of the discrete measure
is a straightforward alternative, but incurs an unknown approximation error. In
this paper, we develop theoretical bounds on the error of finite truncation in
random self-product-measure-based models. We apply the theory to
edge-exchangeable networks, demonstrating that the truncation error for dense
graphs decreases geometrically with the truncation level, but that the
truncation error for sparse graphs decreases much more slowly. This implies
that high truncation levels---and corresponding high computational cost---are
needed to handle sparse graphs in practice. Simulations of commonly used edge
exchangeable graph models confirm the theoretical results in both sparse and
dense settings.
"
"stat.TH","  We develop new approaches in multi-class settings for constructing proper
scoring rules and hinge-like losses and establishing corresponding regret
bounds with respect to the zero-one or cost-weighted classification loss. Our
construction of losses involves deriving new inverse mappings from a concave
generalized entropy to a loss through the use of a convex dissimilarity
function related to the multi-distribution $f$-divergence. We identify new
classes of multi-class proper scoring rules, which recover and reveal
interesting relationships between various composite losses currently in use. We
also derive new hinge-like convex losses, which are tighter convex extensions
than related hinge-like losses and geometrically simpler with fewer
non-differentiable edges, while achieving similar regret bounds. Finally, we
establish new classification regret bounds in general for multi-class proper
scoring rules by exploiting the Bregman divergences of the associated
generalized entropies, and, as applications, provide simple meaningful regret
bounds for two specific classes of proper scoring rules.
"
"stat.TH","  The Weibull distribution is one of the most used tools in reliability
analysis. In this paper, assuming a Bayesian approach, we propose necessary and
sufficient conditions to verify when improper priors lead to proper posteriors
for the parameters of the Weibull distribution in the presence of complete or
right-censored data. Additionally, we proposed sufficient conditions to verify
if the obtained posterior moments are finite. These results can be achieved by
checking the behavior of the improper priors, which are applied in different
objective priors to illustrate the usefulness of the new results. As an
application of our theorem, we prove that if the improper prior leads to a
proper posterior, the posterior mean, as well as other higher moments of the
scale parameter, are not finite and, therefore, should not be used.
"
"stat.TH","  This document aims to provide an accessible tutorial on the unbiased
estimation of multivariate cumulants, using $k$-statistics. We offer an
explicit and general formula for multivariate $k$-statistics of arbitrary
order. We also prove that the $k$-statistics are unbiased, using M\""obius
inversion and rudimentary combinatorics. Many detailed examples are considered
throughout the paper. We conclude with a discussion of $k$-statistics
computation, including the challenge of time complexity, and we examine a
couple of possible avenues to improve the efficiency of this computation. The
purpose of this document is threefold: to provide a clear introduction to
$k$-statistics without relying on specialized tools like the umbral calculus;
to construct an explicit formula for $k$-statistics that might facilitate
future approximations and faster algorithms; and to serve as a companion paper
to our Python library PyMoments, which implements this formula.
"
"stat.TH","  In the present paper we consider the Ornstein-Uhlenbeck process of the second
kind defined as solution to the equation $dX_{t} = -\alpha
X_{t}dt+dY_{t}^{(1)},
  \ \ X_{0}=0$, where $Y_{t}^{(1)}:=\int_{0}^{t}e^{-s}dB^H_{a_{s}}$ with
$a_{t}=He^{\frac{t}{H}}$, and $B^H$ is a fractional Brownian motion with Hurst
parameter $H\in(\frac12,1)$, whereas $\alpha>0$ is unknown parameter to be
estimated. We obtain the upper bound $O(1/\sqrt{T})$ in Kolmogorov distance for
normal approximation of the least squares estimator of the drift parameter
$\alpha$ on the basis of the continuous observation $\{X_t,t\in[0,T]\}$, as
$T\rightarrow\infty$. Our method is based on the work of \cite{kp-JVA}, which
is proved using a combination of Malliavin calculus and Stein's method for
normal approximation.
"
"stat.TH","  In this paper we consider the problem of constructing confidence intervals
for coefficients of martingale regression models (in particular, time series
models) after variable selection. Although constructing confidence intervals
are common practice in statistical analysis, it is challenging in our framework
due to the data-dependence of the selected model and the correlation among the
variables being selected and not selected. We first introduce estimators for
the selected coefficients and show that it is consistent under martingale
regression model, in which the observations can be dependent and the errors can
be heteroskedastic. Then we use the estimators together with a resampling
approach to construct confidence intervals. Our simulation results show that
our approach outperforms other existing approaches in various data structures.
"
"stat.TH","  The main idea of nested sampling is to substitute the high-dimensional
likelihood integral over the parameter space $\Omega$ by an integral over the
unit line $[0,1]$ by employing a push-forward with respect to a suitable
transformation. For this substitution, it is often implicitly or explicitly
assumed that samples from the prior are uniformly distributed along this unit
line after having been mapped by this transformation. We show that this
assumption is wrong, especially in the case of a likelihood function with
plateaus. Nevertheless, we show that the substitution enacted by nested
sampling works because of more interesting reasons which we lay out. Although
this means that analytically, nested sampling can deal with plateaus in the
likelihood function, the actual performance of the algorithm suffers under such
a setting and the method fails to approximate the evidence, mean and variance
appropriately. We suggest a robust implementation of nested sampling by a
simple decomposition idea which demonstrably overcomes this issue.
"
"stat.TH","  This work considers methods for imposing sparsity in Bayesian regression with
applications in nonlinear system identification. We first review automatic
relevance determination (ARD) and analytically demonstrate the need to
additional regularization or thresholding to achieve sparse models. We then
discuss two classes of methods, regularization based and thresholding based,
which build on ARD to learn parsimonious solutions to linear problems. In the
case of orthogonal covariates, we analytically demonstrate favorable
performance with regards to learning a small set of active terms in a linear
system with a sparse solution. Several example problems are presented to
compare the set of proposed methods in terms of advantages and limitations to
ARD in bases with hundreds of elements. The aim of this paper is to analyze and
understand the assumptions that lead to several algorithms and to provide
theoretical and empirical results so that the reader may gain insight and make
more informed choices regarding sparse Bayesian regression.
"
"stat.TH","  The spread of infectious disease in a human community or the proliferation of
fake news on social media can be modeled as a randomly growing tree-shaped
graph. The history of the random growth process is often unobserved but
contains important information such as the source of the infection. We consider
the problem of statistical inference on aspects of the latent history using
only a single snapshot of the final tree. Our approach is to apply random
labels to the observed unlabeled tree and analyze the resulting distribution of
the growth process, conditional on the final outcome. We show that this
conditional distribution is tractable under a shape-exchangeability condition,
which we introduce here, and that this condition is satisfied for many popular
models for randomly growing trees such as uniform attachment, linear
preferential attachment and uniform attachment on a $D$-regular tree. For
inference of the root under shape-exchangeability, we propose computationally
scalable algorithms for constructing confidence sets with valid frequentist
coverage as well as bounds on the expected size of the confidence sets. We also
provide efficient sampling algorithms that extend our methods to a wide class
of inference problems.
"
"stat.TH","  Optimal linear prediction (also known as kriging) of a random field
$\{Z(x)\}_{x\in\mathcal{X}}$ indexed by a compact metric space
$(\mathcal{X},d_{\mathcal{X}})$ can be obtained if the mean value function
$m\colon\mathcal{X}\to\mathbb{R}$ and the covariance function
$\varrho\colon\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ of $Z$ are known. We
consider the problem of predicting the value of $Z(x^*)$ at some location
$x^*\in\mathcal{X}$ based on observations at locations $\{x_j\}_{j=1}^n$ which
accumulate at $x^*$ as $n\to\infty$ (or, more generally, predicting $f(Z)$
based on $\{ f_j(Z) \}_{j=1}^n$ for linear functionals $f, f_1, \ldots, f_n$).
Our main result characterizes the asymptotic performance of linear predictors
(as $n$ increases) based on an incorrect second order structure
$(\tilde{m},\tilde{\varrho})$, without any restrictive assumptions on $\varrho,
\tilde{\varrho}$ such as stationarity. We, for the first time, provide
necessary and sufficient conditions on $(\tilde{m},\tilde{\varrho})$ for
asymptotic optimality of the corresponding linear predictor holding uniformly
with respect to $f$. These general results are illustrated by an example on the
sphere $\mathbb{S}^2$ for the case of two isotropic covariance functions.
"
"stat.TH","  We present a consistent approach to density-based clustering, which satisfies
a stability theorem that holds without any distributional assumptions. We also
show that the algorithm can be combined with standard procedures to extract a
flat clustering from a hierarchical clustering, and that the resulting flat
clustering algorithms satisfy stability theorems. The algorithms and proofs are
inspired by topological data analysis.
"
"stat.TH","  Focus on linear regression model, in this paper we introduce a bootstrap
algorithm for prediction which controls the possibility of under-coverage and
provide the theoretical proof on validity of this algorithm. In addition, we
derive the asymptotic distribution of the difference between probability of
future observation conditioning on observed data and conditional coverage
probability generated by residual-based bootstrap algorithm. By applying this
result, we show that residual-based bootstrap algorithm asymptotically has
$50\%$ possibility of under-coverage without modification. We perform several
numerical experiments and the proposed algorithm has desired possibilities of
under-coverage in those experiments with moderate sample sizes. Results
mentioned in this paper can be extended to different statistical inference
models, especially to dependent situations like ARMA models, Arch models and
others.
"
"stat.TH","  The shortest path problem is formulated as an $l_1$-regularized regression
problem, known as lasso. Based on this formulation, a connection is established
between Dijkstra's shortest path algorithm and the least angle regression
(LARS) for the lasso problem. Specifically, the solution path of the lasso
problem, obtained by varying the regularization parameter from infinity to zero
(the regularization path), corresponds to shortest path trees that appear in
the bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the
LARS formulation provide exact solutions, they become impractical when the size
of the graph is exceedingly large. To overcome this issue, the alternating
direction method of multipliers (ADMM) is proposed to solve the lasso
formulation. The resulting algorithm produces good and fast approximations of
the shortest path by sacrificing exactness that may not be absolutely essential
in many applications. Numerical experiments are provided to illustrate the
performance of the proposed approach.
"
"stat.TH","  Exchange algorithm is one of the most popular extensions of
Metropolis-Hastings algorithm to sample from doubly-intractable distributions.
However, theoretical exploration of exchange algorithm is very limited. For
example, natural questions like `Does exchange algorithm converge at a
geometric rate?' or `Does the exchange algorithm admit a Central Limit
Theorem?' have not been answered. In this paper, we study the theoretical
properties of exchange algorithm, in terms of asymptotic variance and
convergence speed. We compare the exchange algorithm with the original
Metropolis-Hastings algorithm and provide both necessary and sufficient
conditions for geometric ergodicity of the exchange algorithm, which can be
applied to various practical applications such as exponential random graph
models and Ising models. A central limit theorem for the exchange algorithm is
also established. Meanwhile, a concrete example, involving the Binomial model
with conjugate and non-conjugate priors, is treated in detail with sharp
convergence rates. Our results justify the theoretical usefulness of the
exchange algorithm.
"
"stat.TH","  Although statistical inference in stochastic differential equations (SDEs)
driven by Wiener process has received significant attention in the literature,
inference in those driven by fractional Brownian motion seem to have seen much
less development in comparison, despite their importance in modeling long range
dependence. In this article, we consider both classical and Bayesian inference
in such fractional Brownian motion based SDEs. In particular, we consider
asymptotic inference for two parameters in this regard; a multiplicative
parameter associated with the drift function, and the so-called ""Hurst
parameter"" of the fractional Brownian motion, when the time domain tends to
infinity. For unknown Hurst parameter, the likelihood does not lend itself
amenable to the popular Girsanov form, rendering usual asymptotic development
difficult. As such, we develop increasing domain infill asymptotic theory, by
discretizing the SDE. In this setup, we establish consistency and asymptotic
normality of the maximum likelihood estimators, as well as consistency and
asymptotic normality of the Bayesian posterior distributions. However,
classical or Bayesian asymptotic normality with respect to the Hurst parameter
could not be established. We supplement our theoretical investigations with
simulation studies in a non-asymptotic setup, prescribing suitable
methodologies for classical and Bayesian analyses of SDEs driven by fractional
Brownian motion.
"
"stat.TH","  A new generalization of the family of Poisson-G is called beta Poisson-G
family of distribution. Useful expansions of the probability density function
and the cumulative distribution function of the proposed family are derived and
seen as infinite mixtures of the Poisson-G distribution. Moment generating
function, power moments, entropy, quantile function, skewness and kurtosis are
investigated. Numerical computation of moments, skewness, kurtosis and entropy
are tabulated for select parameter values. Furthermore, estimation by methods
of maximum likelihood is discussed. A simulation study is carried at under
varying sample size to assess the performance of this model. Finally
suitability check of the proposed model in comparison to its recently
introduced models is carried out by considering two real life data sets
modeling.
"
"stat.TH","  We revisit the problem of \emph{missing mass concentration}, developing a new
method of estimating concentration of heterogenic sums, in spirit of celebrated
Rosenthal's inequality. As a result we slightly improve the state-of-art bounds
due to Ben-Hamou at al., and simplify the proofs.
"
"stat.TH","  Motivated by the problem of sampling from ill-conditioned log-concave
distributions, we give a clean non-asymptotic convergence analysis of
mirror-Langevin diffusions as introduced in Zhang et al. (2020). As a special
case of this framework, we propose a class of diffusions called Newton-Langevin
diffusions and prove that they converge to stationarity exponentially fast with
a rate which not only is dimension-free, but also has no dependence on the
target distribution. We give an application of this result to the problem of
sampling from the uniform distribution on a convex body using a strategy
inspired by interior-point methods. Our general approach follows the recent
trend of linking sampling and optimization and highlights the role of the
chi-squared divergence. In particular, it yields new results on the convergence
of the vanilla Langevin diffusion in Wasserstein distance.
"
"stat.TH","  We propose a new estimator for the change point parameter in a dynamic high
dimensional graphical model setting. We show that the proposed estimator
retains sufficient adaptivity against plugin estimates of the edge structure of
the underlying graphical models, in order to yield an $O(\psi^{-2})$ rate of
convergence of the change point estimator in the integer scale. This rate is
preserved while allowing high dimensionality as well as a diminishing jump size
$\psi,$ provided $s\log^{3/2}(p\vee T)=o\big(\surd(Tl_T)\big).$ Here $s,p,T$
and $l_T$ represent a sparsity parameter, model dimension, sampling period and
the separation of the change point from its parametric boundary, respectively.
Moreover, since the rate of convergence is free of $s,p$ and logarithmic terms
of $T,$ it allows the existence of a limiting distribution valid in the high
dimensional setting, which is then derived. The method does not assume an
underlying Gaussian distribution. Theoretical results are supported numerically
with monte carlo simulations.
"
"stat.TH","  We study the problem of a decision maker who must provide the best possible
treatment recommendation based on an experiment. The desirability of the
outcome distribution resulting from the policy recommendation is measured
through a functional capturing the distributional characteristic that the
decision maker is interested in optimizing. This could be, e.g., its inherent
inequality, welfare, level of poverty or its distance to a desired outcome
distribution. If the functional of interest is not quasi-convex or if there are
constraints, the optimal recommendation may be a mixture of treatments. This
vastly expands the set of recommendations that must be considered. We
characterize the difficulty of the problem by obtaining maximal expected regret
lower bounds. Furthermore, we propose two regret-optimal policies. The first
policy is static and thus applicable irrespectively of the subjects arriving
sequentially or not in the course of the experimental phase. The second policy
can utilize that subjects arrive sequentially by successively eliminating
inferior treatments and thus spends the sampling effort where it is most
needed.
"
"stat.TH","  The use of continuous probability distributions has been widespread in
problems with purely discrete nature. In general, such distributions are not
appropriate in this scenario. In this paper, we introduce a class of discrete
and asymmetric distributions based on the family of continuous log-symmetric
distributions. Some properties are discussed as well as estimation by the
maximum likelihood method. A Monte Carlo simulation study is carried out to
evaluate the performance of the estimators, and censored and uncensored data
sets are used to illustrate the proposed methodology.
"
"stat.TH","  A new type of robust estimation problem is introduced where the goal is to
recover a statistical model that has been corrupted after it has been estimated
from data. Methods are proposed for ""repairing"" the model using only the design
and not the response values used to fit the model in a supervised learning
setting. Theory is developed which reveals that two important ingredients are
necessary for model repair---the statistical model must be over-parameterized,
and the estimator must incorporate redundancy. In particular, estimators based
on stochastic gradient descent are seen to be well suited to model repair, but
sparse estimators are not in general repairable. After formulating the problem
and establishing a key technical lemma related to robust estimation, a series
of results are presented for repair of over-parameterized linear models, random
feature models, and artificial neural networks. Simulation studies are
presented that corroborate and illustrate the theoretical findings.
"
"stat.TH","  In this paper, we introduce a new smooth estimator for continuous
distribution functions on the positive real half-line using Szasz-Mirakyan
Operators. The approach is similar to the idea of the Bernstein estimator. We
show that the proposed estimator outperforms the empirical distribution
function in terms of asymptotic (integrated) mean-squared error, and generally
compares favourably with other competitors in theoretical comparisons and in a
simulation study.
"
"stat.TH","  Given a functional central limit (fCLT) and a parameter transformation, we
use the functional delta method to construct random processes, called
functional delta residuals, which asymptotically have the same covariance
structure as the transformed limit process. Moreover, we prove a multiplier
bootstrap fCLT theorem for these transformed residuals and show how this can be
used to construct simultaneous confidence bands for transformed functional
parameters. As motivation for this methodology, we provide the formal
application of these residuals to a functional version of the effect size
parameter Cohen's $d$, a problem appearing in current brain imaging
applications. The performance and necessity of such residuals is illustrated in
a simulation experiment for the covering rate of simultaneous confidence bands
for the functional Cohen's $d$ parameter.
"
"stat.TH","  This paper introduces the $(\alpha, \Gamma)$-descent, an iterative algorithm
which operates on measures and performs $\alpha$-divergence minimisation in a
Bayesian framework. This gradient-based procedure extends the commonly-used
variational approximation by adding a prior on the variational parameters in
the form of a measure. We prove that for a rich family of functions $\Gamma$,
this algorithm leads at each step to a systematic decrease in the
$\alpha$-divergence and derive convergence results. Our framework recovers the
Entropic Mirror Descent algorithm and provides an alternative algorithm that we
call the Power Descent. Moreover, in its stochastic formulation, the $(\alpha,
\Gamma)$-descent allows to optimise the mixture weights of any given mixture
model without any information on the underlying distribution of the variational
parameters. This renders our method compatible with many choices of parameters
updates and applicable to a wide range of Machine Learning tasks. We
demonstrate empirically on both toy and real-world examples the benefit of
using the Power descent and going beyond the Entropic Mirror Descent framework,
which fails as the dimension grows.
"
"stat.TH","  We study a prototypical problem in empirical Bayes. Namely, consider a
population consisting of $k$ individuals each belonging to one of $k$ types
(some types can be empty). Without any structural restrictions, it is
impossible to learn the composition of the full population having observed only
a small (random) subsample of size $m = o(k)$. Nevertheless, we show that in
the sublinear regime of $m =\omega(k/\log k)$, it is possible to consistently
estimate in total variation the \emph{profile} of the population, defined as
the empirical distribution of the sizes of each type, which determines many
symmetric properties of the population. We also prove that in the linear regime
of $m=c k$ for any constant $c$ the optimal rate is $\Theta(1/\log k)$. Our
estimator is based on Wolfowitz's minimum distance method, which entails
solving a linear program (LP) of size $k$. We show that there is a single
infinite-dimensional LP whose value simultaneously characterizes the risk of
the minimum distance estimator and certifies its minimax optimality. The sharp
convergence rate is obtained by evaluating this LP using complex-analytic
techniques.
"
"stat.TH","  We formalize the problem of detecting the presence of a botnet in a network
as an hypothesis testing problem where we observe a single instance of a graph.
The null hypothesis, corresponding to the absence of a botnet, is modeled as a
random geometric graph where every vertex is assigned a location on a
$d$-dimensional torus and two vertices are connected when their distance is
smaller than a certain threshold. The alternative hypothesis is similar, except
that there is a small number of vertices, called the botnet, that ignore this
geometric structure and simply connect randomly to every other vertex with a
prescribed probability.
  We present two tests that are able to detect the presence of such a botnet.
The first test is based on the idea that botnet vertices tend to form large
isolated stars that are not present under the null hypothesis. The second test
uses the average graph distance, which becomes significantly shorter under the
alternative hypothesis. We show that both these tests are asymptotically
optimal. However, numerical simulations show that the isolated star test
performs significantly better than the average distance test on networks of
moderate size. Finally, we construct a robust scheme based on the isolated star
test that is also able to identify the vertices in the botnet.
"
"stat.TH","  This paper studies the statistical and computational limits of high-order
clustering with planted structures. We focus on two clustering models, constant
high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and
study the methods and theory for testing whether a cluster exists (detection)
and identifying the support of cluster (recovery).
  Specifically, we identify the sharp boundaries of signal-to-noise ratio for
which CHC and ROHC detection/recovery are statistically possible. We also
develop the tight computational thresholds: when the signal-to-noise ratio is
below these thresholds, we prove that polynomial-time algorithms cannot solve
these problems under the computational hardness conjectures of hypergraphic
planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)
recovery. We also propose polynomial-time tensor algorithms that achieve
reliable detection and recovery when the signal-to-noise ratio is above these
thresholds. Both sparsity and tensor structures yield the computational
barriers in high-order tensor clustering. The interplay between them results in
significant differences between high-order tensor clustering and matrix
clustering in literature in aspects of statistical and computational phase
transition diagrams, algorithmic approaches, hardness conjecture, and proof
techniques. To our best knowledge, we are the first to give a thorough
characterization of the statistical and computational trade-off for such a
double computational-barrier problem. Finally, we provide evidence for the
computational hardness conjectures of HPC detection and HPDS recovery.
"
"stat.TH","  The large communication cost for exchanging gradients between different nodes
significantly limits the scalability of distributed training for large-scale
learning models. Motivated by this observation, there has been significant
recent interest in techniques that reduce the communication cost of distributed
Stochastic Gradient Descent (SGD), with gradient sparsification techniques such
as top-k and random-k shown to be particularly effective. The same observation
has also motivated a separate line of work in distributed statistical
estimation theory focusing on the impact of communication constraints on the
estimation efficiency of different statistical models. The primary goal of this
paper is to connect these two research lines and demonstrate how statistical
estimation models and their analysis can lead to new insights in the design of
communication-efficient training techniques. We propose a simple statistical
estimation model for the stochastic gradients which captures the sparsity and
skewness of their distribution. The statistically optimal communication scheme
arising from the analysis of this model leads to a new sparsification technique
for SGD, which concatenates random-k and top-k, considered separately in the
prior literature. We show through extensive experiments on both image and
language domains with CIFAR-10, ImageNet, and Penn Treebank datasets that the
concatenated application of these two sparsification methods consistently and
significantly outperforms either method applied alone.
"
"stat.TH","  We develop data processing inequalities that describe how Fisher information
from statistical samples can scale with the privacy parameter $\varepsilon$
under local differential privacy constraints. These bounds are valid under
general conditions on the distribution of the score of the statistical model,
and they elucidate under which conditions the dependence on $\varepsilon$ is
linear, quadratic, or exponential. We show how these inequalities imply order
optimal lower bounds for private estimation for both the Gaussian location
model and discrete distribution estimation for all levels of privacy
$\varepsilon>0$. We further apply these inequalities to sparse Bernoulli models
and demonstrate privacy mechanisms and estimators with order-matching squared
$\ell^2$ error.
"
"stat.TH","  We study statistical and computational limits of clustering when the means of
the centres are sparse and their dimension is possibly much larger than the
sample size. Our theoretical analysis focuses on the simple model $X_i = z_i
\theta + \varepsilon_i$, $z_i \in \{-1,1\}$, $\varepsilon_i \thicksim
\mathcal{N}(0, I)$, which has two clusters with centres $\theta$ and $-\theta$.
  We provide a finite sample analysis of a new sparse clustering algorithm
based on sparse PCA and show that it achieves the minimax optimal misclustering
rate in the regime $\|\theta\| \rightarrow \infty$, matching asymptotically the
Bayes error.
  Our results require the sparsity to grow slower than the square root of the
sample size. Using a recent framework for computational lower bounds---the
low-degree likelihood ratio---we give evidence that this condition is necessary
for any polynomial-time clustering algorithm to succeed below the BBP
threshold. This complements existing evidence based on reductions and
statistical query lower bounds. Compared to these existing results, we cover a
wider set of parameter regimes and give a more precise understanding of the
runtime required and the misclustering error achievable.
  We also discuss extensions of our results to more than two clusters.
"
"stat.TH","  We introduce an estimation method for the scaled skewness coefficient of the
sample mean of short and long memory linear processes. This method can be
extended to estimate higher moments such as curtosis coefficient of the sample
mean. Also a general result on computing all asymptotic moments of partial sums
is obtained, allowing in particular a much easier derivation of some existing
central limit theorems for linear processes. The introduced skewness estimator
provides a tool to empirically examine the error of the central limit theorem
for long and short memory linear processes. We also show that, for both short
and long memory linear processes, the skewness coefficient of the sample mean
converges to zero at the same rate as in the i.i.d. case.
"
"stat.TH","  Hypergeometric functions and zonal polynomials are the tools usually
addressed in the literature to deal with the expected value of the elementary
symmetric functions in non-central Wishart latent roots. The method here
proposed recovers the expected value of these symmetric functions by using the
umbral operator applied to the trace of suitable polynomial matrices and their
cumulants. The employment of a suitable linear operator in place of
hypergeometric functions and zonal polynomials was conjectured by de Waal in
1972. Here we show how the umbral operator accomplishes this task and
consequently represents an alternative tool to deal with these symmetric
functions. When special formal variables are plugged in the variables, the
evaluation through the umbral operator deletes all the monomials in the latent
roots except those contributing in the elementary symmetric functions.
Cumulants further simplify the computations taking advantage of the convolution
structure of the polynomial trace. Open problems are addressed at the end of
the paper.
"
"stat.TH","  The saddlepoint approximation gives an approximation to the density of a
random variable in terms of its moment generating function. When the underlying
random variable is itself the sum of $n$ unobserved i.i.d. terms, the basic
classical result is that the relative error in the density is of order $1/n$.
If instead the approximation is interpreted as a likelihood and maximized as a
function of model parameters, the result is an approximation to the maximum
likelihood estimator (MLE) that is often much faster to compute than the true
MLE. This paper proves the analogous basic result for the approximation error
between the saddlepoint MLE and the true MLE: it is of order $1/n^2$.
  The proof is based on a factorization of the saddlepoint likelihood into an
exact and approximate term, along with an analysis of the approximation error
in the gradient of the log-likelihood. This factorization also gives insight
into alternatives to the saddlepoint approximation, including a new and simpler
saddlepoint approximation, for which we derive analogous error bounds. In
addition, we prove asymptotic central limit theorem results for the sampling
distribution of the saddlepoint MLE and for the Bayesian posterior distribution
based on the saddlepoint likelihood. Notably, in the asymptotic regime that we
consider, the difference between the true and approximate MLEs is negligible
compared to the asymptotic size of the confidence region for the MLE. In
particular, the true MLE and the saddlepoint MLE have the same asymptotic
coverage properties, and the saddlepoint MLE can be used as a readily
calculated substitute when the true MLE is difficult to compute.
"
"stat.TH","  We study supervised learning by extreme learning machines and regression for
autonomous objects moving in a non-stationary spatial environment. In general,
this results in non-stationary data in contrast to the i.i.d. sampling
typically studied in learning theory. The stochastic model for the environment
and data collection especially allows for algebraically decaying weak
dependence and spatial heterogeneity, for example induced by interactions of
the object with sources of randomness spread over the spatial domain. Both
least squares and ridge learning as a computationally cheap regularization
method is studied. Consistency and asymptotic normality of the least squares
and ridge regression estimates is shown under weak conditions. The results also
cover consistency in terms of bounds for the sample squared predicition error.
Lastly, we discuss a resampling method to compute confidence regions.
"
"stat.TH","  In the common time series model $X_{i,n} = \mu (i/n) + \varepsilon_{i,n}$
with non-stationary errors we consider the problem of detecting a significant
deviation of the mean function $\mu$ from a benchmark $g (\mu )$ (such as the
initial value $\mu (0)$ or the average trend $\int_{0}^{1} \mu (t) dt$). The
problem is motivated by a more realistic modelling of change point analysis,
where one is interested in identifying relevant deviations in a smoothly
varying sequence of means $ (\mu (i/n))_{i =1,\ldots ,n }$ and cannot assume
that the sequence is piecewise constant. A test for this type of hypotheses is
developed using an appropriate estimator for the integrated squared deviation
of the mean function and the threshold. By a new concept of self-normalization
adapted to non-stationary processes an asymptotically pivotal test for the
hypothesis of a relevant deviation is constructed. The results are illustrated
by means of a simulation study and a data example.
"
"stat.TH","  In compressed sensing, the restricted isometry property (RIP) on $M \times N$
sensing matrices (where $M < N$) guarantees efficient reconstruction of sparse
vectors. A matrix has the $(s,\delta)$-$\mathsf{RIP}$ property if behaves as a
$\delta$-approximate isometry on $s$-sparse vectors. It is well known that an
$M\times N$ matrix with i.i.d. $\mathcal{N}(0,1/M)$ entries is
$(s,\delta)$-$\mathsf{RIP}$ with high probability as long as $s\lesssim
\delta^2 M/\log N$. On the other hand, most prior works aiming to
deterministically construct $(s,\delta)$-$\mathsf{RIP}$ matrices have failed
when $s \gg \sqrt{M}$. An alternative way to find an RIP matrix could be to
draw a random gaussian matrix and certify that it is indeed RIP. However, there
is evidence that this certification task is computationally hard when $s \gg
\sqrt{M}$, both in the worst case and the average case.
  In this paper, we investigate the exact average-case time complexity of
certifying the RIP property for $M\times N$ matrices with i.i.d.
$\mathcal{N}(0,1/M)$ entries, in the ""possible but hard"" regime $\sqrt{M} \ll
s\lesssim M/\log N$, assuming that $M$ scales proportional to $N$. Based on
analysis of the low-degree likelihood ratio, we give rigorous evidence that
subexponential runtime $N^{\tilde\Omega(s^2/N)}$ is required, demonstrating a
smooth tradeoff between the maximum tolerated sparsity and the required
computational power. The lower bound is essentially tight, matching the runtime
of an existing algorithm due to Koiran and Zouzias. Our hardness result allows
$\delta$ to take any constant value in $(0,1)$, which captures the relevant
regime for compressed sensing. This improves upon the existing average-case
hardness result of Wang, Berthet, and Plan, which is limited to $\delta =
o(1)$.
"
"stat.TH","  Inverse probability weighted estimators are the oldest and potentially most
commonly used class of procedures for the estimation of causal effects. By
adjusting for selection biases via a weighting mechanism, these procedures
estimate an effect of interest by constructing a pseudo-population in which
selection biases are eliminated. Despite their ease of use, these estimators
require the correct specification of a model for the weighting mechanism, are
known to be inefficient, and suffer from the curse of dimensionality. We
propose a class of nonparametric inverse probability weighted estimators in
which the weighting mechanism is estimated via undersmoothing of the highly
adaptive lasso, a nonparametric regression function proven to converge at
$n^{-1/3}$-rate to the true weighting mechanism. We demonstrate that our
estimators are asymptotically linear with variance converging to the
nonparametric efficiency bound. Unlike doubly robust estimators, our procedures
require neither derivation of the efficient influence function nor
specification of the conditional outcome model. Our theoretical developments
have broad implications for the construction of efficient inverse probability
weighted estimators in large statistical models and a variety of problem
settings. We assess the practical performance of our estimators in simulation
studies and demonstrate use of our proposed methodology with data from a
large-scale epidemiologic study.
"
"stat.TH","  Hierarchical modeling and learning has proven very powerful in the field of
Gaussian process regression and kernel methods, especially for machine learning
applications and, increasingly, within the field of inverse problems more
generally. The classical approach to learning hierarchical information is
through Bayesian formulations of the problem, implying a posterior distribution
on the hierarchical parameters or, in the case of empirical Bayes, providing an
optimization criterion for them. Recent developments in the machine learning
literature have suggested new criteria for hierarchical learning, based on
approximation theoretic considerations that can be interpreted as variants of
cross-validation, and exploiting approximation consistency in data splitting.
The purpose of this paper is to compare the empirical Bayesian and
approximation theoretic approaches to hierarchical learning, in terms of large
data consistency, variance of estimators, robustness of the estimators to model
misspecification, and computational cost. Our analysis is rooted in the setting
of Mat\'ern-like Gaussian random field priors, with smoothness, amplitude and
inverse lengthscale as hierarchical parameters, in the regression setting.
Numerical experiments validate the theory and extend the scope of the paper
beyond the Mat\'ern setting.
"
"stat.TH","  Cluster indices describe extremal behaviour of stationary time series. We
consider their sliding blocks estimators. Using a modern theory of
multivariate, regularly varying time series, we obtain central limit theorems
under conditions that can be easily verified for a large class of models. In
particular, we show that in the Peak over Threshold framework, sliding and
disjoint blocks estimators have the same limiting variance.
"
"stat.TH","  Many statistical estimators are defined as the fixed point of a
data-dependent operator, with estimators based on minimizing a cost function
being an important special case. The limiting performance of such estimators
depends on the properties of the population-level operator in the idealized
limit of infinitely many samples. We develop a general framework that yields
bounds on statistical accuracy based on the interplay between the deterministic
convergence rate of the algorithm at the population level, and its degree of
(in)stability when applied to an empirical object based on $n$ samples. Using
this framework, we analyze both stable forms of gradient descent and some
higher-order and unstable algorithms, including Newton's method and its
cubic-regularized variant, as well as the EM algorithm. We provide applications
of our general results to several concrete classes of models, including
Gaussian mixture estimation, single-index models, and informative non-response
models. We exhibit cases in which an unstable algorithm can achieve the same
statistical accuracy as a stable algorithm in exponentially fewer
steps---namely, with the number of iterations being reduced from polynomial to
logarithmic in sample size $n$.
"
"stat.TH","  Information geometry uses the formal tools of differential geometry to
describe the space of probability distributions as a Riemannian manifold with
an additional dual structure. The formal equivalence of compositional data with
discrete probability distributions makes it possible to apply the same
description to the sample space of Compositional Data Analysis (CoDA). The
latter has been formally described as a Euclidean space with an orthonormal
basis featuring components that are suitable combinations of the original
parts. In contrast to the Euclidean metric, the information-geometric
description singles out the Fisher information metric as the only one keeping
the manifold's geometric structure invariant under equivalent representations
of the underlying random variables. Well-known concepts that are valid in
Euclidean coordinates, e.g., the Pythogorean theorem, are generalized by
information geometry to corresponding notions that hold for more general
coordinates. In briefly reviewing Euclidean CoDA and, in more detail, the
information-geometric approach, we show how the latter justifies the use of
distance measures and divergences that so far have received little attention in
CoDA as they do not fit the Euclidean geometry favored by current thinking. We
also show how entropy and relative entropy can describe amalgamations in a
simple way, while Aitchison distance requires the use of geometric means to
obtain more succinct relationships. We proceed to prove the information
monotonicity property for Aitchison distance. We close with some thoughts about
new directions in CoDA where the rich structure that is provided by information
geometry could be exploited.
"
"stat.TH","  The joint distribution $P(X,Y)$ cannot be determined from its marginals
$P(X)$ and $P(Y)$ alone; one also needs one of the conditionals $P(X|Y)$ or
$P(Y|X)$. But is there a best guess, given only the marginals? Here we answer
this question in the affirmative, obtaining in closed form the function of the
marginals that has the lowest expected Kullbach-Liebler (KL) divergence between
the unknown ""true"" joint probability and the function value. The expectation is
taken with respect to Jeffreys' non-informative prior over the possible joint
probability values, given the marginals. This distribution can also be used to
obtain the expected information loss for any other ""aggregation operator"", as
such estimators are often called in fuzzy logic, for any given pair of marginal
input values. This enables such such operators, including ours, to be compared
according to their expected loss under the minimal knowledge conditions we
assume. We go on to develop a method for evaluating the expected accuracy of
any aggregation operator in the absence of knowledge of its inputs. This
requires averaging the expected loss over all possible input pairs, weighted by
an appropriate distribution. We obtain this distribution by marginalizing
Jeffreys' prior over the possible joint distributions (over the 3 functionally
independent coordinates of the space of joint distributions over two Boolean
variables) onto a joint distribution over the pair of marginal distributions, a
2-dimensional space with one parameter for each marginal. We report the
resulting input-averaged expected losses for a few commonly used operators, as
well as the optimal operator. Finally, we discuss the potential to develop our
methodology into a principled risk management approach to replace the often
rather arbitrary conditional-independence assumptions made for probabilistic
graphical models.
"
"stat.TH","  Estimation of the number of components (or order) of a finite mixture model
is a long standing and challenging problem in statistics. We propose the
Group-Sort-Fuse (GSF) procedure---a new penalized likelihood approach for
simultaneous estimation of the order and mixing measure in multidimensional
finite mixture models. Unlike methods which fit and compare mixtures with
varying orders using criteria involving model complexity, our approach directly
penalizes a continuous function of the model parameters. More specifically,
given a conservative upper bound on the order, the GSF groups and sorts mixture
component parameters to fuse those which are redundant. For a wide range of
finite mixture models, we show that the GSF is consistent in estimating the
true mixture order and achieves the $n^{-1/2}$ convergence rate for parameter
estimation up to polylogarithmic factors. The GSF is implemented for several
univariate and multivariate mixture models in the R package GroupSortFuse. Its
finite sample performance is supported by a thorough simulation study, and its
application is illustrated on two real data examples.
"
"stat.TH","  In the context of regression, we consider the fundamental question of making
an estimator fair while preserving its prediction accuracy as much as possible.
To that end, we define its projection to fairness as its closest fair estimator
in a sense that reflects prediction accuracy. Our methodology leverages tools
from optimal transport to construct efficiently the projection to fairness of
any given estimator as a simple post-processing step. Moreover, our approach
precisely quantifies the cost of fairness, measured in terms of prediction
accuracy.
"
"stat.TH","  Thesis is devoted to the application of cumulant analysis in the estimation
of impulse response functions for continuous time-invariant linear systems,
including systems with inner noises. The main assumption of the work is the
second-order integration of the impulse response function. Our study deals with
cumulant analysis of sample cross-correlograms between stationary Gaussian
stochastic processes. An important role was played by integral representations
for the higher-order cumulants of these second-order statistics. Using the
diagram formula, all representations are reduced to the finite sums of
integrals involving cyclic products of kernels. In the work we proved the
convergence to zero of the corresponding integrals. Then, since the Gaussian
distribution is uniquelly determined by its cumulants and also all higher-order
cumulants of the estimators tend to zero, we establish the asymptotic normality
of the integral-type cross-correlogram estimators.
"
"stat.TH","  We study the denoising of low-rank matrices by singular value shrinkage.
Recent work of Gavish and Donoho constructs a framework for finding optimal
singular value shrinkers for a wide class of loss functions. We use this
framework to derive the optimal shrinker for operator norm loss. The optimal
shrinker matches the shrinker proposed by Gavish and Donoho in the special case
of square matrices, but differs for all other aspect ratios. We precisely
quantify the gain in accuracy from using the optimal shrinker. We also show
that the optimal shrinker converges to the best linear predictor in the
classical regime of aspect ratio zero.
"
"stat.TH","  A traditional approach to initialization in deep neural networks (DNNs) is to
sample the network weights randomly for preserving the variance of
pre-activations. On the other hand, several studies show that during the
training process, the distribution of stochastic gradients can be heavy-tailed
especially for small batch sizes. In this case, weights and therefore
pre-activations can be modeled with a heavy-tailed distribution that has an
infinite variance but has a finite (non-integer) fractional moment of order $s$
with $s<2$. Motivated by this fact, we develop initialization schemes for fully
connected feed-forward networks that can provably preserve any given moment of
order $s \in (0, 2]$ over the layers for a class of activations including ReLU,
Leaky ReLU, Randomized Leaky ReLU, and linear activations. These generalized
schemes recover traditional initialization schemes in the limit $s \to 2$ and
serve as part of a principled theory for initialization. For all these schemes,
we show that the network output admits a finite almost sure limit as the number
of layers grows, and the limit is heavy-tailed in some settings. This sheds
further light into the origins of heavy tail during signal propagation in DNNs.
We prove that the logarithm of the norm of the network outputs, if properly
scaled, will converge to a Gaussian distribution with an explicit mean and
variance we can compute depending on the activation used, the value of s chosen
and the network width. We also prove that our initialization scheme avoids
small network output values more frequently compared to traditional approaches.
Furthermore, the proposed initialization strategy does not have an extra cost
during the training procedure. We show through numerical experiments that our
initialization can improve the training and test performance.
"
"stat.TH","  We study the eigenvalue distributions of the Conjugate Kernel and Neural
Tangent Kernel associated to multi-layer feedforward neural networks. In an
asymptotic regime where network width is increasing linearly in sample size,
under random initialization of the weights, and for input samples satisfying a
notion of approximate pairwise orthogonality, we show that the eigenvalue
distributions of the CK and NTK converge to deterministic limits. The limit for
the CK is described by iterating the Marcenko-Pastur map across the hidden
layers. The limit for the NTK is equivalent to that of a linear combination of
the CK matrices across layers, and may be described by recursive fixed-point
equations that extend this Marcenko-Pastur map. We demonstrate the agreement of
these asymptotic predictions with the observed spectra for both synthetic and
CIFAR-10 training data, and we perform a small simulation to investigate the
evolutions of these spectra over training.
"
"stat.TH","  The advent of modern data collection and processing techniques has seen the
size, scale, and complexity of data grow exponentially. A seminal step in
leveraging these rich datasets for downstream inference is understanding the
characteristics of the data which are repeatable -- the aspects of the data
that are able to be identified under a duplicated analysis. Conflictingly, the
utility of traditional repeatability measures, such as the intraclass
correlation coefficient, under these settings is limited. In recent work, novel
data repeatability measures have been introduced in the context where a set of
subjects are measured twice or more, including: fingerprinting, rank sums, and
generalizations of the intraclass correlation coefficient. However, the
relationships between, and the best practices among these measures remains
largely unknown. In this manuscript, we formalize a novel repeatability
measure, discriminability. We show that it is deterministically linked with the
correlation coefficient under univariate random effect models, and has desired
property of optimal accuracy for inferential tasks using multivariate
measurements. Additionally, we overview and systematically compare
repeatability statistics using both theoretical results and simulations. We
show that the rank sum statistic is deterministically linked to a consistent
estimator of discriminability. The power of permutation tests derived from
these measures are compared numerically under Gaussian and non-Gaussian
settings, with and without simulated batch effects. Motivated by both
theoretical and empirical results, we provide methodological recommendations
for each benchmark setting to serve as a resource for future analyses. We
believe these recommendations will play an important role towards improving
repeatability in fields such as functional magnetic resonance imaging,
genomics, pharmacology, and more.
"
"stat.TH","  A reinforcement algorithm introduced by H.A. Simon \cite{Simon} produces a
sequence of uniform random variables with memory as follows. At each step, with
a fixed probability $p\in(0,1)$, $\hat U_{n+1}$ is sampled uniformly from $\hat
U_1, \ldots, \hat U_n$, and with complementary probability $1-p$, $\hat
U_{n+1}$ is a new independent uniform variable. The Glivenko-Cantelli theorem
remains valid for the reinforced empirical measure, but not the Donsker
theorem. Specifically, we show that the sequence of empirical processes
converges in law to a Brownian bridge only up to a constant factor when
$p<1/2$, and that a further rescaling is needed when $p>1/2$ and the limit is
then a bridge with exchangeable increments and discontinuous paths. This is
related to earlier limit theorems for correlated Bernoulli processes, the
so-called elephant random walk, and more generally step reinforced random
walks.
"
"stat.TH","  We consider the problem of variable screening in ultra-high dimensional (of
non-polynomial order) generalized linear models (GLMs). Since the popular SIS
approach is extremely unstable in the presence of contamination and noises,
which may frequently arise in the large scale sample data (e.g., Omics data),
we discuss a new robust screening procedure based on the minimum density power
divergence estimator (MDPDE) of the marginal regression coefficients. Our
proposed screening procedure performs extremely well both under pure and
contaminated data scenarios. We also theoretically justify the use of this
marginal MDPDEs for variable screening from the population as well as sample
aspects; in particular, we prove that these marginal MDPDEs are uniformly
consistent leading to the sure screening property of our proposed algorithm. We
have also proposed an appropriate MDPDE based extension for robust conditional
screening in the GLMs along with the derivation of its sure screening property.
"
"stat.TH","  We consider integer-valued GARCH processes, where the count variable
conditioned on past values of the count and state variables follows a so-called
Skellam distribution. Using arguments for contractive Markov chains we prove
that the process has a unique stationary regime. Furthermore, we show
asymptotic regularity ($\beta$-mixing) with geometrically decaying coefficients
for the count process. These probabilistic results are complemented by a
statistical analysis, a few simulations as well as an application to recent
COVID-19 data.
"
"stat.TH","  We consider the matrix completion problem of recovering a structured low rank
matrix with partially observed entries with mixed data types. Vast majority of
the solutions have proposed computationally feasible estimators with strong
statistical guarantees for the case where the underlying distribution of data
in the matrix is continuous. A few recent approaches have extended using
similar ideas these estimators to the case where the underlying distributions
belongs to the exponential family. Most of these approaches assume that there
is only one underlying distribution and the low rank constraint is regularized
by the matrix Schatten Norm. We propose a computationally feasible statistical
approach with strong recovery guarantees along with an algorithmic framework
suited for parallelization to recover a low rank matrix with partially observed
entries for mixed data types in one step. We also provide extensive simulation
evidence that corroborate our theoretical results.
"
"stat.TH","  We investigate Bayes estimation of a normal mean matrix under the matrix
quadratic loss, which is viewed as a class of loss functions including the
Frobenius loss and quadratic loss for each column. First, we derive an unbiased
estimate of risk and show that the Efron--Morris estimator is minimax. Next, we
introduce a notion of matrix superharmonicity for matrix-variate functions and
show that it has analogous properties with usual superharmonic functions, which
may be of independent interest. Then, we show that the generalized Bayes
estimator with respect to a matrix superharmonic prior is minimax. We also
provide a class of matrix superharmonic priors that include the previously
proposed generalization of Stein's prior. Numerical results demonstrate that
matrix superharmonic priors work well for low rank matrices.
"
"stat.TH","  We find separation rates for testing multinomial or more general discrete
distributions under the constraint of local differential privacy. We construct
efficient randomized algorithms and test procedures, in both the case where
only non-interactive privacy mechanisms are allowed and also in the case where
all sequentially interactive privacy mechanisms are allowed. The separation
rates are faster in the latter case. We prove general information theoretical
bounds that allow us to establish the optimality of our algorithms among all
pairs of privacy mechanisms and test procedures, in most usual cases.
Considered examples include testing uniform, polynomially and exponentially
decreasing distributions.
"
"stat.TH","  We consider the problem of modelling restricted interactions between
continuously-observed time series as given by a known static graph (or network)
structure. For this purpose, we define a parametric multivariate Graph
Ornstein-Uhlenbeck (GrOU) process driven by a general \Levy process to study
the momentum and network effects amongst nodes. We distinguish the cases of the
\emph{network-level} GrOU and the \emph{node-level} GrOU processes where the
latter allows for the directed graph edges to be node-dependent. Given general
likelihood frameworks, we derive maximum likelihood estimators and their usual
properties (existence, uniqueness, consistency and efficiency). To quantify the
estimation uncertainty, we present two novel central limit theorems under
general assumptions with closed-form covariance matrices as the time horizon
goes to infinity. Finally, we extend the \levy-driven case to include a
stochastic volatility modulation term and show that the central limit theorems
still hold.
"
"stat.TH","  We investigate the sample efficiency of reinforcement learning in a
$\gamma$-discounted infinite-horizon Markov decision process (MDP) with state
space $\mathcal{S}$ and action space $\mathcal{A}$, assuming access to a
generative model. Despite a number of prior work tackling this problem, a
complete picture of the trade-offs between sample complexity and statistical
accuracy is yet to be determined. In particular, prior results suffer from a
sample size barrier, in the sense that their claimed statistical guarantees
hold only when the sample size exceeds at least
$\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)^2}$ (up to some log factor). The
current paper overcomes this barrier by certifying the minimax optimality of
model-based reinforcement learning as soon as the sample size exceeds the order
of $\frac{|\mathcal{S}||\mathcal{A}|}{1-\gamma}$ (modulo some log factor). More
specifically, a perturbed model-based planning algorithm provably finds an
$\varepsilon$-optimal policy with an order of $\frac{|\mathcal{S}||\mathcal{A}|
}{(1-\gamma)^3\varepsilon^2}\log\frac{|\mathcal{S}||\mathcal{A}|}{(1-\gamma)\varepsilon}$
samples for any $\varepsilon \in (0, \frac{1}{1-\gamma}]$. Along the way, we
derive improved (instance-dependent) guarantees for model-based policy
evaluation. To the best of our knowledge, this work provides the first
minimax-optimal guarantee in a generative model that accommodates the entire
range of sample sizes (beyond which finding a meaningful policy is information
theoretically impossible).
"
"stat.TH","  Classical tests of goodness-of-fit aim to validate the conformity of a
postulated model to the data under study. Given their inferential nature, they
can be considered a crucial step in confirmatory data analysis. In their
standard formulation, however, they do not allow exploring how the hypothesized
model deviates from the truth nor do they provide any insight into how the
rejected model could be improved to better fit the data. The main goal of this
work is to establish a comprehensive framework for goodness-of-fit which
naturally integrates modeling, estimation, inference, and graphics. Modeling
and estimation focus on a novel formulation of smooth tests that easily extends
to arbitrary distributions, either continuous or discrete. Inference and
adequate post-selection adjustments are performed via a specially designed
smoothed bootstrap and the results are summarized via an exhaustive graphical
tool called CD-plot.
"
"stat.TH","  Delayed rewards problem in contextual bandits has been of interest in various
practical settings. We study randomized allocation strategies and provide an
understanding on how the exploration-exploitation tradeoff is affected by
delays in observing the rewards. In randomized strategies, the extent of
exploration-exploitation is controlled by a user-determined exploration
probability sequence. In the presence of delayed rewards, one may choose
between using the original exploration sequence that updates at every time
point or update the sequence only when a new reward is observed, leading to two
competing strategies. In this work, we show that while both strategies may lead
to strong consistency in allocation, the property holds for a wider scope of
situations for the latter. However, for finite sample performance, we
illustrate that both strategies have their own advantages and disadvantages,
depending on the severity of the delay and underlying reward generating
mechanisms.
"
"stat.TH","  In this paper we study the asymptotic theory for spectral analysis of
stationary random fields, including linear and nonlinear fields. Asymptotic
properties of Fourier coefficients and periodograms, including limiting
distributions of Fourier coefficients, and the uniform consistency of kernel
spectral density estimators are obtained under various mild conditions on
moments and dependence structures. The validity of the aforementioned
asymptotic results for estimated spatial fields is also established.
"
"stat.TH","  In order to identify the infected individuals of a population, their samples
are divided in equally sized groups called pools and a single laboratory test
is applied to each pool. Individuals whose samples belong to pools that test
negative are declared healthy, while each pool that tests positive is divided
into smaller, equally sized pools which are tested in the next stage. This
scheme is called adaptive, because the composition of the pools at each stage
depends on results from previous stages, and nested because each pool is a
subset of a pool of the previous stage. Is the infection probability $p$ is not
smaller than $1-3^{-1/3}$ it is best to test each sample (no pooling). If
$p<1-3^{-1/3}$, we compute the mean $D_k(m,p)$ and the variance of the number
of tests per individual as a function of the pool sizes $m=(m_1,\dots,m_k)$ in
the first $k$ stages; in the $(k+1)$-th stage all remaining samples are tested.
The case $k=1$ was proposed by Dorfman in his seminal paper in 1943. The goal
is to minimize $D_k(m,p)$, which is called the cost associated to~$m$. We show
that for $p\in (0, 1-3^{-1/3})$ the optimal choice is one of four possible
schemes, which are explicitly described. For $p>2^{-51}$ we show overwhelming
numerical evidence that the best choice is $(3^k\text{ or
}3^{k-1}4,3^{k-1},\dots,3^2,3 )$, with a precise description of the range of
$p$'s where each holds. We then focus on schemes of the type $(3^k,\dots,3)$,
and estimate that the cost of the best scheme of this type for $p$, determined
by the choice of $k=k_3(p)$, is of order $O\big(p\log(1/p)\big)$. This is the
same order as that of the cost of the optimal scheme, and the difference of
these costs is explicitly bounded. As an example, for $p=0.02$ the optimal
choice is $k=3$, $m=(27,9,3)$, with cost $0.20$; that is, the mean number of
tests required to screen 100 individuals is 20.
"
"stat.TH","  We propose two new kernel-type estimators of the mean residual life function
$m_X(t)$ of bounded or half-bounded interval supported distributions. Though
not as severe as the boundary problems in the kernel density estimation,
eliminating the boundary bias problems that occur in the naive kernel estimator
of the mean residual life function is needed. In this article, we utilize the
property of bijective transformation. Furthermore, our proposed methods
preserve the mean value property, which cannot be done by the naive kernel
estimator. Some simulation results showing the estimators' performances and a
real data analysis will be presented in the last part of this article.
"
"stat.TH","  The Hawks process is a point process with a self-exciting property. It has
been used to model earthquakes, social media events, infections, etc., and is
getting a lot of attention. However, as a real problem, there are often
situations where we can not obtain data with sufficient observation time. In
such cases, it is not appropriate to approximate the error distribution of an
estimator by the normal distribution. To overcome this problem, we provide a
rigorous mathematical foundation of the theory for the higher-order asymptotic
behavior of the one-dimensional Hawkes process with an exponential kernel. As
an important application, we give the second-order asymptotic distribution for
the maximum likelihood estimator of the exponential Hawkes process.
Furthermore, we also present the simulation results.
"
"stat.TH","  We consider an RCAR$(p)$ process and we establish that the standard
estimation lacks consistency as soon as there exists a nonzero serial
correlation in the coefficients. We give the correct asymptotic behavior and
some simulations come to illustrate the results.
"
"stat.TH","  We explore why many recently proposed robust estimation problems are
efficiently solvable, even though the underlying optimization problems are
non-convex. We study the loss landscape of these robust estimation problems,
and identify the existence of ""generalized quasi-gradients"". Whenever these
quasi-gradients exist, a large family of low-regret algorithms are guaranteed
to approximate the global minimum; this includes the commonly-used filtering
algorithm.
  For robust mean estimation of distributions under bounded covariance, we show
that any first-order stationary point of the associated optimization problem is
an {approximate global minimum} if and only if the corruption level $\epsilon <
1/3$. Consequently, any optimization algorithm that aproaches a stationary
point yields an efficient robust estimator with breakdown point $1/3$. With
careful initialization and step size, we improve this to $1/2$, which is
optimal.
  For other tasks, including linear regression and joint mean and covariance
estimation, the loss landscape is more rugged: there are stationary points
arbitrarily far from the global minimum. Nevertheless, we show that generalized
quasi-gradients exist and construct efficient algorithms. These algorithms are
simpler than previous ones in the literature, and for linear regression we
improve the estimation error from $O(\sqrt{\epsilon})$ to the optimal rate of
$O(\epsilon)$ for small $\epsilon$ assuming certified hypercontractivity. For
mean estimation with near-identity covariance, we show that a simple gradient
descent algorithm achieves breakdown point $1/3$ and iteration complexity
$\tilde{O}(d/\epsilon^2)$.
"
"stat.TH","  The research described herewith is to re-visit the classical doubly robust
estimation of average treatment effect by conducting a systematic study on the
comparisons, in the sense of asymptotic efficiency, among all possible
combinations of the estimated propensity score and outcome regression. To this
end, we consider all nine combinations under, respectively, parametric,
nonparametric and semiparametric structures. The comparisons provide useful
information on when and how to efficiently utilize the model structures in
practice. Further, when there is model-misspecification, either propensity
score or outcome regression, we also give the corresponding comparisons. Three
phenomena are observed. Firstly, when all models are correctly specified, any
combination can achieve the same semiparametric efficiency bound, which
coincides with the existing results of some combinations. Secondly, when the
propensity score is correctly modeled and estimated, but the outcome regression
is misspecified parametrically or semiparametrically, the asymptotic variance
is always larger than or equal to the semiparametric efficiency bound. Thirdly,
in contrast, when the propensity score is misspecified parametrically or
semiparametrically, while the outcome regression is correctly modeled and
estimated, the asymptotic variance is not necessarily larger than the
semiparametric efficiency bound. In some cases, the ""super-efficiency""
phenomenon occurs. We also conduct a small numerical study.
"
"stat.TH","  We study sufficient conditions for a local asymptotic mixed normality
property of statistical models. We develop a scheme with the $L^2$ regularity
condition proposed by Jeganathan [\textit{Sankhya Ser. A} \textbf{44} (1982)
173--212] so that it is applicable to high-frequency observations of stochastic
processes. Moreover, by combining with Malliavin calculus techniques by Gobet
[\textit{Bernoulli} \textbf{7} (2001) 899--912, 2001], we introduce tractable
sufficient conditions for smooth observations in the Malliavin sense, which do
not require Aronson-type estimates of the transition density function. Our
results, unlike those in the literature, can be applied even when the
transition density function has zeros. For an application, we show the local
asymptotic mixed normality property of degenerate (hypoelliptic) diffusion
models under high-frequency observations, in both complete and partial
observation frameworks. The former and the latter extend previous results for
elliptic diffusions and for integrated diffusions, respectively.
"
"stat.TH","  Many Bayesian inference problems involve target distributions whose density
functions are computationally expensive to evaluate. Replacing the target
density with a local approximation based on a small number of carefully chosen
density evaluations can significantly reduce the computational expense of
Markov chain Monte Carlo (MCMC) sampling. Moreover, continual refinement of the
local approximation can guarantee asymptotically exact sampling. We devise a
new strategy for balancing the decay rate of the bias due to the approximation
with that of the MCMC variance. We prove that the error of the resulting local
approximation MCMC (LA-MCMC) algorithm decays at roughly the expected
$1/\sqrt{T}$ rate, and we demonstrate this rate numerically. We also introduce
an algorithmic parameter that guarantees convergence given very weak tail
bounds, significantly strengthening previous convergence results. Finally, we
apply LA-MCMC to a computationally intensive Bayesian inverse problem arising
in groundwater hydrology.
"
"stat.TH","  We consider the problem of estimating the scale matrix $\Sigma$ of the
additif model $Y_{p\times n} = M + \mathcal{E}$, under a theoretical decision
point of view. Here, $ p $ is the number of variables, $ n$ is the number of
observations, $ M $ is a matrix of unknown parameters with rank $q<p$ and $
\mathcal {E}$ is a random noise, whose distribution is elliptically symmetric
with covariance matrix proportional to $ I_n \otimes \Sigma $\,. We deal with a
canonical form of this model where $Y$ is decomposed in two matrices, namely,
$Z_{q\times p}$ which summarizes the information contained in $ M $, and $
U_{m\times p}$, where $m=n-q$, which summarizes the sufficient information to
estimate $ \Sigma $. As the natural estimators of the form ${\hat
{\Sigma}}_a=a\, S$ (where $ S=U^{T}\,U$ and $a$ is a positive constant) perform
poorly when $p >m$ (S non-invertible), we propose estimators of the form
${\hat{\Sigma}}_{a, G} = a\big( S+ S \, {S^{+}\,G(Z,S)}\big)$ where ${S^{+}}$
is the Moore-Penrose inverse of $ S$ (which coincides with $S^{-1}$ when $S$ is
invertible). We provide conditions on the correction matrix $SS^{+}{G(Z,S)}$
such that ${\hat {\Sigma}}_{a, G}$ improves over ${\hat {\Sigma}}_a$ under the
data-based loss $L _S( \Sigma, \hat { \Sigma}) ={\rm tr} \big (
S^{+}\Sigma\,({\hat{\Sigma}} \, {\Sigma} ^ {- 1} - {I}_ {p} )^ {2}\big) $. We
adopt a unified approach of the two cases where $ S$ is invertible ($p \leq m$)
and $ S$ is non-invertible ($p>m$).
"
"stat.TH","  It is a common phenomenon that for high-dimensional and nonparametric
statistical models, rate-optimal estimators balance squared bias and variance.
Although this balancing is widely observed, little is known whether methods
exist that could avoid the trade-off between bias and variance. We propose a
general strategy to obtain lower bounds on the variance of any estimator with
bias smaller than a prespecified bound. This shows to which extent the
bias-variance trade-off is unavoidable and allows to quantify the loss of
performance for methods that do not obey it. The approach is based on a number
of abstract lower bounds for the variance involving the change of expectation
with respect to different probability measures as well as information measures
such as the Kullback-Leibler or chi-square divergence. Some of these
inequalities rely on a new concept of information matrices. In a second part of
the article, the abstract lower bounds are applied to several statistical
models including the Gaussian white noise model, a boundary estimation problem,
the Gaussian sequence model and the high-dimensional linear regression model.
For these specific statistical applications, different types of bias-variance
trade-offs occur that vary considerably in their strength. For the trade-off
between integrated squared bias and integrated variance in the Gaussian white
noise model, we propose to combine the general strategy for lower bounds with a
reduction technique. This allows us to reduce the original problem to a lower
bound on the bias-variance trade-off for estimators with additional symmetry
properties in a simpler statistical model. To highlight possible extensions of
the proposed framework, we moreover briefly discuss the trade-off between bias
and mean absolute deviation.
"
"stat.TH","  Neural networks have become standard tools in the analysis of data, but they
lack comprehensive mathematical theories. For example, there are very few
statistical guarantees for learning neural networks from data, especially for
classes of estimators that are used in practice or at least similar to such. In
this paper, we develop a general statistical guarantee for estimators that
consist of a least-squares term and a regularizer. We then exemplify this
guarantee with $\ell_1$-regularization, showing that the corresponding
prediction error increases at most sub-linearly in the number of layers and at
most logarithmically in the total number of parameters. Our results establish a
mathematical basis for regularized estimation of neural networks, and they
deepen our mathematical understanding of neural networks and deep learning more
generally.
"
"stat.TH","  One main obstacle for the wide use of deep learning in medical and
engineering sciences is its interpretability. While neural network models are
strong tools for making predictions, they often provide little information
about which features play significant roles in influencing the prediction
accuracy. To overcome this issue, many regularization procedures for learning
with neural networks have been proposed for dropping non-significant features.
Unfortunately, the lack of theoretical results casts doubt on the applicability
of such pipelines. In this work, we propose and establish a theoretical
guarantee for the use of the adaptive group lasso for selecting important
features of neural networks. Specifically, we show that our feature selection
method is consistent for single-output feed-forward neural networks with one
hidden layer and hyperbolic tangent activation function. We demonstrate its
applicability using both simulation and data analysis.
"
"stat.TH","  Testing large covariance matrices is of fundamental importance in statistical
analysis with high-dimensional data. In the past decade, three types of test
statistics have been studied in the literature: quadratic form statistics,
maximum form statistics, and their weighted combination. It is known that
quadratic form statistics would suffer from low power against sparse
alternatives and maximum form statistics would suffer from low power against
dense alternatives. The weighted combination methods were introduced to enhance
the power of quadratic form statistics or maximum form statistics when the
weights are appropriately chosen. In this paper, we provide a new perspective
to exploit the full potential of quadratic form statistics and maximum form
statistics for testing high-dimensional covariance matrices. We propose a
scale-invariant power enhancement test based on Fisher's method to combine the
p-values of quadratic form statistics and maximum form statistics. After
carefully studying the asymptotic joint distribution of quadratic form
statistics and maximum form statistics, we prove that the proposed combination
method retains the correct asymptotic size and boosts the power against more
general alternatives. Moreover, we demonstrate the finite-sample performance in
simulation studies and a real application.
"
"stat.TH","  Bayes' rule tells us how to invert a causal process in order to update our
beliefs in light of new evidence. If the process is believed to have a complex
compositional structure, we may ask whether composing the inversions of the
component processes gives the same belief update as the inversion of the whole.
We answer this question affirmatively, showing that the relevant compositional
structure is precisely that of the lens pattern, and that we can think of
Bayesian inversion as a particular instance of a state-dependent morphism in a
corresponding fibred category. We define a general notion of (mixed) Bayesian
lens, and discuss the (un)lawfulness of these lenses when their contravariant
components are exact Bayesian inversions. We prove our main result both
abstractly and concretely, for both discrete and continuous states, taking care
to illustrate the common structures.
"
"stat.TH","  The ability of ensemble Kalman filter (EnKF) algorithms to extract
information from observations is analyzed with the aid of the concept of the
degrees of freedom for signal (DFS). A simple mathematical argument shows that
DFS for EnKF is bounded from above by the ensemble size, which entails that
assimilating much more observations than the ensemble size automatically leads
to DFS underestimation. Since DFS is a trace of the posterior error covariance
mapped onto the normalized observation space, underestimated DFS implies
overconfidence (underdispersion) in the analysis spread, which, in a cycled
context, requires covariance inflation to be applied. The theory is then
extended to cases where covariance localization schemes (either B-localization
or R-localization) are applied to show how they alleviate the DFS
underestimation issue. These findings from mathematical argument are
demonstrated with a simple one-dimensional covariance model. Finally, the DFS
concept is used to form speculative arguments about how to interpret several
puzzling features of LETKF previously reported in the literature such as why
using less observations can lead to better performance, when optimal
localization scales tend to occur, and why covariance inflation methods based
on relaxation to prior information approach are particularly successful when
observations are inhomogeneously distributed. A presumably first application of
DFS diagnostics to a quasi-operational global EnKF system is presented in
Appendix.
"
"stat.TH","  We study estimation of a gradient-sparse parameter vector
$\boldsymbol{\theta}^* \in \mathbb{R}^p$, having strong gradient-sparsity
$s^*:=\|\nabla_G \boldsymbol{\theta}^*\|_0$ on an underlying graph $G$. Given
observations $Z_1,\ldots,Z_n$ and a smooth, convex loss function $\mathcal{L}$
for which $\boldsymbol{\theta}^*$ minimizes the population risk
$\mathbb{E}[\mathcal{L}(\boldsymbol{\theta};Z_1,\ldots,Z_n)]$, we propose to
estimate $\boldsymbol{\theta}^*$ by a projected gradient descent algorithm that
iteratively and approximately projects gradient steps onto spaces of vectors
having small gradient-sparsity over low-degree spanning trees of $G$. We show
that, under suitable restricted strong convexity and smoothness assumptions for
the loss, the resulting estimator achieves the squared-error risk
$\frac{s^*}{n} \log (1+\frac{p}{s^*})$ up to a multiplicative constant that is
independent of $G$. In contrast, previous polynomial-time algorithms have only
been shown to achieve this guarantee in more specialized settings, or under
additional assumptions for $G$ and/or the sparsity pattern of $\nabla_G
\boldsymbol{\theta}^*$. As applications of our general framework, we apply our
results to the examples of linear models and generalized linear models with
random design.
"
"stat.TH","  Change point detection in high dimensional data has found considerable
interest interest in recent years. Most of the literature designs methodology
for a retrospective analysis, where the whole sample is already available when
the statistical inference begins. This paper develops monitoring schemes for
the online scenario, where high dimensional data arrives steadily and changes
shall be detected as fast as possible controlling at the same time the
probability of a false alarm. We develop sequential procedures capable of
detecting changes in the mean vector of a successively observed high
dimensional time series with spatial and temporal dependence. In a high
dimensional scenario it is shown that the new monitoring schemes have
asymptotic level alpha under the null hypothesis of no change and are
consistent under the alternative of a change in at least one component. The
properties of the new methodology are illustrated by means of a simulation
study and in the analysis of a data example. As a side result, we show that the
range of a Brownian motion is in the domain of attraction of the Gumbel
distribution.
"
"stat.TH","  This paper studies binary logistic regression for rare events data, or
imbalanced data, where the number of events (observations in one class, often
called cases) is significantly smaller than the number of nonevents
(observations in the other class, often called controls). We first derive the
asymptotic distribution of the maximum likelihood estimator (MLE) of the
unknown parameter, which shows that the asymptotic variance convergences to
zero in a rate of the inverse of the number of the events instead of the
inverse of the full data sample size. This indicates that the available
information in rare events data is at the scale of the number of events instead
of the full data sample size. Furthermore, we prove that under-sampling a small
proportion of the nonevents, the resulting under-sampled estimator may have
identical asymptotic distribution to the full data MLE. This demonstrates the
advantage of under-sampling nonevents for rare events data, because this
procedure may significantly reduce the computation and/or data collection
costs. Another common practice in analyzing rare events data is to over-sample
(replicate) the events, which has a higher computational cost. We show that
this procedure may even result in efficiency loss in terms of parameter
estimation.
"
"stat.TH","  We derive uniform convergence rates for the maximum likelihood estimator and
minimax lower bounds for parameter estimation in two-component location-scale
Gaussian mixture models with unequal variances. We assume the mixing
proportions of the mixture are known and fixed, but make no separation
assumption on the underlying mixture components. A phase transition is shown to
exist in the optimal parameter estimation rate, depending on whether or not the
mixture is balanced. Key to our analysis is a careful study of the dependence
between the parameters of location-scale Gaussian mixture models, as captured
through systems of polynomial equalities and inequalities whose solution set
drives the rates we obtain. A simulation study illustrates the theoretical
findings of this work.
"
"stat.TH","  The Gaussian correlation inequality (GCI) for symmetrical n-rectangles is
improved if the absolute components have a joint MTP2-distribution
(multivariate totally positive of order 2). Inequalities of the here given type
hold at least for all MTP2-probability measures on R^n or (0,infinity)^n with
everywhere positive smooth densities. In particular, at least some infinitely
divisible multivariate chi-square distributions (gamma distributions in the
sense of Krishnamoorthy and Parthasarathy) with any positive real ""degree of
freedom"" are shown to be MTP2. Moreover, further numerically calculable
probability inequalities for a broad class of multivariate gamma distributions
are derived and a different improvement for inequalities of the GCI-type - and
of a similar type with three instead of two groups of components - with more
special correlation structures. The main idea behind these inequalities is to
find for a given correlation matrix with positive correlations a further
correlation matrix with smaller correlations whose inverse is an M-matrix and
where the corresponding multivariate gamma distribution function is numerically
available.
"
"stat.TH","  Many datasets are collected automatically, and are thus easily contaminated
by outliers. In order to overcome this issue, there was recently a regain of
interest in robust estimation methods. However, most of these methods are
designed for a specific purpose, such as estimation of the mean, or linear
regression. We propose estimators based on Maximum Mean Discrepancy (MMD)
optimization as a universal framework for robust regression. We provide
non-asymptotic error bounds, and show that our estimators are robust to
Huber-type contamination. We discuss the optimization of the objective
functions via (stochastic) gradient descent in classical examples such as
linear, logistic or Poisson regression. These results are illustrated by a set
of simulations.
"
"stat.TH","  In this paper, we develop uniform inference methods for the conditional mode
based on quantile regression. Specifically, we propose to estimate the
conditional mode by minimizing the derivative of the estimated conditional
quantile function defined by smoothing the linear quantile regression
estimator, and develop a novel bootstrap method, which we call the pivotal
bootstrap, for our conditional mode estimator. Building on high-dimensional
Gaussian approximation techniques, we establish the validity of simultaneous
confidence rectangles constructed from the pivotal bootstrap for the
conditional mode. We also extend the preceding analysis to the case where the
dimension of the covariate vector is increasing with the sample size. Finally,
we conduct simulation experiments and a real data analysis using U.S. wage data
to demonstrate the finite sample performance of our inference method.
"
"stat.TH","  We congratulate Engelke and Hitz on a thought-provoking paper on graphical
models for extremes. A key contribution of the paper is the introduction of a
novel definition of conditional independence for a multivariate Pareto
distribution. Here, we outline a proposal for independence and conditional
independence of general random variables whose support is a general set Omega
in multidimensional real number space. Our proposal includes the authors'
definition of conditional independence, and the analogous definition of
independence as special cases. By making our proposal independent of the
context of extreme value theory, we highlight the importance of the authors'
contribution beyond this particular context.
"
"stat.TH","  In this paper, we study the asymptotic properties of regularized least
squares with indefinite kernels in reproducing kernel Krein spaces (RKKS). By
introducing a bounded hyper-sphere constraint to such non-convex regularized
risk minimization problem, we theoretically demonstrate that this problem has a
globally optimal solution with a closed form on the sphere, which makes
approximation analysis feasible in RKKS. Regarding to the original regularizer
induced by the indefinite inner product, we modify traditional error
decomposition techniques, prove convergence results for the introduced
hypothesis error based on matrix perturbation theory, and derive learning rates
of such regularized regression problem in RKKS. Under some conditions, the
derived learning rates in RKKS are the same as that in reproducing kernel
Hilbert spaces (RKHS), which is actually the first work on approximation
analysis of regularized learning algorithms in RKKS.
"
"stat.TH","  We propose two robust methods for testing hypotheses on unknown parameters of
predictive regression models under heterogeneous and persistent volatility as
well as endogenous, persistent and/or fat-tailed regressors and errors. The
proposed robust testing approaches are applicable both in the case of discrete
and continuous time models. Both of the methods use the Cauchy estimator to
effectively handle the problems of endogeneity, persistence and/or
fat-tailedness in regressors and errors. The difference between our two methods
is how the heterogeneous volatility is controlled. The first method relies on
robust t-statistic inference using group estimators of a regression parameter
of interest proposed in Ibragimov and Muller, 2010. It is simple to implement,
but requires the exogenous volatility assumption. To relax the exogenous
volatility assumption, we propose another method which relies on the
nonparametric correction of volatility. The proposed methods perform well
compared with widely used alternative inference procedures in terms of their
finite sample properties.
"
"stat.TH","  Many key variables in finance, economics and risk management, including
financial returns and foreign exchange rates, exhibit nonlinear dependence,
heterogeneity and heavy-tailedness of some usually largely unknown type.
  The presence of non-linear dependence (usually modelled using GARCH-type
dynamics) and heavy-tailedness may make problematic the analysis of
(non-)efficiency, volatility clustering and predictive regressions in economic
and financial markets using traditional approaches that appeal to asymptotic
normality of sample autocorrelation functions (ACFs) of returns and their
squares.
  The paper presents several new approaches to deal with the above problems. We
provide the results that motivate the use of measures of market
(non-)efficiency, volatility clustering and nonlinear dependence based on
(small) powers of absolute returns and their signed versions. The paper
provides asymptotic theory for sample analogues of the above measures in the
case of general time series, including GARCH-type processes. It further
develops new approaches to robust inference on them in the case of general
GARCH-type processes exhibiting heavy-tailedness properties. The approaches are
based on robust inference methods exploiting conservativeness properties of
t-statistics Ibragimov and Muller (2010,2016) and several new results on their
applicability in the settings considered. In the approaches, estimates of
parameters of interest are computed for groups of data and the inference is
based on t-statistics in resulting group estimates. This results in valid
robust inference under a wide range of heterogeneity and dependence assumptions
satisfied in financial and economic markets. Numerical results and empirical
applications confirm advantages of the new approaches over existing ones and
their wide applicability.
"
"stat.TH","  We develop a general non-asymptotic analysis of learning rates in kernel
ridge regression (KRR), applicable for arbitrary Mercer kernels with
multi-dimensional support. Our analysis is based on an operator-theoretic
framework, at the core of which lies two error bounds under reproducing kernel
Hilbert space norms encompassing a general class of kernels and regression
functions, with remarkable extensibility to various inferential goals through
augmenting results. When applied to KRR estimators, our analysis leads to error
bounds under the stronger supremum norm, in addition to the commonly studied
weighted $L_2$ norm; in a concrete example specialized to the Mat\'ern kernel,
the established bounds recover the nearly minimax optimal rates. The wide
applicability of our analysis is further demonstrated through two new
theoretical results: (1) non-asymptotic learning rates for mixed partial
derivatives of KRR estimators, and (2) a non-asymptotic characterization of the
posterior variances of Gaussian processes, which corresponds to uncertainty
quantification in kernel methods and nonparametric Bayes.
"
"stat.TH","  Given $n$ independent random vectors with common density $f$ on
$\mathbb{R}^d$, we study the weak convergence of three empirical-measure based
estimators of the convex $\lambda$-level set $L_\lambda$ of $f$, namely the
excess mass set, the minimum volume set and the maximum probability set, all
selected from a class of convex sets $\mathcal{A}$ that contains $L_\lambda$.
Since these set-valued estimators approach $L_\lambda$, even the formulation of
their weak convergence is non-standard. We identify the joint limiting
distribution of the symmetric difference of $L_\lambda$ and each of the three
estimators, at rate $n^{-1/3}$. It turns out that the minimum volume set and
the maximum probability set estimators are asymptotically indistinguishable,
whereas the excess mass set estimator exhibits ""richer"" limit behavior.
Arguments rely on the boundary local empirical process, its cylinder
representation, dimension-free concentration around the boundary of
$L_\lambda$, and the set-valued argmax of a drifted Wiener process.
"
"stat.TH","  The notion of exchangeability has been recognized in the causal inference
literature in various guises, but only rarely in the original Bayesian meaning
as a symmetry property between individual units in statistical inference. Since
the latter is a standard ingredient in Bayesian inference, we argue that in
Bayesian causal inference it is natural to link the causal model, including the
notion of confounding and definition of causal contrasts of interest, to the
concept of exchangeability. Here we relate the Bayesian notion of
exchangeability to alternative conditions for unconfounded inferences, commonly
stated using potential outcome variables, and define causal contrasts in the
presence of exchangeability in terms of limits of posterior predictive
expectations for further exchangeable units. We demonstrate that this reasoning
also carries over to longitudinal settings where parametric inferences are
susceptible to the so-called null paradox. We interpret the paradox in terms of
an exchangeability assumption made on too coarse a scale.
"
"stat.TH","  Let $X$ be a random variable with unknown mean and finite variance. We
present a new estimator of the mean of $X$ that is robust with respect to the
possible presence of outliers in the sample, provides tight sub-Gaussian
deviation guarantees without any additional assumptions on the shape or tails
of the distribution, and moreover is asymptotically efficient. This is the
first estimator that provably combines all these qualities in one package. Our
construction is inspired by robustness properties possessed by the
self-normalized sums. Finally, theoretical findings are supplemented by
numerical simulations highlighting the strong performance of the proposed
estimator in comparison with previously known techniques.
"
"stat.TH","  Diffusion maps is a manifold learning algorithm widely used for
dimensionality reduction. Using a sample from a distribution, it approximates
the eigenvalues and eigenfunctions of associated Laplace-Beltrami operators.
Theoretical bounds on the approximation error are however generally much weaker
than the rates that are seen in practice. This paper uses new approaches to
improve the error bounds in the model case where the distribution is supported
on a hypertorus. For the data sampling (variance) component of the error we
make spatially localised compact embedding estimates on certain Hardy spaces;
we study the deterministic (bias) component as a perturbation of the
Laplace-Beltrami operator's associated PDE, and apply relevant spectral
stability results. Using these approaches, we match long-standing pointwise
error bounds for both the spectral data and the norm convergence of the
operator discretisation.
  We also introduce an alternative normalisation for diffusion maps based on
Sinkhorn weights. This normalisation approximates a Langevin diffusion on the
sample and yields a symmetric operator approximation. We prove that it has
better convergence compared with the standard normalisation on flat domains,
and present a highly efficient algorithm to compute the Sinkhorn weights.
"
"stat.TH","  The least squares estimator (LSE) is shown to be suboptimal in squared error
loss in the usual nonparametric regression model with Gaussian errors for $d
\geq 5$ for each of the following families of functions: (i) convex functions
supported on a polytope (in fixed design), (ii) bounded convex functions
supported on a polytope (in random design), and (iii) convex Lipschitz
functions supported on any convex domain (in random design). For each of these
families, the risk of the LSE is proved to be of the order $n^{-2/d}$ (up to
logarithmic factors) while the minimax risk is $n^{-4/(d+4)}$, for $d \ge 5$.
In addition, the first rate of convergence results (worst case and adaptive)
for the full convex LSE are established for polytopal domains for all $d \geq
1$. Some new metric entropy results for convex functions are also proved which
are of independent interest.
"
"stat.TH","  In this paper, we address the estimation of the sensitivity indices called
""Shapley eects"". These sensitivity indices enable to handle dependent input
variables. The Shapley eects are generally dicult to estimate, but they are
easily computable in the Gaussian linear framework. The aim of this work is to
use the values of the Shapley eects in an approximated Gaussian linear
framework as estimators of the true Shapley eects corresponding to a non-linear
model. First, we assume that the input variables are Gaussian with small
variances. We provide rates of convergence of the estimated Shapley eects to
the true Shapley eects. Then, we focus on the case where the inputs are given
by an non-Gaussian empirical mean. We prove that, under some mild assumptions,
when the number of terms in the empirical mean increases, the dierence between
the true Shapley eects and the estimated Shapley eects given by the Gaussian
linear approximation converges to 0. Our theoretical results are supported by
numerical studies, showing that the Gaussian linear approximation is accurate
and enables to decrease the computational time signicantly.
"
"stat.TH","  We adapt conformal e-prediction to change detection, defining analogues of
the Shiryaev-Roberts and CUSUM procedures for detecting violations of the IID
assumption. Asymptotically, the frequency of false alarms for these analogues
does not exceed the usual bounds.
"
"stat.TH","  We propose a general method of producing synthetic data, which is widely
applicable for parametric models, has asymptotically efficient summary
statistics, and is both easily implemented and highly computationally
efficient. Our approach allows for the construction of both partially synthetic
datasets, which preserve the summary statistics without formal privacy methods,
as well as fully synthetic data which satisfy the strong guarantee of
differential privacy (DP), both with asymptotically efficient summary
statistics. While our theory deals with asymptotics, we demonstrate through
simulations that our approach offers high utility in small samples as well. In
particular we 1) apply our method to the Burr distribution, evaluating the
parameter estimates as well as distributional properties with the
Kolmogorov-Smirnov test, 2) demonstrate the performance of our mechanism on a
log-linear model based on a car accident dataset, and 3) produce DP synthetic
data for the beta distribution using a customized Laplace mechanism.
"
"stat.TH","  Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is
often described as the kernelized gradient flow for the Kullback-Leibler
divergence in the geometry of optimal transport. We introduce a new perspective
on SVGD that instead views SVGD as the (kernelized) gradient flow of the
chi-squared divergence which, we show, exhibits a strong form of uniform
exponential ergodicity under conditions as weak as a Poincar\'e inequality.
This perspective leads us to propose an alternative to SVGD, called Laplacian
Adjusted Wasserstein Gradient Descent (LAWGD), that can be implemented from the
spectral decomposition of the Laplacian operator associated with the target
density. We show that LAWGD exhibits strong convergence guarantees and good
practical performance.
"
"stat.TH","  Topological data analysis (TDA) allows us to explore the topological features
of a dataset. Among topological features, lower dimensional ones have recently
drawn the attention of practitioners in mathematics and statistics due to their
potential to aid the discovery of low dimensional structure in a data set.
However, lower dimensional features are usually challenging to detect from a
probabilistic perspective.
  In this paper, lower dimensional topological features occurring as
zero-density regions of density functions are introduced and thoroughly
investigated. Specifically, we consider sequences of coverings for the support
of a density function in which the coverings are comprised of balls with
shrinking radii. We show that, when these coverings satisfy certain sufficient
conditions as the sample size goes to infinity, we can detect lower
dimensional, zero-density regions with increasingly higher probability while
guarding against false detection. We supplement the theoretical developments
with the discussion of simulated experiments that elucidate the behavior of the
methodology for different choices of the tuning parameters that govern the
construction of the covering sequences and characterize the asymptotic results.
"
"stat.TH","  Although optimal transport (OT) problems admit closed form solutions in a
very few notable cases, e.g. in 1D or between Gaussians, these closed forms
have proved extremely fecund for practitioners to define tools inspired from
the OT geometry. On the other hand, the numerical resolution of OT problems
using entropic regularization has given rise to many applications, but because
there are no known closed-form solutions for entropic regularized OT problems,
these approaches are mostly algorithmic, not informed by elegant closed forms.
In this paper, we propose to fill the void at the intersection between these
two schools of thought in OT by proving that the entropy-regularized optimal
transport problem between two Gaussian measures admits a closed form. Contrary
to the unregularized case, for which the explicit form is given by the
Wasserstein-Bures distance, the closed form we obtain is differentiable
everywhere, even for Gaussians with degenerate covariance matrices. We obtain
this closed form solution by solving the fixed-point equation behind Sinkhorn's
algorithm, the default method for computing entropic regularized OT.
Remarkably, this approach extends to the generalized unbalanced case -- where
Gaussian measures are scaled by positive constants. This extension leads to a
closed form expression for unbalanced Gaussians as well, and highlights the
mass transportation / destruction trade-off seen in unbalanced optimal
transport. Moreover, in both settings, we show that the optimal transportation
plans are (scaled) Gaussians and provide analytical formulas of their
parameters. These formulas constitute the first non-trivial closed forms for
entropy-regularized optimal transport, thus providing a ground truth for the
analysis of entropic OT and Sinkhorn's algorithm.
"
"stat.TH","  Motivated by the prevalent data science applications of processing and mining
large-scale graph data such as social networks, web graphs, and biological
networks, as well as the high I/O and communication costs of storing and
transmitting such data, this paper investigates lossless compression of data
appearing in the form of a labeled graph. A universal graph compression scheme
is proposed, which does not depend on the underlying statistics/distribution of
the graph model. For graphs generated by a stochastic block model, which is a
widely used random graph model capturing the clustering effects in social
networks, the proposed scheme achieves the optimal theoretical limit of
lossless compression without the need to know edge probabilities, community
labels, or the number of communities.
  The key ideas in establishing universality for stochastic block models
include: 1) block decomposition of the adjacency matrix of the graph; 2)
generalization of the Krichevsky-Trofimov probability assignment, which was
initially designed for i.i.d. random processes. In four benchmark graph
datasets (protein-to-protein interaction, LiveJournal friendship, Flickr, and
YouTube), the compressed files from competing algorithms (including CSR,
Ligra+, PNG image compressor, and Lempel-Ziv compressor for two-dimensional
data) take 2.4 to 27 times the space needed by the proposed scheme.
"
"stat.TH","  We consider a change-point test based on the Hill estimator to test for
structural changes in the tail index of Long Memory Stochastic Volatility time
series. In order to determine the asymptotic distribution of the corresponding
test statistic, we prove a uniform reduction principle for the tail empirical
process in a two-parameter Skorohod space. It is shown that such a process
displays a dichotomous behavior according to an interplay between the Hurst
parameter, i.e., a parameter characterizing the dependence in the data, and the
tail index. Our theoretical results are accompanied by simulation studies and
the analysis of financial time series with regard to structural changes in the
tail index.
"
"stat.TH","  The main problem considered in this paper is construction and theoretical
study of efficient $n$-point coverings of a $d$-dimensional cube $[-1,1]^d$.
Targeted values of $d$ are between 5 and 50; $n$ can be in hundreds or
thousands and the designs (collections of points) are nested. This paper is a
continuation of our paper \cite{us}, where we have theoretically investigated
several simple schemes and numerically studied many more. In this paper, we
extend the theoretical constructions of \cite{us} for studying the designs
which were found to be superior to the ones theoretically investigated in
\cite{us}. We also extend our constructions for new construction schemes which
provide even better coverings (in the class of nested designs) than the ones
numerically found in \cite{us}. In view of a close connection of the problem of
quantization to the problem of covering, we extend our theoretical
approximations and practical recommendations to the problem of construction of
efficient quantization designs in a cube $[-1,1]^d$. In the last section, we
discuss the problems of covering and quantization in a $d$-dimensional simplex;
practical significance of this problem has been communicated to the authors by
Professor Michael Vrahatis, a co-editor of the present volume.
"
"stat.TH","  In this paper, we investigate a general class of stochastic gradient descent
(SGD) algorithms, called conditioned SGD, based on a preconditioning of the
gradient direction. Under some mild assumptions, namely the $L$-smoothness of
the objective function and some weak growth condition on the noise, we
establish the almost sure convergence and the asymptotic normality for a broad
class of conditioning matrices. In particular, when the conditioning matrix is
an estimate of the inverse Hessian at the optimal point, the algorithm is
proved to be asymptotically optimal. The benefits of this approach are
validated on simulated and real datasets.
"
"stat.TH","  We study graph-based Laplacian semi-supervised learning at low labeling
rates. Laplacian learning uses harmonic extension on a graph to propagate
labels. At very low label rates, Laplacian learning becomes degenerate and the
solution is roughly constant with spikes at each labeled data point. Previous
work has shown that this degeneracy occurs when the number of labeled data
points is finite while the number of unlabeled data points tends to infinity.
In this work we allow the number of labeled data points to grow to infinity
with the number of labels. Our results show that for a random geometric graph
with length scale $\varepsilon>0$ and labeling rate $\beta>0$, if $\beta
\ll\varepsilon^2$ then the solution becomes degenerate and spikes form, and if
$\beta\gg \varepsilon^2$ then Laplacian learning is well-posed and consistent
with a continuum Laplace equation. Furthermore, in the well-posed setting we
prove quantitative error estimates of $O(\varepsilon\beta^{-1/2})$ for the
difference between the solutions of the discrete problem and continuum PDE, up
to logarithmic factors. We also study $p$-Laplacian regularization and show the
same degeneracy result when $\beta \ll \varepsilon^p$. The proofs of our
well-posedness results use the random walk interpretation of Laplacian learning
and PDE arguments, while the proofs of the ill-posedness results use
$\Gamma$-convergence tools from the calculus of variations. We also present
numerical results on synthetic and real data to illustrate our results.
"
"stat.TH","  In a multi-index model with $k$ index vectors, the input variables are
transformed by taking inner products with the index vectors. A transfer
function $f: \mathbb{R}^k \to \mathbb{R}$ is applied to these inner products to
generate the output. Thus, multi-index models are a generalization of linear
models. In this paper, we consider monotone multi-index models. Namely, the
transfer function is assumed to be coordinate-wise monotone. The monotone
multi-index model therefore generalizes both linear regression and isotonic
regression, which is the estimation of a coordinate-wise monotone function. We
consider the case of nonnegative index vectors. We provide an algorithm based
on integer programming for the estimation of monotone multi-index models, and
provide guarantees on the $L_2$ loss of the estimated function relative to the
ground truth.
"
"stat.TH","  Asynchronous Q-learning aims to learn the optimal action-value function (or
Q-function) of a Markov decision process (MDP), based on a single trajectory of
Markovian samples induced by a behavior policy. Focusing on a
$\gamma$-discounted MDP with state space $\mathcal{S}$ and action space
$\mathcal{A}$, we demonstrate that the $\ell_{\infty}$-based sample complexity
of classical asynchronous Q-learning -- namely, the number of samples needed to
yield an entrywise $\varepsilon$-accurate estimate of the Q-function -- is at
most on the order of \begin{equation*}
\frac{1}{\mu_{\mathsf{min}}(1-\gamma)^5\varepsilon^2}+
\frac{t_{\mathsf{mix}}}{\mu_{\mathsf{min}}(1-\gamma)} \end{equation*} up to
some logarithmic factor, provided that a proper constant learning rate is
adopted. Here, $t_{\mathsf{mix}}$ and $\mu_{\mathsf{min}}$ denote respectively
the mixing time and the minimum state-action occupancy probability of the
sample trajectory. The first term of this bound matches the complexity in the
case with independent samples drawn from the stationary distribution of the
trajectory. The second term reflects the expense taken for the empirical
distribution of the Markovian trajectory to reach a steady state, which is
incurred at the very beginning and becomes amortized as the algorithm runs.
Encouragingly, the above bound improves upon the state-of-the-art result by a
factor of at least $|\mathcal{S}||\mathcal{A}|$. Further, the scaling on the
discount complexity can be improved by means of variance reduction.
"
"stat.TH","  We investigate sequential change point estimation and detection in univariate
nonparametric settings, where a stream of independent observations from
sub-Gaussian distributions with a common variance factor and piecewise-constant
but otherwise unknown means are collected. We develop a simple CUSUM-based
methodology that provably control the probability of false alarms or the
average run length while minimizing, in a minimax sense, the detection delay.
We allow for all the model parameters to vary in order to capture a broad range
of levels of statistical hardness for the problem at hand. We further show how
our methodology is applicable to the case in which multiple change points are
to be estimated sequentially.
"
"stat.TH","  This paper analyzes a new regularized learning scheme for high dimensional
partially linear support vector machine. The proposed approach consists of an
empirical risk and the Lasso-type penalty for linear part, as well as the
standard functional norm for nonlinear part. Here the linear kernel is used for
model interpretation and feature selection, while the nonlinear kernel is
adopted to enhance algorithmic flexibility. In this paper, we develop a new
technical analysis on the weighted empirical process, and establish the sharp
learning rates for the semi-parametric estimator under the regularized
conditions. Specially, our derived learning rates for semi-parametric SVM
depend on not only the sample size and the functional complexity, but also the
sparsity and the margin parameters.
"
"stat.TH","  Covariance or scatter matrix estimation is ubiquitous in most modern
statistical and machine learning applications. The task becomes especially
challenging since most real-world datasets are essentially non-Gaussian. The
data is often contaminated by outliers and/or has heavy-tailed distribution
causing the sample covariance to behave very poorly and calling for robust
estimation methodology. The natural framework for the robust scatter matrix
estimation is based on elliptical populations. Here, Tyler's estimator stands
out by being distribution-free within the elliptical family and easy to
compute. The existing works thoroughly study the performance of Tyler's
estimator assuming ellipticity but without providing any tools to verify this
assumption when the covariance is unknown in advance. We address the following
open question: Given the sampled data and having no prior on the data
generating process, how to assess the quality of the scatter matrix estimator?
In this work we show that this question can be reformulated as an asymptotic
uniformity test for certain sequences of exchangeable vectors on the unit
sphere. We develop a consistent and easily applicable goodness-of-fit test
against all alternatives to ellipticity when the scatter matrix is unknown. The
findings are supported by numerical simulations demonstrating the power of the
suggest technique.
"
"stat.TH","  We consider stochastic bandit problems with $K$ arms, each associated with a
bounded distribution supported on the range $[m,M]$. We do not assume that the
range $[m,M]$ is known and show that there is a cost for learning this range.
Indeed, a new trade-off between distribution-dependent and distribution-free
regret bounds arises, which prevents from simultaneously achieving the typical
$\ln T$ and \smash{$\sqrt{T}$} bounds. For instance, a \smash{$\sqrt{T}$}
distribution-free regret bound may only be achieved if the
distribution-dependent regret bounds are at least of order \smash{$\sqrt{T}$}.
We exhibit a strategy achieving the rates for regret indicated by the new
trade-off.
"
"stat.TH","  Exact inference for hidden Markov models requires the evaluation of all
distributions of interest - filtering, prediction, smoothing and likelihood -
with a finite computational effort. This article provides sufficient conditions
for exact inference for a class of hidden Markov models on general state spaces
given a set of discretely collected indirect observations linked non linearly
to the signal, and a set of practical algorithms for inference. The conditions
we obtain are concerned with the existence of a certain type of dual process,
which is an auxiliary process embedded in the time reversal of the signal, that
in turn allows to represent the distributions and functions of interest as
finite mixtures of elementary densities or products thereof. We describe
explicitly how to update recursively the parameters involved, yielding
qualitatively similar results to those obtained with Baum--Welch filters on
finite state spaces. We then provide practical algorithms for implementing the
recursions, as well as approximations thereof via an informed pruning of the
mixtures, and we show superior performance to particle filters both in accuracy
and computational efficiency. The code for optimal filtering, smoothing and
parameter inference is made available in the Julia package
DualOptimalFiltering.
"
"stat.TH","  High-dimensional self-exciting point processes have been widely used in many
application areas to model discrete event data in which past and current events
affect the likelihood of future events. In this paper, we are concerned with
detecting abrupt changes of the coefficient matrices in discrete-time
high-dimensional self-exciting Poisson processes, which have yet to be studied
in the existing literature due to both theoretical and computational challenges
rooted in the non-stationary and high-dimensional nature of the underlying
process. We propose a penalized dynamic programming approach which is supported
by a theoretical rate analysis and numerical evidence.
"
"stat.TH","  Unifying the generalized Marshall-Olkin (GMO) and Poisson-G (P-G) a new
family of distribution is proposed. Density and the survival function are
expressed as infinite mixtures of P-G family. The quantile function,
asymptotes, shapes, stochastic ordering, moment generating function, order
statistics, probability weighted moments and R\'enyi entropy are derived.
Maximum likelihood estimation with large sample properties is presented. A
Monte Carlo simulation is used to examine the pattern of the bias and the mean
square error of the maximum likelihood estimators. An illustration of
comparison with some of the important sub models of the family in modeling a
real data reveals the utility of the proposed family.
"
"stat.TH","  We establish higher-order expansions for a difference between probability
distributions of sums of i.i.d. random vectors in a Euclidean space. The
derived bounds are uniform over two classes of sets: the set of all Euclidean
balls and the set of all half-spaces. These results allow to account for an
impact of higher-order moments or cumulants of the considered distributions;
the obtained error terms depend on a sample size and a dimension explicitly.
The new inequalities outperform accuracy of the normal approximation in
existing Berry--Esseen inequalities under very general conditions. For
symmetrically distributed random summands, the obtained results are optimal in
terms of the ratio between the dimension and the sample size. Using the new
higher-order inequalities, we study accuracy of the nonparametric bootstrap
approximation and propose a bootstrap score test under possible model
misspecification. The proposed results include also explicit error bounds for
general elliptical confidence regions for an expected value of the random
summands, and optimality of the Gaussian anti-concentration inequality over the
set of all Euclidean balls.
"
"stat.TH","  We introduce new shape-constrained classes of distribution functions on R,
the bi-$s^*$-concave classes. In parallel to results of D\""umbgen, Kolesnyk,
and Wilke (2017) for what they called the class of bi-log-concave distribution
functions, we show that every $s$-concave density $f$ has a bi-$s^*$-concave
distribution function $F$ for $s^*\leq s/(s+1)$. Confidence bands building on
existing nonparametric bands, but accounting for the shape constraint of
bi-$s^*$-concavity, are also considered. The new bands extend those developed
by D\""umbgen et al. (2017) for the constraint of bi-log-concavity. We also make
connections between bi-$s^*$-concavity and finiteness of the Cs\""org\H{o} -
R\'ev\'esz constant of $F$ which plays an important role in the theory of
quantile processes.
"
"stat.TH","  Some reasons for high leverage are analytically investigated by decomposing
leverage into meaningful components. The results in this work can be used for
remedial action as a next step of data analysis.
"
"stat.TH","  We develop a technique for establishing lower bounds on the sample complexity
of Least Squares (or, Empirical Risk Minimization) for large classes of
functions. As an application, we settle an open problem regarding optimality of
Least Squares in estimating a convex set from noisy support function
measurements in dimension $d\geq 6$. Specifically, we establish that Least
Squares is mimimax sub-optimal, and achieves a rate of
$\tilde{\Theta}_d(n^{-2/(d-1)})$ whereas the minimax rate is
$\Theta_d(n^{-4/(d+3)})$.
"
"stat.TH","  We consider nonparametric Bayesian estimation and prediction for
nonhomogeneous Poisson process models with unknown intensity functions. We
propose a class of improper priors for intensity functions. Nonparametric
Bayesian inference with kernel mixture based on the class improper priors is
shown to be useful, although improper priors have not been widely used for
nonparametric Bayes problems. Several theorems corresponding to those for
finite-dimensional independent Poisson models hold for nonhomogeneous Poisson
process models with infinite-dimensional parameter spaces. Bayesian estimation
and prediction based on the improper priors are shown to be admissible under
the Kullback--Leibler loss. Numerical methods for Bayesian inference based on
the priors are investigated.
"
"stat.TH","  The $k$ principal points of a random vector $\mathbf{X}$ are defined as a set
of points which minimize the expected squared distance between $\mathbf{X}$ and
the nearest point in the set. They are thoroughly studied in Flury (1990,
1993), Tarpey (1995) and Tarpey, Li and Flury (1995). For their treatment, the
examination is usually restricted to the family of elliptical distributions. In
this paper, we present an extension of the previous results to the functional
elliptical distribution case, i.e., when dealing with random elements over a
separable Hilbert space ${\cal H}$. Principal points for gaussian processes
were defined in Tarpey and Kinateder (2003). In this paper, we generalize the
concepts of principal points, self-consistent points and elliptical
distributions so as to fit them in this functional framework. Results linking
self-consistency and the eigenvectors of the covariance operator are
re-obtained in this new setting as well as an explicit formula for the $k=2$
case so as to include elliptically distributed random elements in ${\cal H}$.
"
"stat.TH","  The increasing application of deep-learning is accompanied by a shift towards
highly non-linear statistical models. In terms of their geometry it is natural
to identify these models with Riemannian manifolds. The further analysis of the
statistical models therefore raises the issue of a correlation measure, that in
the cutting planes of the tangent spaces equals the respective Pearson
correlation and extends to a correlation measure that is normalized with
respect to the underlying manifold. In this purpose the article reconstitutes
elementary properties of the Pearson correlation to successively derive a
linear generalization to multiple dimensions and thereupon a nonlinear
generalization to principal manifolds, given by the Riemann-Pearson
Correlation.
"
"stat.TH","  Decision trees with binary splits are popularly constructed using
Classification and Regression Trees (CART) methodology. For regression models,
this approach recursively divides the data into two near-homogenous daughter
nodes according to a split point that maximizes the reduction in sum of squares
error (the impurity) along a particular variable. This paper aims to study the
statistical properties of regression trees constructed with CART methodology.
In doing so, we find that the training error is governed by the Pearson
correlation between the optimal decision stump and response data in each node,
which we bound by constructing a prior distribution on the split points and
solving a nonlinear optimization problem. We leverage this connection between
the training error and Pearson correlation to show that CART with
cost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff
when the depth scales with the logarithm of the sample size. Data dependent
quantities, which adapt to the dimensionality and latent structure of the
regression model, are seen to govern the rates of convergence of the prediction
error.
"
"stat.TH","  The purpose of this paper is to present a mathematical theory that can be
used as a foundation for statistics that include improper priors. This theory
includes improper laws in the initial axioms and has in particular Bayes
theorem as a consequence. Another consequence is that some of the usual
calculation rules are modified. This is important in relation to common
statistical practice which usually include improper priors, but tends to use
unaltered calculation rules. In some cases the results are valid, but in other
cases inconsistencies may appear. The famous marginalization paradoxes
exemplify this latter case. An alternative mathematical theory for the
foundations of statistics can be formulated in terms of conditional probability
spaces. In this case the appearance of improper laws is a consequence of the
theory. It is proved here that the resulting mathematical structures for the
two theories are equivalent. The conclusion is that the choice of the first or
the second formulation for the initial axioms can be considered a matter of
personal preference. Readers that initially have concerns regarding improper
priors can possibly be more open toward a formulation of the initial axioms in
terms of conditional probabilities. The interpretation of an improper law is
given by the corresponding conditional probabilities.
  Keywords: Axioms of statistics, Conditional probability space, Improper
prior, Projective space
"
"stat.TH","  Many practical tasks involve sampling sequentially without replacement (WoR)
from a finite population of size $N$, in an attempt to estimate some parameter
$\theta^\star$. Accurately quantifying uncertainty throughout this process is a
nontrivial task, but is necessary because it often determines when we stop
collecting samples and confidently report a result. We present a suite of tools
for designing confidence sequences (CS) for $\theta^\star$. A CS is a sequence
of confidence sets $(C_n)_{n=1}^N$, that shrink in size, and all contain
$\theta^\star$ simultaneously with high probability. We first exploit a
relationship between Bayesian posteriors and martingales to construct a
(frequentist) CS for the parameters of a hypergeometric distribution. We then
present Hoeffding- and empirical-Bernstein-type time-uniform CSs and fixed-time
confidence intervals for sampling WoR which improve on previous bounds in the
literature.
"
"stat.TH","  The proportional odds (PO) model not only capable of generating new family of
flexible distributions but also is a very important model in reliability theory
and survival analysis. In this study, we investigate comparisons of minimums as
well as maximums of samples from dependent random variables following the PO
model and with Archimedean copulas, in terms of dispersive and star orders.
Numerical examples are provided to illustrate the the findings.
"
"stat.TH","  To perform statistical inference for time series, one should be able to
assess if they present deterministic or stochastic trends. For univariate
analysis one way to detect stochastic trends is to test if the series has unit
roots, and for multivariate studies it is often relevant to search for
stationary linear relationships between the series, or if they cointegrate. The
main goal of this article is to briefly review the shortcomings of unit root
and cointegration tests proposed by the Bayesian approach of statistical
inference and to show how they can be overcome by the fully Bayesian
significance test (FBST), a procedure designed to test sharp or precise
hypothesis. We will compare its performance with the most used frequentist
alternatives, namely, the Augmented Dickey-Fuller for unit roots and the
maximum eigenvalue test for cointegration. Keywords: Time series; Bayesian
inference; Hypothesis testing; Unit root; Cointegration.
"
"stat.TH","  We prove some efficient inference results concerning estimation of a
Ornstein-Uhlenbeck regression model, which is driven by a non-Gaussian stable
Levy process and where the output process is observed at high-frequency over a
fixed time period. Local asymptotics for the likelihood function is presented,
followed by a way to construct an asymptotically efficient estimator through a
suboptimal yet very simple preliminary estimator, which enables us to bypass
not only numerical optimization of the likelihood function, but also the
multiple-root problem.
"
"stat.TH","  A sample covariance matrix $\boldsymbol{S}$ of completely observed data is
the key statistic in a large variety of multivariate statistical procedures,
such as structured covariance/precision matrix estimation, principal component
analysis, and testing of equality of mean vectors. However, when the data are
partially observed, the sample covariance matrix from the available data is
biased and does not provide valid multivariate procedures. To correct the bias,
a simple adjustment method called inverse probability weighting (IPW) has been
used in previous research, yielding the IPW estimator. The estimator plays the
role of $\boldsymbol{S}$ in the missing data context so that it can be plugged
into off-the-shelf multivariate procedures. However, theoretical properties
(e.g. concentration) of the IPW estimator have been only established under very
simple missing structures; every variable of each sample is independently
subject to missing with equal probability.
  We investigate the deviation of the IPW estimator when observations are
partially observed under general missing dependency. We prove the optimal
convergence rate $O_p(\sqrt{\log p / n})$ of the IPW estimator based on the
element-wise maximum norm. We also derive similar deviation results even when
implicit assumptions (known mean and/or missing probability) are relaxed. The
optimal rate is especially crucial in estimating a precision matrix, because of
the ""meta-theorem"" that claims the rate of the IPW estimator governs that of
the resulting precision matrix estimator. In the simulation study, we discuss
non-positive semi-definiteness of the IPW estimator and compare the estimator
with imputation methods, which are practically important.
"
"stat.TH","  We characterize $D$-optimal designs in the two-dimensional Poisson regression
model with synergetic interaction and provide an explicit proof. The proof is
based on the idea of reparameterization of the design region in terms of
contours of constant intensity. This approach leads to a substantial reduction
of complexity as properties of the sensitivity can be treated along and across
the contours separately. Furthermore, some extensions of this result to higher
dimensions are presented.
"
"stat.TH","  In recent years, various notions of capacity and complexity have been
proposed for characterizing the generalization properties of stochastic
gradient descent (SGD) in deep learning. Some of the popular notions that
correlate well with the performance on unseen data are (i) the `flatness' of
the local minimum found by SGD, which is related to the eigenvalues of the
Hessian, (ii) the ratio of the stepsize $\eta$ to the batch size $b$, which
essentially controls the magnitude of the stochastic gradient noise, and (iii)
the `tail-index', which measures the heaviness of the tails of the network
weights at convergence. In this paper, we argue that these three seemingly
unrelated perspectives for generalization are deeply linked to each other. We
claim that depending on the structure of the Hessian of the loss at the
minimum, and the choices of the algorithm parameters $\eta$ and $b$, the SGD
iterates will converge to a \emph{heavy-tailed} stationary distribution. We
rigorously prove this claim in the setting of quadratic optimization: we show
that even in a simple linear regression problem with independent and
identically distributed Gaussian data, the iterates can be heavy-tailed with
infinite variance. We further characterize the behavior of the tails with
respect to algorithm parameters, the dimension, and the curvature. We then
translate our results into insights about the behavior of SGD in deep learning.
We finally support our theory with experiments conducted on both synthetic data
and fully connected neural networks.
"
"stat.TH","  In this paper we revisit some classic problems on classification under
misspecification. In particular, we study the problem of learning halfspaces
under Massart noise with rate $\eta$. In a recent work, Diakonikolas,
Goulekakis, and Tzamos resolved a long-standing problem by giving the first
efficient algorithm for learning to accuracy $\eta + \epsilon$ for any
$\epsilon > 0$. However, their algorithm outputs a complicated hypothesis,
which partitions space into $\text{poly}(d,1/\epsilon)$ regions. Here we give a
much simpler algorithm and in the process resolve a number of outstanding open
questions:
  (1) We give the first proper learner for Massart halfspaces that achieves
$\eta + \epsilon$. We also give improved bounds on the sample complexity
achievable by polynomial time algorithms.
  (2) Based on (1), we develop a blackbox knowledge distillation procedure to
convert an arbitrarily complex classifier to an equally good proper classifier.
  (3) By leveraging a simple but overlooked connection to evolvability, we show
any SQ algorithm requires super-polynomially many queries to achieve
$\mathsf{OPT} + \epsilon$.
  Moreover we study generalized linear models where $\mathbb{E}[Y|\mathbf{X}] =
\sigma(\langle \mathbf{w}^*, \mathbf{X}\rangle)$ for any odd, monotone, and
Lipschitz function $\sigma$. This family includes the previously mentioned
halfspace models as a special case, but is much richer and includes other
fundamental models like logistic regression. We introduce a challenging new
corruption model that generalizes Massart noise, and give a general algorithm
for learning in this setting. Our algorithms are based on a small set of core
recipes for learning to classify in the presence of misspecification.
  Finally we study our algorithm for learning halfspaces under Massart noise
empirically and find that it exhibits some appealing fairness properties.
"
"stat.TH","  We consider a distributionally robust formulation of stochastic optimization
problems arising in statistical learning, where robustness is with respect to
uncertainty in the underlying data distribution. Our formulation builds on
risk-averse optimization techniques and the theory of coherent risk measures.
It uses semi-deviation risk for quantifying uncertainty, allowing us to compute
solutions that are robust against perturbations in the population data
distribution. We consider a large family of loss functions that can be
non-convex and non-smooth and develop an efficient stochastic subgradient
method. We prove that it converges to a point satisfying the optimality
conditions. To our knowledge, this is the first method with rigorous
convergence guarantees in the context of non-convex non-smooth distributionally
robust stochastic optimization. Our method can achieve any desired level of
robustness with little extra computational cost compared to population risk
minimization. We also illustrate the performance of our algorithm on real
datasets arising in convex and non-convex supervised learning problems.
"
"stat.TH","  Many inference problems, such as sequential decision problems like A/B
testing, adaptive sampling schemes like bandit selection, are often online in
nature. The fundamental problem for online inference is to provide a sequence
of confidence intervals that are valid uniformly over the growing-into-infinity
sample sizes. To address this question, we provide a near-optimal confidence
sequence for bounded random variables by utilizing Bentkus' concentration
results. We show that it improves on the existing approaches that use the
Cram{\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett
inequalities. The resulting confidence sequence is confirmed to be favorable in
both synthetic coverage problems and an application to adaptive stopping
algorithms.
"
"stat.TH","  We propose to tackle the problem of understanding the effect of
regularization in Sinkhorn algotihms. In the case of Gaussian distributions we
provide a closed form for the regularized optimal transport which enables to
provide a better understanding of the effect of the regularization from a
statistical framework.
"
"stat.TH","  We consider the phase retrieval problem of reconstructing a $n$-dimensional
real or complex signal $\mathbf{X}^{\star}$ from $m$ (possibly noisy)
observations $Y_\mu = | \sum_{i=1}^n \Phi_{\mu i} X^{\star}_i/\sqrt{n}|$, for a
large class of correlated real and complex random sensing matrices
$\mathbf{\Phi}$, in a high-dimensional setting where $m,n\to\infty$ while
$\alpha = m/n=\Theta(1)$. First, we derive sharp asymptotics for the lowest
possible estimation error achievable statistically and we unveil the existence
of sharp phase transitions for the weak- and full-recovery thresholds as a
function of the singular values of the matrix $\mathbf{\Phi}$. This is achieved
by providing a rigorous proof of a result first obtained by the replica method
from statistical mechanics. In particular, the information-theoretic transition
to perfect recovery for full-rank matrices appears at $\alpha=1$ (real case)
and $\alpha=2$ (complex case). Secondly, we analyze the performance of the
best-known polynomial time algorithm for this problem -- approximate
message-passing -- establishing the existence of a statistical-to-algorithmic
gap depending, again, on the spectral properties of $\mathbf{\Phi}$. Our work
provides an extensive classification of the statistical and algorithmic
thresholds in high-dimensional phase retrieval for a broad class of random
matrices.
"
"stat.TH","  Fiducial inference, as generalized by Hannig et al. (2016), is applied to
nonparametric g-modeling (Efron, 2016) in the discrete case. We propose a
computationally efficient algorithm to sample from the fiducial distribution,
and use generated samples to construct point estimates and confidence
intervals. We study the theoretical properties of the fiducial distribution and
perform extensive simulations in various scenarios. The proposed approach gives
rise to surprisingly good statistical performance in terms of the mean squared
error of point estimators and coverage of confidence intervals. Furthermore, we
apply the proposed fiducial method to estimate the probability of each
satellite site being malignant using gastric adenocarcinoma data with 844
patients (Efron, 2016).
"
"stat.TH","  The study of records in the Linear Drift Model (LDM) has attracted much
attention recently due to applications in several fields. In the present paper
we study $\delta$-records in the LDM, defined as observations which are greater
than all previous observations, plus a fixed real quantity $\delta$. We give
analytical properties of the probability of $\delta$-records and study the
correlation between $\delta$-record events. We also analyse the asymptotic
behaviour of the number of $\delta$-records among the first $n$ observations
and give conditions for convergence to the Gaussian distribution. As a
consequence of our results, we solve a conjecture posed in J. Stat. Mech. 2010,
P10013, regarding the total number of records in a LDM with negative drift.
Examples of application to particular distributions, such as Gumbel or Pareto
are also provided. We illustrate our results with a real data set of summer
temperatures in Spain, where the LDM is consistent with the global-warming
phenomenon.
"
"stat.TH","  As learning solutions reach critical applications in social, industrial, and
medical domains, the need to curtail their behavior becomes paramount. There is
now ample evidence that without explicit tailoring, learning can lead to
biased, unsafe, and prejudiced solutions. To tackle these problems, we develop
a generalization theory of constrained learning based on the probably
approximately correct (PAC) learning framework. In particular, we show that
imposing requirements does not make a learning problem harder in the sense that
any PAC learnable class is also PAC constrained learnable using a constrained
counterpart of the empirical risk minimization (ERM) rule. For typical
parametrized models, however, this learner involves solving a non-convex
optimization program for which even obtaining a feasible solution may be hard.
To overcome this issue, we prove that under mild conditions the empirical dual
problem of constrained learning is also a PAC constrained learner that now
leads to a practical constrained learning algorithm. We analyze the
generalization properties of this solution and use it to illustrate how
constrained learning can address problems in fair and robust classification.
"
"stat.TH","  We establish a nonasymptotic lower bound on the $L_2$ minimax risk for a
class of generalized linear models. It is further shown that the minimax risk
for the canonical linear model matches this lower bound up to a universal
constant. Therefore, the canonical linear model may be regarded as most
favorable among the considered class of generalized linear models (in terms of
minimax risk). The proof makes use of an information-theoretic Bayesian
Cram\'er-Rao bound for log-concave priors, established by Aras et al. (2019).
"
"stat.TH","  In this note we describe in detail how to apply the pool-adjacent-violators
algorithm (PAVA) efficiently in the context of estimating stochastically
ordered distribution functions. The main idea is that the solution of a
weighted monotone least squares problem changes only little if one component of
the target vector to be approximated is changed.
"
"stat.TH","  Non-parametric and distribution-free two-sample tests have been the
foundation of many change point detection algorithms. However, randomness in
the test statistic as a function of time makes them susceptible to false
positives and localization ambiguity. We address these issues by deriving and
applying filters matched to the expected temporal signatures of a change for
various sliding window, two-sample tests under IID assumptions on the data.
These filters are derived asymptotically with respect to the window size for
the Wasserstein quantile test, the Wasserstein-1 distance test, Maximum Mean
Discrepancy squared (MMD^2), and the Kolmogorov-Smirnov (KS) test. The matched
filters are shown to have two important properties. First, they are
distribution-free, and thus can be applied without prior knowledge of the
underlying data distributions. Second, they are peak-preserving, which allows
the filtered signal produced by our methods to maintain expected statistical
significance. Through experiments on synthetic data as well as activity
recognition benchmarks, we demonstrate the utility of this approach for
mitigating false positives and improving the test precision. Our method allows
for the localization of change points without the use of ad-hoc post-processing
to remove redundant detections common to current methods. We further highlight
the performance of statistical tests based on the Quantile-Quantile (Q-Q)
function and show how the invariance property of the Q-Q function to
order-preserving transformations allows these tests to detect change points of
different scales with a single threshold within the same dataset.
"
"stat.TH","  In this paper, we consider a property of univariate Gaussian distributions
namely conditional expectation shift (or centroid shift). Specifically, we
compare two Gaussian distributions in which they differ only in their means.
Equivalently, we can view this situation as one of the distribution is shifted
to the right. These two distributions are conditioned on the same event in
which the realizations fall in the right interval or left interval. We show
that if a Gaussian distribution is shifted to the right while the conditioning
event remains the same then the conditional expectation is shifted to the right
concurrently.
"
"stat.TH","  Suppose an online platform wants to compare a treatment and control policy,
e.g., two different matching algorithms in a ridesharing system, or two
different inventory management algorithms in an online retail site. Standard
randomized controlled trials are typically not feasible, since the goal is to
estimate policy performance on the entire system. Instead, the typical current
practice involves dynamically alternating between the two policies for fixed
lengths of time, and comparing the average performance of each over the
intervals in which they were run as an estimate of the treatment effect.
However, this approach suffers from *temporal interference*: one algorithm
alters the state of the system as seen by the second algorithm, biasing
estimates of the treatment effect. Further, the simple non-adaptive nature of
such designs implies they are not sample efficient.
  We develop a benchmark theoretical model in which to study optimal
experimental design for this setting. We view testing the two policies as the
problem of estimating the steady state difference in reward between two unknown
Markov chains (i.e., policies). We assume estimation of the steady state reward
for each chain proceeds via nonparametric maximum likelihood, and search for
consistent (i.e., asymptotically unbiased) experimental designs that are
efficient (i.e., asymptotically minimum variance). Characterizing such designs
is equivalent to a Markov decision problem with a minimum variance objective;
such problems generally do not admit tractable solutions. Remarkably, in our
setting, using a novel application of classical martingale analysis of Markov
chains via Poisson's equation, we characterize efficient designs via a succinct
convex optimization problem. We use this characterization to propose a
consistent, efficient online experimental design that adaptively samples the
two Markov chains.
"
"stat.TH","  Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data--an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributional robust policy with
incomplete (bandit) observational data. We propose a novel learning algorithm
that is able to learn a robust policy to adversarial perturbations and unknown
covariate shifts. We first present a policy evaluation procedure in the
ambiguous environment and then give a performance guarantee based on the theory
of uniform convergence. Additionally, we also give a heuristic algorithm to
solve the distributional robust policy learning problems efficiently.
"
"stat.TH","  In a recent paper, Mazucheli et al. (2019) introduced the unit-Gompertz (UG)
distribution and studied some of its properties. It is a continuous
distribution with bounded support, and hence may be useful for modelling
life-time phenomena. We present counter-examples to point out some subtle
errors in their work, and subsequently correct them. We also look at some other
interesting properties of this new distribution. Further, we also study some
important reliability measures and consider some stochastic orderings
associated with this new distribution.
"
"stat.TH","  In this paper, we draw attention to a problem that is often overlooked or
ignored by companies practicing hypothesis testing (A/B testing) in online
environments. We show that conducting experiments on limited inventory that is
shared between variants in the experiment can lead to high false positive rates
since the core assumption of independence between the groups is violated. We
provide a detailed analysis of the problem in a simplified setting whose
parameters are informed by realistic scenarios. The setting we consider is a
$2$-dimensional random walk in a semi-infinite strip. It is rich enough to take
a finite inventory into account, but is at the same time simple enough to allow
for a closed form of the false-positive probability. We prove that high
false-positive rates can occur, and develop tools that are suitable to help
design adequate tests in follow-up work. Our results also show that high
false-negative rates may occur. The proofs rely on a functional limit theorem
for the $2$-dimensional random walk in a semi-infinite strip.
"
"stat.TH","  We consider the linear model $\mathbf{y} = \mathbf{X} \mathbf{\beta}_\star +
\mathbf{\epsilon}$ with $\mathbf{X}\in \mathbb{R}^{n\times p}$ in the
overparameterized regime $p>n$. We estimate $\mathbf{\beta}_\star$ via
generalized (weighted) ridge regression: $\hat{\mathbf{\beta}}_\lambda =
\left(\mathbf{X}^T\mathbf{X} + \lambda \mathbf{\Sigma}_w\right)^\dagger
\mathbf{X}^T\mathbf{y}$, where $\mathbf{\Sigma}_w$ is the weighting matrix.
Under a random design setting with general data covariance $\mathbf{\Sigma}_x$
and anisotropic prior on the true coefficients
$\mathbb{E}\mathbf{\beta}_\star\mathbf{\beta}_\star^T = \mathbf{\Sigma}_\beta$,
we provide an exact characterization of the prediction risk
$\mathbb{E}(y-\mathbf{x}^T\hat{\mathbf{\beta}}_\lambda)^2$ in the proportional
asymptotic limit $p/n\rightarrow \gamma \in (1,\infty)$. Our general setup
leads to a number of interesting findings. We outline precise conditions that
decide the sign of the optimal setting $\lambda_{\rm opt}$ for the ridge
parameter $\lambda$ and confirm the implicit $\ell_2$ regularization effect of
overparameterization, which theoretically justifies the surprising empirical
observation that $\lambda_{\rm opt}$ can be negative in the overparameterized
regime. We also characterize the double descent phenomenon for principal
component regression (PCR) when both $\mathbf{X}$ and $\mathbf{\beta}_\star$
are anisotropic. Finally, we determine the optimal weighting matrix
$\mathbf{\Sigma}_w$ for both the ridgeless ($\lambda\to 0$) and optimally
regularized ($\lambda = \lambda_{\rm opt}$) case, and demonstrate the advantage
of the weighted objective over standard ridge regression and PCR.
"
"stat.TH","  The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and
Integral Probability Metrics (e.g. total variation distance or maximum mean
discrepancies) are widely used to quantify the similarity between probability
distributions. In this work, we systematically study the relationship between
these two families from the perspective of convex duality. Starting from a
tight variational representation of the $f$-divergence, we derive a
generalization of the moment-generating function, which we show exactly
characterizes the best lower bound of the $f$-divergence as a function of a
given IPM. Using this characterization, we obtain new bounds while also
recovering in a unified manner well-known results, such as Hoeffding's lemma,
Pinsker's inequality and its extension to subgaussian functions, and the
Hammersley-Chapman-Robbins bound. The variational representation also allows us
to prove new results on topological properties of the divergence which may be
of independent interest.
"
"stat.TH","  We consider the problem of structure learning for linear causal models based
on observational data. We treat models given by possibly cyclic mixed graphs,
which allow for feedback loops and effects of latent confounders. Generalizing
related work on bow-free acyclic graphs, we assume that the underlying graph is
simple. This entails that any two observed variables can be related through at
most one direct causal effect and that (confounding-induced) correlation
between error terms in structural equations occurs only in absence of direct
causal effects. We show that, despite new subtleties in the cyclic case, the
considered simple cyclic models are of expected dimension and that a previously
considered criterion for distributional equivalence of bow-free acyclic graphs
has an analogue in the cyclic case. Our result on model dimension justifies in
particular score-based methods for structure learning of linear Gaussian mixed
graph models, which we implement via greedy search.
"
"stat.TH","  In the Bayesian literature on model comparison, Bayes factors play the
leading role. In the classical statistical literature, model selection criteria
are often devised used cross-validation ideas. Amalgamating the ideas of Bayes
factor and cross-validation Geisser and Eddy (1979) created the pseudo-Bayes
factor. The usage of cross-validation inculcates several theoretical
advantages, computational simplicity and numerical stability in Bayes factors
as the marginal density of the entire dataset is replaced with products of
cross-validation densities of individual data points.
  However, the popularity of pseudo-Bayes factors is still negligible in
comparison with Bayes factors, with respect to both theoretical investigations
and practical applications. In this article, we establish almost sure
exponential convergence of pseudo-Bayes factors for large samples under a
general setup consisting of dependent data and model misspecifications. We
particularly focus on general parametric and nonparametric regression setups in
both forward and inverse contexts.
  We illustrate our theoretical results with various examples, providing
explicit calculations. We also supplement our asymptotic theory with simulation
experiments in small sample situations of Poisson log regression and geometric
logit and probit regression, additionally addressing the variable selection
problem. We consider both linear and nonparametric regression modeled by
Gaussian processes for our purposes. Our simulation results provide quite
interesting insights into the usage of pseudo-Bayes factors in forward and
inverse setups.
"
"stat.TH","  Sampling from a log-concave distribution function is one core problem that
has wide applications in Bayesian statistics and machine learning. While most
gradient free methods have slow convergence rate, the Langevin Monte Carlo
(LMC) that provides fast convergence requires the computation of gradients. In
practice one uses finite-differencing approximations as surrogates, and the
method is expensive in high-dimensions.
  A natural strategy to reduce computational cost in each iteration is to
utilize random gradient approximations, such as random coordinate descent (RCD)
or simultaneous perturbation stochastic approximation (SPSA). We show by a
counter-example that blindly applying RCD does not achieve the goal in the most
general setting. The high variance induced by the randomness means a larger
number of iterations are needed, and this balances out the saving in each
iteration.
  We then introduce a new variance reduction approach, termed Randomized
Coordinates Averaging Descent (RCAD), and incorporate it with both overdamped
and underdamped LMC. The methods are termed RCAD-O-LMC and RCAD-U-LMC
respectively. The methods still sit in the random gradient approximation
framework, and thus the computational cost in each iteration is low. However,
by employing RCAD, the variance is reduced, so the methods converge within the
same number of iterations as the classical overdamped and underdamped LMC. This
leads to a computational saving overall.
"
"stat.TH","  We analyze in a closed form the learning dynamics of stochastic gradient
descent (SGD) for a single layer neural network classifying a high-dimensional
Gaussian mixture where each cluster is assigned one of two labels. This problem
provides a prototype of a non-convex loss landscape with interpolating regimes
and a large generalization gap. We define a particular stochastic process for
which SGD can be extended to a continuous-time limit that we call stochastic
gradient flow. In the full-batch limit we recover the standard gradient flow.
We apply dynamical mean-field theory from statistical physics to track the
dynamics of the algorithm in the high-dimensional limit via a self-consistent
stochastic process. We explore the performance of the algorithm as a function
of control parameters shedding light on how it navigates the loss landscape.
"
"stat.TH","  Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational
statistics, machine learning and optimisation. Recent years have brought an
influx of interest in SGAs and the non-asymptotic analysis of their bias is by
now well-developed. However, in order to fully understand the efficiency of
Monte Carlo algorithms utilizing stochastic gradients, one also needs to carry
out the analysis of their variance, which turns out to be problem-specific. For
this reason, there is no systematic theory that would specify the optimal
choice of the random approximation of the gradient in SGAs for a given data
regime. Furthermore, while there have been numerous attempts to reduce the
variance of SGAs, these typically exploit a particular structure of the sampled
distribution. In this paper we use the Multi-index Monte Carlo apparatus
combined with the antithetic approach to construct the Multi-index Antithetic
Stochastic Gradient Algorithm (MASGA), which can be used to sample from any
probability distribution. This, to our knowledge, is the first SGA that, for
all data regimes and without relying on any specific structure of the target
measure, achieves performance on par with Monte Carlo estimators that have
access to unbiased samples from the distribution of interest. In other words,
MASGA is an optimal estimator from the error-computational cost perspective
within the class of Monte Carlo estimators.
"
"stat.TH","  Evaluating treatment effect heterogeneity widely informs treatment decision
making. At the moment, much emphasis is placed on the estimation of the
conditional average treatment effect via flexible machine learning algorithms.
While these methods enjoy some theoretical appeal in terms of consistency and
convergence rates, they generally perform poorly in terms of uncertainty
quantification. This is troubling since assessing risk is crucial for reliable
decision-making in sensitive and uncertain environments. In this work, we
propose a conformal inference-based approach that can produce reliable interval
estimates for counterfactuals and individual treatment effects under the
potential outcome framework. For completely randomized or stratified randomized
experiments with perfect compliance, the intervals have guaranteed average
coverage in finite samples regardless of the unknown data generating mechanism.
For randomized experiments with ignorable compliance and general observational
studies obeying the strong ignorability assumption, the intervals satisfy a
doubly robust property which states the following: the average coverage is
approximately controlled if either the propensity score or the conditional
quantiles of potential outcomes can be estimated accurately. Numerical studies
on both synthetic and real datasets empirically demonstrate that existing
methods suffer from a significant coverage deficit even in simple models. In
contrast, our methods achieve the desired coverage with reasonably short
intervals.
"
"stat.TH","  Although stochastic optimization is central to modern machine learning, the
precise mechanisms underlying its success, and in particular, the precise role
of the stochasticity, still remain unclear. Modelling stochastic optimization
algorithms as discrete random recurrence relations, we show that multiplicative
noise, as it commonly arises due to variance in local rates of convergence,
results in heavy-tailed stationary behaviour in the parameters. A detailed
analysis is conducted for SGD applied to a simple linear regression problem,
followed by theoretical results for a much larger class of models (including
non-linear and non-convex) and optimizers (including momentum, Adam, and
stochastic Newton), demonstrating that our qualitative results hold much more
generally. In each case, we describe dependence on key factors, including step
size, batch size, and data variability, all of which exhibit similar
qualitative behavior to recent empirical results on state-of-the-art neural
network models from computer vision and natural language processing.
Furthermore, we empirically demonstrate how multiplicative noise and
heavy-tailed structure improve capacity for basin hopping and exploration of
non-convex loss surfaces, over commonly-considered stochastic dynamics with
only additive noise and light-tailed structure.
"
"stat.TH","  The fiducial coincides with the posterior in a group model equipped with the
right Haar prior. This result is here generalized. For this the underlying
probability space of Kolmogorov is replaced by a $\sigma$-finite measure space
and fiducial theory is presented within this frame. Examples are presented that
demonstrate that this also gives good alternatives to existing Bayesian
sampling methods. It is proved that the results provided here for fiducial
models imply that the theory of invariant measures for groups cannot be
generalized directly to loops: There exist a smooth one-dimensional loop where
an invariant measure does not exist.
  Keywords: Conditional sampling, Improper prior, Haar prior, Sufficient
statistic, Quasi-group
"
"stat.TH","  We analyze the prediction performance of ridge and ridgeless regression when
both the number and the dimension of the data go to infinity. In particular, we
consider a general setting introducing prior assumptions characterizing ""easy""
and ""hard"" learning problems. In this setting, we show that ridgeless (zero
regularisation) regression is optimal for easy problems with a high signal to
noise. Furthermore, we show that additional descents in the ridgeless bias and
variance learning curve can occur beyond the interpolating threshold, verifying
recent empirical observations. More generally, we show how a variety of
learning curves are possible depending on the problem at hand. From a technical
point of view, characterising the influence of prior assumptions requires
extending previous applications of random matrix theory to study ridge
regression.
"
"stat.TH","  We study the efficient PAC learnability of halfspaces in the presence of
Tsybakov noise. In the Tsybakov noise model, each label is independently
flipped with some probability which is controlled by an adversary. This noise
model significantly generalizes the Massart noise model, by allowing the
flipping probabilities to be arbitrarily close to $1/2$ for a fraction of the
samples. Our main result is the first non-trivial PAC learning algorithm for
this problem under a broad family of structured distributions -- satisfying
certain concentration and (anti-)anti-concentration properties -- including
log-concave distributions. Specifically, we given an algorithm that achieves
misclassification error $\epsilon$ with respect to the true halfspace, with
quasi-polynomial runtime dependence in $1/\epsilin$. The only previous upper
bound for this problem -- even for the special case of log-concave
distributions -- was doubly exponential in $1/\epsilon$ (and follows via the
naive reduction to agnostic learning). Our approach relies on a novel
computationally efficient procedure to certify whether a candidate solution is
near-optimal, based on semi-definite programming. We use this certificate
procedure as a black-box and turn it into an efficient learning algorithm by
searching over the space of halfspaces via online convex optimization.
"
"stat.TH","  The use of Gaussian processes (GPs) is supported by efficient sampling
algorithms, a rich methodological literature, and strong theoretical grounding.
However, due to their prohibitive computation and storage demands, the use of
exact GPs in Bayesian models is limited to problems containing at most several
thousand observations. Sampling requires matrix operations that scale at
$\mathcal{O}(n^3),$ where $n$ is the number of unique inputs. Storage of
individual matrices scales at $\mathcal{O}(n^2),$ and can quickly overwhelm the
resources of most modern computers. To overcome these bottlenecks, we develop a
sampling algorithm using $\mathcal{H}$ matrix approximation of the matrices
comprising the GP posterior covariance. These matrices can approximate the true
conditional covariance matrix within machine precision and allow for sampling
algorithms that scale at $\mathcal{O}(n \ \mbox{log}^2 n)$ time and storage
demands scaling at $\mathcal{O}(n \ \mbox{log} \ n).$ We also describe how
these algorithms can be used as building blocks to model higher dimensional
surfaces at $\mathcal{O}(d \ n \ \mbox{log}^2 n)$, where $d$ is the dimension
of the surface under consideration, using tensor products of one-dimensional
GPs. Though various scalable processes have been proposed for approximating
Bayesian GP inference when $n$ is large, to our knowledge, none of these
methods show that the approximation's Kullback-Leibler divergence to the true
posterior can be made arbitrarily small and may be no worse than the
approximation provided by finite computer arithmetic. We describe
$\mathcal{H}-$matrices, give an efficient Gibbs sampler using these matrices
for one-dimensional GPs, offer a proposed extension to higher dimensional
surfaces, and investigate the performance of this fast increased fidelity
approximate GP, FIFA-GP, using both simulated and real data sets.
"
"stat.TH","  Motivated by the increasing popularity and the seemingly broad applicability
of pair-copula constructions underlined by numerous publications in the last
decade, in this contribution we tackle the unavoidable question on how flexible
and simplifying the commonly used 'simplifying assumption' is from an analytic
perspective and provide answers to two related open questions posed by Nagler
and Czado in 2016. Aiming at a simplest possible setup for deriving the main
results we first focus on the three-dimensional setting. We prove flexibility
of simplified copulas in the sense that they are dense in the family of all
three-dimensional copulas with respect to the uniform metric $d_\infty$, show
that the partial vine copula is never the optimal simplified copula
approximation of a given, non-simplified copula $C$, and derive examples
illustrating that the corresponding approximation error can be strikingly large
and extend to more than 28\% of the diameter of the metric space. Moreover, the
mapping $\psi$ assigning each three-dimensional copula its unique partial vine
copula turns out to be discontinuous with respect to $d_\infty$ (but continuous
with respect to other notions of convergence), implying a surprising
sensitivity of partial vine copula approximations. The afore-mentioned main
results are then extended to the general multivariate setting.
"
"stat.TH","  We consider a commonly studied supervised classification of a synthetic
dataset whose labels are generated by feeding a one-layer neural network with
random iid inputs. We study the generalization performances of standard
classifiers in the high-dimensional regime where $\alpha=n/d$ is kept finite in
the limit of a high dimension $d$ and number of samples $n$. Our contribution
is three-fold: First, we prove a formula for the generalization error achieved
by $\ell_2$ regularized classifiers that minimize a convex loss. This formula
was first obtained by the heuristic replica method of statistical physics.
Secondly, focussing on commonly used loss functions and optimizing the $\ell_2$
regularization strength, we observe that while ridge regression performance is
poor, logistic and hinge regression are surprisingly able to approach the
Bayes-optimal generalization error extremely closely. As $\alpha \to \infty$
they lead to Bayes-optimal rates, a fact that does not follow from predictions
of margin-based generalization error bounds. Third, we design an optimal loss
and regularizer that provably leads to Bayes-optimal generalization error.
"
"stat.TH","  The choice of step-size used in Stochastic Gradient Descent (SGD)
optimization is empirically selected in most training procedures. Moreover, the
use of scheduled learning techniques such as Step-Decaying, Cyclical-Learning,
and Warmup to tune the step-size requires extensive practical
experience--offering limited insight into how the parameters update--and is not
consistent across applications. This work attempts to answer a question of
interest to both researchers and practitioners, namely \textit{""how much
knowledge is gained in iterative training of deep neural networks?""} Answering
this question introduces two useful metrics derived from the singular values of
the low-rank factorization of convolution layers in deep neural networks. We
introduce the notions of \textit{""knowledge gain""} and \textit{""mapping
condition""} and propose a new algorithm called Adaptive Scheduling (AdaS) that
utilizes these derived metrics to adapt the SGD learning rate proportionally to
the rate of change in knowledge gain over successive iterations.
Experimentation reveals that, using the derived metrics, AdaS exhibits: (a)
faster convergence and superior generalization over existing adaptive learning
methods; and (b) lack of dependence on a validation set to determine when to
stop training. Code is available at
\url{https://github.com/mahdihosseini/AdaS}.
"
"stat.TH","  We present simple differentially private estimators for the mean and
covariance of multivariate sub-Gaussian data that are accurate at small sample
sizes. We demonstrate the effectiveness of our algorithms both theoretically
and empirically using synthetic and real-world datasets---showing that their
asymptotic error rates match the state-of-the-art theoretical bounds, and that
they concretely outperform all previous methods. Specifically, previous
estimators either have weak empirical accuracy at small sample sizes, perform
poorly for multivariate data, or require the user to provide strong a priori
estimates for the parameters.
"
"stat.TH","  We classify the two-way independence quasi-independence models (or
independence models with structural zeros) that have rational maximum
likelihood estimators, or MLEs. We give a necessary and sufficient condition on
the bipartite graph associated to the model for the MLE to be rational. In this
case, we give an explicit formula for the MLE in terms of combinatorial
features of this graph. We also use the Horn uniformization to show that for
general log-linear models $\mathcal{M}$ with rational MLE, any model obtained
by restricting to a face of the cone of sufficient statistics of $\mathcal{M}$
also has rational MLE.
"
"stat.TH","  We propose a robust and scalable procedure for general optimization and
inference problems on manifolds leveraging the classical idea of
`median-of-means' estimation. This is motivated by ubiquitous examples and
applications in modern data science in which a statistical learning problem can
be cast as an optimization problem over manifolds. Being able to incorporate
the underlying geometry for inference while addressing the need for robustness
and scalability presents great challenges. We address these challenges by first
proving a key lemma that characterizes some crucial properties of geometric
medians on manifolds. In turn, this allows us to prove robustness and tighter
concentration of our proposed final estimator in a subsequent theorem. This
estimator aggregates a collection of subset estimators by taking their
geometric median over the manifold. We illustrate bounds on this estimator via
calculations in explicit examples. The robustness and scalability of the
procedure is illustrated in numerical examples on both simulated and real data
sets.
"
"stat.TH","  The weighted forms of generalized survival and failure entropies of order
($\alpha,\beta$) are proposed and some properties are obtained. We further
propose the dynamic versions of weighted generalized survival and failures
entropies and obtained some properties and bounds. Characterization for
Rayleigh and power distributions are done by dynamic weighted generalized
entropies. We further consider the empirical versions of generalized weighted
survival and failure entropies and using the difference between theoretical and
empirical survival entropies a test for exponentiality is considered.
"
"stat.TH","  Let $\rho$ and $\pi$ be two probability measures on $[-1,1]^d$ with positive
and analytic Lebesgue densities. We investigate the approximation of the unique
triangular monotone (Knothe-Rosenblatt) transport $T:[-1,1]^d\to [-1,1]^d$,
such that the pushforward $T_\sharp\rho$ equals $\pi$. It is shown that for
$d\in\mathbb{N}$ there exist approximations $\tilde T$ of $T$ based on either
sparse polynomial expansions or ReLU networks, such that the distance between
$\tilde T_\sharp\rho$ and $\pi$ decreases exponentially. More precisely, we
show error bounds of the type $\exp(-\beta N^{1/d})$ (or $\exp(-\beta
N^{1/(d+1)})$ for neural networks), where $N$ refers to the dimension of the
ansatz space (or the size of the network) containing $\tilde T$; the notion of
distance comprises, among others, the Hellinger distance and the
Kullback--Leibler divergence. The construction guarantees $\tilde T$ to be a
monotone triangular bijective transport on the hypercube $[-1,1]^d$. Analogous
results hold for the inverse transport $S=T^{-1}$. The proofs are constructive,
and we give an explicit a priori description of the ansatz space, which can be
used for numerical implementations. Additionally we discuss the
high-dimensional case: for $d=\infty$ a dimension-independent algebraic
convergence rate is proved for a class of probability measures occurring widely
in Bayesian inference for uncertainty quantification, thus verifying that the
curse of dimensionality can be overcome in this setting.
"
"stat.TH","  Despite the widespread use of gradient-based algorithms for optimizing
high-dimensional non-convex functions, understanding their ability of finding
good minima instead of being trapped in spurious ones remains to a large extent
an open problem. Here we focus on gradient flow dynamics for phase retrieval
from random measurements. When the ratio of the number of measurements over the
input dimension is small the dynamics remains trapped in spurious minima with
large basins of attraction. We find analytically that above a critical ratio
those critical points become unstable developing a negative direction toward
the signal. By numerical experiments we show that in this regime the gradient
flow algorithm is not trapped; it drifts away from the spurious critical points
along the unstable direction and succeeds in finding the global minimum. Using
tools from statistical physics we characterize this phenomenon, which is
related to a BBP-type transition in the Hessian of the spurious minima.
"
"stat.TH","  We propose a theoretical study of two realistic estimators of conditional
distribution functions and conditional quantiles using random forests. The
estimation process uses the bootstrap samples generated from the original
dataset when constructing the forest. Bootstrap samples are reused to define
the first estimator, while the second requires only the original sample, once
the forest has been built. We prove that both proposed estimators of the
conditional distribution functions are consistent uniformly a.s. To the best of
our knowledge, it is the first proof of consistency including the bootstrap
part. We also illustrate the estimation procedures on a numerical example.
"
"stat.TH","  We introduce Markov Random Geometric Graphs (MRGGs), a growth model for
temporal dynamic networks. It is based on a Markovian latent space dynamic:
consecutive latent points are sampled on the Euclidean Sphere using an unknown
Markov kernel; and two nodes are connected with a probability depending on a
unknown function of their latent geodesic distance. More precisely, at each
stamp-time k we add a latent point X k sampled by jumping from the previous one
X k--1 in a direction chosen uniformly Y k and with a length r k drawn from an
unknown distribution called the latitude function. The connection probabilities
between each pair of nodes are equal to the envelope function of the distance
between these two latent points. We provide theoretical guarantees for the
non-parametric estimation of the latitude and the envelope functions. We
propose an efficient algorithm that achieves those non-parametric estimation
tasks based on an ad-hoc Hierarchical Agglomerative Clustering approach, and we
deploy this analysis on a real data-set given by exchange of messages on a
social network.
"
"stat.TH","  In this paper, we consider the problem of estimating the density function of
a Chi-squared variable on the basis of observations of another Chi-squared
variable and a normal variable under the Kullback-Leibler divergence. We assume
that these variables have a common unknown scale parameter and that the mean of
the normal variable is also unknown. We compare the risk functions of two
Bayesian predictive densities: one with respect to a hierarchical shrinkage
prior and the other based on a noninformative prior. The hierarchical Bayesian
predictive density depends on the normal variable while the Bayesian predictive
density based on the noninformative prior does not. Sufficient conditions for
the former to dominate the latter are obtained. These predictive densities are
compared by simulation.
"
"stat.TH","  Looking at bivariate copulas from the perspective of conditional
distributions and considering weak convergence of almost all conditional
distributions yields the notion of weak conditional convergence. At first
glance, this notion of convergence for copulas might seem far too restrictive
to be of any practical importance - in fact, given samples of a copula $C$ the
corresponding empirical copulas do not converge weakly conditional to $C$ with
probability one in general. Within the class of Archimedean copulas and the
class of Extreme Value copulas, however, standard pointwise convergence and
weak conditional convergence can even be proved to be equivalent. Moreover, it
can be shown that every copula $C$ is the weak conditional limit of a sequence
of checkerboard copulas. After proving these three main results and pointing
out some consequences we sketch some implications for two recently introduced
dependence measures and for the nonparametric estimation of Archimedean and
Extreme Value copulas.
"
"stat.TH","  In this paper, an approximate version of the Barndorff-Nielsen and Shephard
model, driven by a Brownian motion and a L\'evy subordinator, is formulated.
The first-exit time of the log-return process for this model is analyzed. It is
shown that with certain probability, the first-exit time process of the
log-return is decomposable into the sum of the first exit time of the Brownian
motion with drift, and the first exit time of a L\'evy subordinator with drift.
Subsequently, the probability density functions of the first exit time of some
specific L\'evy subordinators, connected to stationary, self-decomposable
variance processes, are studied. Analytical expressions of the probability
density function of the first-exit time of three such L\'evy subordinators are
obtained in terms of various special functions. The results are implemented to
empirical S&P 500 dataset.
"
"stat.TH","  We develop an approach for estimating models described via conditional moment
restrictions, with a prototypical application being non-parametric instrumental
variable regression. We introduce a min-max criterion function, under which the
estimation problem can be thought of as solving a zero-sum game between a
modeler who is optimizing over the hypothesis space of the target model and an
adversary who identifies violating moments over a test function space. We
analyze the statistical estimation rate of the resulting estimator for
arbitrary hypothesis spaces, with respect to an appropriate analogue of the
mean squared error metric, for ill-posed inverse problems. We show that when
the minimax criterion is regularized with a second moment penalty on the test
function and the test function space is sufficiently rich, then the estimation
rate scales with the critical radius of the hypothesis and test function
spaces, a quantity which typically gives tight fast rates. Our main result
follows from a novel localized Rademacher analysis of statistical learning
problems defined via minimax objectives. We provide applications of our main
results for several hypothesis spaces used in practice such as: reproducing
kernel Hilbert spaces, high dimensional sparse linear functions, spaces defined
via shape constraints, ensemble estimators such as random forests, and neural
networks. For each of these applications we provide computationally efficient
optimization methods for solving the corresponding minimax problem (e.g.
stochastic first-order heuristics for neural networks). In several
applications, we show how our modified mean squared error rate, combined with
conditions that bound the ill-posedness of the inverse problem, lead to mean
squared error rates. We conclude with an extensive experimental analysis of the
proposed methods.
"
"stat.TH","  We present new PAC-Bayesian generalisation bounds for learning problems with
unbounded loss functions. This extends the relevance and applicability of the
PAC-Bayes learning framework, where most of the existing literature focuses on
supervised learning problems with a bounded loss function (typically assumed to
take values in the interval [0;1]). In order to relax this assumption, we
propose a new notion called HYPE (standing for \emph{HYPothesis-dependent
rangE}), which effectively allows the range of the loss to depend on each
predictor. Based on this new notion we derive a novel PAC-Bayesian
generalisation bound for unbounded loss functions, and we instantiate it on a
linear regression problem. To make our theory usable by the largest audience
possible, we include discussions on actual computation, practicality and
limitations of our assumptions.
"
"stat.TH","  We study the problem of learning a real-valued function that satisfies the
Demographic Parity constraint. It demands the distribution of the predicted
output to be independent of the sensitive attribute. We consider the case that
the sensitive attribute is available for prediction. We establish a connection
between fair regression and optimal transport theory, based on which we derive
a close form expression for the optimal fair predictor. Specifically, we show
that the distribution of this optimum is the Wasserstein barycenter of the
distributions induced by the standard regression function on the sensitive
groups. This result offers an intuitive interpretation of the optimal fair
prediction and suggests a simple post-processing algorithm to achieve fairness.
We establish risk and distribution-free fairness guarantees for this procedure.
Numerical experiments indicate that our method is very effective in learning
fair models, with a relative increase in error rate that is inferior to the
relative gain in fairness.
"
"stat.TH","  In this paper we propose statistical inference tools for the covariance
operators of functional time series in the two sample and change point problem.
In contrast to most of the literature the focus of our approach is not testing
the null hypothesis of exact equality of the covariance operators. Instead we
propose to formulate the null hypotheses in them form that ""the distance
between the operators is small"", where we measure deviations by the sup-norm.
We provide powerful bootstrap tests for these type of hypotheses, investigate
their asymptotic properties and study their finite sample properties by means
of a simulation study.
"
"stat.TH","  As machine learning becomes more pervasive, the urgency of assuring its
fairness increases. Consider training data that capture the behaviour of
multiple subgroups of some underlying population over time. When the amounts of
training data for the subgroups are not controlled carefully,
under-representation bias may arise. We introduce two natural concepts of
subgroup fairness and instantaneous fairness to address such
under-representation bias in forecasting problems. In particular, we consider
the learning of a linear dynamical system from multiple trajectories of varying
lengths, and the associated forecasting problems. We provide globally
convergent methods for the subgroup-fair and instant-fair estimation using
hierarchies of convexifications of non-commutative polynomial optimisation
problems. We demonstrate both the beneficial impact of fairness considerations
on the statistical performance and the encouraging effects of exploiting
sparsity on the estimators' run-time in our computational experiments.
"
"stat.TH","  Robust methods, though ubiquitous in practice, are yet to be fully understood
in the context of regularized estimation and high dimensions. Even simple
questions become challenging very quickly. For example, classical statistical
theory identifies equivalence between model-averaged and composite quantile
estimation. However, little to nothing is known about such equivalence between
methods that encourage sparsity. This paper provides a toolbox to further study
robustness in these settings and focuses on prediction. In particular, we study
optimally weighted model-averaged as well as composite $l_1$-regularized
estimation. Optimal weights are determined by minimizing the asymptotic mean
squared error. This approach incorporates the effects of regularization,
without the assumption of perfect selection, as is often used in practice. Such
weights are then optimal for prediction quality. Through an extensive
simulation study, we show that no single method systematically outperforms
others. We find, however, that model-averaged and composite quantile estimators
often outperform least-squares methods, even in the case of Gaussian model
noise. Real data application witnesses the method's practical use through the
reconstruction of compressed audio signals.
"
"stat.TH","  Recent research has established sufficient conditions for finite mixture
models to be identifiable from grouped observations. These conditions allow the
mixture components to be nonparametric and have substantial (or even total)
overlap. This work proposes an algorithm that consistently estimates any
identifiable mixture model from grouped observations. Our analysis leverages an
oracle inequality for weighted kernel density estimators of the distribution on
groups, together with a general result showing that consistent estimation of
the distribution on groups implies consistent estimation of mixture components.
A practical implementation is provided for paired observations, and the
approach is shown to outperform existing methods, especially when mixture
components overlap significantly.
"
"stat.TH","  Multivariate Hawkes processes are commonly used to model streaming networked
event data in a wide variety of applications. However, it remains a challenge
to extract reliable inference from complex datasets with uncertainty
quantification. Aiming towards this, we develop a statistical inference
framework to learn causal relationships between nodes from networked data,
where the underlying directed graph implies Granger causality. We provide
uncertainty quantification for the maximum likelihood estimate of the network
multivariate Hawkes process by providing a non-asymptotic confidence set. The
main technique is based on the concentration inequalities of continuous-time
martingales. We compare our method to the previously-derived asymptotic Hawkes
process confidence interval, and demonstrate the strengths of our method in an
application to neuronal connectivity reconstruction.
"
"stat.TH","  We consider the theory of regression on a manifold using reproducing kernel
Hilbert space methods. Manifold models arise in a wide variety of modern
machine learning problems, and our goal is to help understand the effectiveness
of various implicit and explicit dimensionality-reduction methods that exploit
manifold structure. Our first key contribution is to establish a novel
nonasymptotic version of the Weyl law from differential geometry. From this we
are able to show that certain spaces of smooth functions on a manifold are
effectively finite-dimensional, with a complexity that scales according to the
manifold dimension rather than any ambient data dimension. Finally, we show
that given (potentially noisy) function values taken uniformly at random over a
manifold, a kernel regression estimator (derived from the spectral
decomposition of the manifold) yields minimax-optimal error bounds that are
controlled by the effective dimension.
"
"stat.TH","  In the regression model $Y = b(X) +\varepsilon$, where $X$ has a density $f$,
this paper deals with an oracle inequality for an estimator of $bf$, involving
a kernel in the sense of Lerasle et al. (2016), selected via the PCO method. In
addition to the bandwidth selection for kernel-based estimators already studied
in Lacour, Massart and Rivoirard (2017) and Comte and Marie (2020), the
dimension selection for anisotropic projection estimators of $f$ and $bf$ is
covered.
"
"stat.TH","  The problem of learning graphons has attracted considerable attention across
several scientific communities, with significant progress over the recent years
in sparser regimes. Yet, the current techniques still require diverging degrees
in order to succeed with efficient algorithms in the challenging cases where
the local structure of the graph is homogeneous. This paper provides an
efficient algorithm to learn graphons in the constant expected degree regime.
The algorithm is shown to succeed in estimating the rank-$k$ projection of a
graphon in the $L_2$ metric if the top $k$ eigenvalues of the graphon satisfy a
generalized Kesten-Stigum condition.
"
"stat.TH","  We study minimax estimation of two-dimensional totally positive
distributions. Such distributions pertain to pairs of strongly positively
dependent random variables and appear frequently in statistics and probability.
In particular, for distributions with $\beta$-H\""older smooth densities where
$\beta \in (0, 2)$, we observe polynomially faster minimax rates of estimation
when, additionally, the total positivity condition is imposed. Moreover, we
demonstrate fast algorithms to compute the proposed estimators and corroborate
the theoretical rates of estimation by simulation studies.
"
"stat.TH","  Structured non-convex learning problems, for which critical points have
favorable statistical properties, arise frequently in statistical machine
learning. Algorithmic convergence and statistical estimation rates are
well-understood for such problems. However, quantifying the uncertainty
associated with the underlying training algorithm is not well-studied in the
non-convex setting. In order to address this shortcoming, in this work, we
establish an asymptotic normality result for the constant step size stochastic
gradient descent (SGD) algorithm--a widely used algorithm in practice.
Specifically, based on the relationship between SGD and Markov Chains [DDB19],
we show that the average of SGD iterates is asymptotically normally distributed
around the expected value of their unique invariant distribution, as long as
the non-convex and non-smooth objective function satisfies a dissipativity
property. We also characterize the bias between this expected value and the
critical points of the objective function under various local regularity
conditions. Together, the above two results could be leveraged to construct
confidence intervals for non-convex problems that are trained using the SGD
algorithm.
"
"stat.TH","  A recurrence formula for absolute central moments of Poisson distribution is
suggested.
"
"stat.TH","  Many problems in statistics and machine learning require the reconstruction
of a rank-one signal matrix from noisy data. Enforcing additional prior
information on the rank-one component is often key to guaranteeing good
recovery performance. One such prior on the low-rank component is sparsity,
giving rise to the sparse principal component analysis problem. Unfortunately,
there is strong evidence that this problem suffers from a
computational-to-statistical gap, which may be fundamental. In this work, we
study an alternative prior where the low-rank component is in the range of a
trained generative network. We provide a non-asymptotic analysis with optimal
sample complexity, up to logarithmic factors, for rank-one matrix recovery
under an expansive-Gaussian network prior. Specifically, we establish a
favorable global optimization landscape for a nonlinear least squares
objective, provided the number of samples is on the order of the dimensionality
of the input to the generative model. This result suggests that generative
priors have no computational-to-statistical gap for structured rank-one matrix
recovery in the finite data, nonasymptotic regime. We present this analysis in
the case of both the Wishart and Wigner spiked matrix models.
"
"stat.TH","  We are interested in recovering information on a stochastic block model from
the subgraph discovered by an exploring random walk. Stochastic block models
correspond to populations structured into a finite number of types, where two
individuals are connected by an edge independently from the other pairs and
with a probability depending on their types. We consider here the dense case
where the random network can be approximated by a graphon. This problem is
motivated from the study of chain-referral surveys where each interviewee
provides information on her/his contacts in the social network. First, we write
the likelihood of the subgraph discovered by the random walk: biases are
appearing since hubs and majority types are more likely to be sampled. Even for
the case where the types are observed, the maximum likelihood estimator is not
explicit any more. When the types of the vertices is unobserved, we use an SAEM
algorithm to maximize the likelihood. Second, we propose a different estimation
strategy using new results by Athreya and Roellin. It consists in de-biasing
the maximum likelihood estimator proposed in Daudin et al. and that ignores the
biases.
"
"stat.TH","  The squared Wasserstein distance is a natural quantity to compare probability
distributions in a non-parametric setting. This quantity is usually estimated
with the plug-in estimator, defined via a discrete optimal transport problem
which can be solved to $\epsilon$-accuracy by adding an entropic regularization
of order $\epsilon$ and using for instance Sinkhorn's algorithm. In this work,
we propose instead to estimate it with the Sinkhorn divergence, which is also
built on entropic regularization but includes debiasing terms. We show that,
for smooth densities, this estimator has a comparable sample complexity but
allows higher regularization levels, of order $\epsilon^{1/2}$, which leads to
improved computational complexity bounds and a strong speedup in practice. Our
theoretical analysis covers the case of both randomly sampled densities and
deterministic discretizations on uniform grids. We also propose and analyze an
estimator based on Richardson extrapolation of the Sinkhorn divergence which
enjoys improved statistical and computational efficiency guarantees, under a
condition on the regularity of the approximation error, which is in particular
satisfied for Gaussian densities. We finally demonstrate the efficiency of the
proposed estimators with numerical experiments.
"
"stat.TH","  In this paper, we study the problem of learning the skill distribution of a
population of agents from observations of pairwise games in a tournament. These
games are played among randomly drawn agents from the population. The agents in
our model can be individuals, sports teams, or Wall Street fund managers.
Formally, we postulate that the likelihoods of game outcomes are governed by
the Bradley-Terry-Luce (or multinomial logit) model, where the probability of
an agent beating another is the ratio between its skill level and the pairwise
sum of skill levels, and the skill parameters are drawn from an unknown skill
density of interest. The problem is, in essence, to learn a distribution from
noisy, quantized observations. We propose a simple and tractable algorithm that
learns the skill density with near-optimal minimax mean squared error scaling
as $n^{-1+\varepsilon}$, for any $\varepsilon>0$, when the density is smooth.
Our approach brings together prior work on learning skill parameters from
pairwise comparisons with kernel density estimation from non-parametric
statistics. Furthermore, we prove minimax lower bounds which establish minimax
optimality of the skill parameter estimation technique used in our algorithm.
These bounds utilize a continuum version of Fano's method along with a covering
argument. We apply our algorithm to various soccer leagues and world cups,
cricket world cups, and mutual funds. We find that the entropy of a learnt
distribution provides a quantitative measure of skill, which provides rigorous
explanations for popular beliefs about perceived qualities of sporting events,
e.g., soccer league rankings. Finally, we apply our method to assess the skill
distributions of mutual funds. Our results shed light on the abundance of low
quality funds prior to the Great Recession of 2008, and the domination of the
industry by more skilled funds after the financial crisis.
"
"stat.TH","  It is known that the current graph neural networks (GNNs) are difficult to
make themselves deep due to the problem known as over-smoothing. Multi-scale
GNNs are a promising approach for mitigating the over-smoothing problem.
However, there is little explanation of why it works empirically from the
viewpoint of learning theory. In this study, we derive the optimization and
generalization guarantees of transductive learning algorithms that include
multi-scale GNNs. Using the boosting theory, we prove the convergence of the
training error under weak learning-type conditions. By combining it with
generalization gap bounds in terms of transductive Rademacher complexity, we
show that a test error bound of a specific type of multi-scale GNNs that
decreases corresponding to the number of node aggregations under some
conditions. Our results offer theoretical explanations for the effectiveness of
the multi-scale structure against the over-smoothing problem. We apply boosting
algorithms to the training of multi-scale GNNs for real-world node prediction
tasks. We confirm that its performance is comparable to existing GNNs, and the
practical behaviors are consistent with theoretical observations. Code is
available at https://github.com/delta2323/GB-GNN.
"
"stat.TH","  We study the distribution and uncertainty of nonconvex optimization for noisy
tensor completion -- the problem of estimating a low-rank tensor given
incomplete and corrupted observations of its entries. Focusing on a two-stage
estimation algorithm proposed by Cai et al. (2019), we characterize the
distribution of this nonconvex estimator down to fine scales. This
distributional theory in turn allows one to construct valid and short
confidence intervals for both the unseen tensor entries and the unknown tensor
factors. The proposed inferential procedure enjoys several important features:
(1) it is fully adaptive to noise heteroscedasticity, and (2) it is data-driven
and automatically adapts to unknown noise distributions. Furthermore, our
findings unveil the statistical optimality of nonconvex tensor completion: it
attains un-improvable $\ell_{2}$ accuracy -- including both the rates and the
pre-constants -- when estimating both the unknown tensor and the underlying
tensor factors.
"
"stat.TH","  This article applies the principle of Occam's Razor to non-parametric model
building of statistical data, by finding a model with the minimal number of
bits, leading to an exceptionally effective regularization method for
probability density estimators. The idea comes from the fact that likelihood
maximization also minimizes the number of bits required to encode a dataset.
However, traditional methods overlook that the optimization of model parameters
may also inadvertently play the part in encoding data points. The article shows
how to extend the bit counting to the model parameters as well, providing the
first true measure of complexity for parametric models. Minimizing the total
bit requirement of a model of a dataset favors smaller derivatives, smoother
probability density function estimates and most importantly, a phase space with
fewer relevant parameters. In fact, it is able prune parameters and detect
features with small probability at the same time. It is also shown, how it can
be applied to any smooth, non-parametric probability density estimator.
"
"stat.TH","  The purpose of this review is to present a comprehensive overview of the
theory of ensemble Kalman-Bucy filtering for linear-Gaussian signal models. We
present a system of equations that describe the flow of individual particles
and the flow of the sample covariance and the sample mean in continuous-time
ensemble filtering. We consider these equations and their characteristics in a
number of popular ensemble Kalman filtering variants. Given these equations, we
study their asymptotic convergence to the optimal Bayesian filter. We also
study in detail some non-asymptotic time-uniform fluctuation, stability, and
contraction results on the sample covariance and sample mean (or sample error
track). We focus on testable signal/observation model conditions, and we
accommodate fully unstable (latent) signal models. We discuss the relevance and
importance of these results in characterising the filter's behaviour, e.g. it's
signal tracking performance, and we contrast these results with those in
classical studies of stability in Kalman-Bucy filtering. We provide intuition
for how these results extend to nonlinear signal models and comment on their
consequence on some typical filter behaviours seen in practice, e.g.
catastrophic divergence.
"
"stat.TH","  We propose a flexible ensemble classification framework, Random Subspace
Ensemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate
many weak learners, where each weak learner is a base classifier trained in a
subspace optimally selected from a collection of random subspaces. To conduct
subspace selection, we propose a new criterion, ratio information criterion
(RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis
includes the risk and Monte-Carlo variance of RaSE classifier, establishing the
screening consistency and weak consistency of RIC, and providing an upper bound
for the misclassification rate of RaSE classifier. In addition, we show that in
a high-dimensional framework, the number of random subspaces needs to be very
large to guarantee that a subspace covering signals is selected. Therefore, we
propose an iterative version of RaSE algorithm and prove that under some
specific conditions, a smaller number of generated random subspaces are needed
to find a desirable subspace through iteration. An array of simulations under
various models and real-data applications demonstrate the effectiveness and
robustness of the RaSE classifier and its iterative version in terms of low
misclassification rate and accurate feature ranking. The RaSE algorithm is
implemented in the R package RaSEn on CRAN.
"
"stat.TH","  As the twin movements of open science and open source bring an ever greater
share of the scientific process into the digital realm, new opportunities arise
for the meta-scientific study of science itself, including of data science and
statistics. Future science will likely see machines play an active role in
processing, organizing, and perhaps even creating scientific knowledge. To make
this possible, large engineering efforts must be undertaken to transform
scientific artifacts into useful computational resources, and conceptual
advances must be made in the organization of scientific theories, models,
experiments, and data.
  This dissertation takes steps toward digitizing and systematizing two major
artifacts of data science, statistical models and data analyses. Using tools
from algebra, particularly categorical logic, a precise analogy is drawn
between models in statistics and logic, enabling statistical models to be seen
as models of theories, in the logical sense. Statistical theories, being
algebraic structures, are amenable to machine representation and are equipped
with morphisms that formalize the relations between different statistical
methods. Turning from mathematics to engineering, a software system for
creating machine representations of data analyses, in the form of Python or R
programs, is designed and implemented. The representations aim to capture the
semantics of data analyses, independent of the programming language and
libraries in which they are implemented.
"
"stat.TH","  This paper is concerned with functional learning by utilizing two-stage
sampled distribution regression. We study a multi-penalty regularization
algorithm for distribution regression under the framework of learning theory.
The algorithm aims at regressing to real valued outputs from probability
measures. The theoretical analysis on distribution regression is far from
maturity and quite challenging, since only second stage samples are observable
in practical setting. In the algorithm, to transform information from samples,
we embed the distributions to a reproducing kernel Hilbert space
$\mathcal{H}_K$ associated with Mercer kernel $K$ via mean embedding technique.
The main contribution of the paper is to present a novel multi-penalty
regularization algorithm to capture more features of distribution regression
and derive optimal learning rates for the algorithm. The work also derives
learning rates for distribution regression in the nonstandard setting
$f_{\rho}\notin\mathcal{H}_K$, which is not explored in existing literature.
Moreover, we propose a distribution regression-based distributed learning
algorithm to face large-scale data or information challenge. The optimal
learning rates are derived for the distributed learning algorithm. By providing
new algorithms and showing their learning rates, we improve the existing work
in different aspects in the literature.
"
"stat.TH","  Consider the problem of learning a large number of response functions
simultaneously based on the same input variables. The training data consist of
a single independent random sample of the input variables drawn from a common
distribution together with the associated responses. The input variables are
mapped into a high-dimensional linear space, called the feature space, and the
response functions are modelled as linear functionals of the mapped features,
with coefficients calibrated via ordinary least squares. We provide convergence
guarantees on the worst-case excess prediction risk by controlling the
convergence rate of the excess risk uniformly in the response function. The
dimension of the feature map is allowed to tend to infinity with the sample
size. The collection of response functions, although potentiallyinfinite, is
supposed to have a finite Vapnik-Chervonenkis dimension. The bound derived can
be applied when building multiple surrogate models in a reasonable computing
time.
"
"stat.TH","  Theorem 12 of Simon-Gabriel & Sch\""olkopf (JMLR, 2018) seemed to close a
40-year-old quest to characterize maximum mean discrepancies (MMD) that metrize
the weak convergence of probability measures. We prove, however, that the
theorem is incorrect and provide a correction. We show that, on a locally
compact, non-compact, Hausdorff space, the MMD of a bounded continuous Borel
measurable kernel k, whose RKHS-functions vanish at infinity, metrizes the weak
convergence of probability measures if and only if k is continuous and
integrally strictly positive definite (ISPD) over all signed, finite, regular
Borel measures. We also show that, contrary to the claim of the aforementioned
Theorem 12, there exist both bounded continuous ISPD kernels that do not
metrize weak convergence and bounded continuous non-ISPD kernels that do
metrize it.
"
"stat.TH","  Gaussian process regression is a popular Bayesian framework for surrogate
modeling of expensive data sources. As part of a broader effort in scientific
machine learning, many recent works have incorporated physical constraints or
other a priori information within Gaussian process regression to supplement
limited data and regularize the behavior of the model. We provide an overview
and survey of several classes of Gaussian process constraints, including
positivity or bound constraints, monotonicity and convexity constraints,
differential equation constraints provided by linear PDEs, and boundary
condition constraints. We compare the strategies behind each approach as well
as the differences in implementation, concluding with a discussion of the
computational challenges introduced by constraints.
"
"stat.TH","  In a recent work, we introduced a rigorous framework to describe the mean
field limit of the gradient-based learning dynamics of multilayer neural
networks, based on the idea of a neuronal embedding. There we also proved a
global convergence guarantee for three-layer (as well as two-layer) networks
using this framework.
  In this companion note, we point out that the insights in our previous work
can be readily extended to prove a global convergence guarantee for multilayer
networks of any depths. Unlike our previous three-layer global convergence
guarantee that assumes i.i.d. initializations, our present result applies to a
type of correlated initialization. This initialization allows to, at any finite
training time, propagate a certain universal approximation property through the
depth of the neural network. To achieve this effect, we introduce a
bidirectional diversity condition.
"
"stat.TH","  The present paper implements a complex analytic method to recover the
spectrum of a matrix perturbed by either the addition or the multiplication of
a random matrix noise, under the assumption that the distribution of the noise
is unitarily invariant. This method, introduced by Arizmendi, Tarrago and
Vargas in arXiv:1711.08871, is done in two steps : the first step consists in a
fixed point method to compute the Stieltjes transform of the desired
distribution in a certain domain, and the second step is a classical
deconvolution by a Cauchy distribution, whose parameter depends on the
intensity of the noise. We also provide explicit bounds for the mean squared
error of the first step.
"
"stat.TH","  We study Voronoi cells in the statistical setting by considering preimages of
the maximum likelihood estimator that tessellate an open probability simplex.
In general, logarithmic Voronoi cells are convex sets. However, for certain
algebraic models, namely finite models, models with ML degree 1, linear models,
and log-linear (or toric) models, we show that logarithmic Voronoi cells are
polytopes. As a corollary, the algebraic moment map has polytopes for both its
fibres and its image, when restricted to the simplex. We also compute
non-polytopal logarithmic Voronoi cells using numerical algebraic geometry.
Finally, we determine logarithmic Voronoi polytopes for the finite model
consisting of all empirical distributions of a fixed sample size. These
polytopes are dual to the logarithmic root polytopes of Lie type A, and we
characterize their faces.
"
"stat.TH","  We develop a goodness-of-fit (GOF) test for generative models of
self-exciting processes by making a new connection to this problem with the
classical statistical theory of Quasi-maximum-likelihood estimator (QMLE). We
present a non-parametric self-normalizing statistic for the GOF test: the
Generalized Score (GS) statistics, and explicitly capture the model
misspecification when establishing the asymptotic distribution of the GS
statistic. Numerical experiments based on simulation and real-data validate our
theory and demonstrate the proposed GS test's good performance.
"
"stat.TH","  It is common in the internet industry to use offline-developed algorithms to
power online products that contribute to the success of a business.
Offline-developed algorithms are guided by offline evaluation metrics, which
are often different from online business key performance indicators (KPIs). To
maximize business KPIs, it is important to pick a north star among all
available offline evaluation metrics. By noting that online products can be
measured by online evaluation metrics, the online counterparts of offline
evaluation metrics, we decompose the problem into two parts. As the offline A/B
test literature works out the first part: counterfactual estimators of offline
evaluation metrics that move the same way as their online counterparts, we
focus on the second part: causal effects of online evaluation metrics on
business KPIs. The north star of offline evaluation metrics should be the one
whose online counterpart causes the most significant lift in the business KPI.
We model the online evaluation metric as a mediator and formalize its causality
with the business KPI as dose-response function (DRF). Our novel approach,
causal meta-mediation analysis, leverages summary statistics of many existing
randomized experiments to identify, estimate, and test the mediator DRF. It is
easy to implement and to scale up, and has many advantages over the literature
of mediation analysis and meta-analysis. We demonstrate its effectiveness by
simulation and implementation on real data.
"
"stat.TH","  In this paper, we study high-dimensional estimation from truncated samples.
We focus on two fundamental and classical problems: (i) inference of sparse
Gaussian graphical models and (ii) support recovery of sparse linear models.
  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\bf x}$
are generated from a Gaussian $N(\mu,\Sigma)$ and observed only if they belong
to a subset $S \subseteq \mathbb{R}^d$. We show that ${\mu}$ and ${\Sigma}$ can
be estimated with error $\epsilon$ in the Frobenius norm, using
$\tilde{O}\left(\frac{\textrm{nz}({\Sigma}^{-1})}{\epsilon^2}\right)$ samples
from a truncated $\mathcal{N}({\mu},{\Sigma})$ and having access to a
membership oracle for $S$. The set $S$ is assumed to have non-trivial measure
under the unknown distribution but is otherwise arbitrary.
  (ii) For sparse linear regression, suppose samples $({\bf x},y)$ are
generated where $y = {\bf x}^\top{{\Omega}^*} + \mathcal{N}(0,1)$ and $({\bf
x}, y)$ is seen only if $y$ belongs to a truncation set $S \subseteq
\mathbb{R}$. We consider the case that ${\Omega}^*$ is sparse with a support
set of size $k$. Our main result is to establish precise conditions on the
problem dimension $d$, the support size $k$, the number of observations $n$,
and properties of the samples and the truncation that are sufficient to recover
the support of ${\Omega}^*$. Specifically, we show that under some mild
assumptions, only $O(k^2 \log d)$ samples are needed to estimate ${\Omega}^*$
in the $\ell_\infty$-norm up to a bounded error.
  For both problems, our estimator minimizes the sum of the finite population
negative log-likelihood function and an $\ell_1$-regularization term.
"
"stat.TH","  In this paper, the uniformly asymptotic normality for sample quantiles of
associated random variables is investigated under some conditions on the decay
of the covariances. We obtain the rate of normal approximation of order
$O(n^{-1/2}\log^2 n)$ if the covariances decrease exponentially to $0$. The
best rate is shown as $O(n^{-1/3})$ under a polynomial decay of the
covariances.
"
"stat.TH","  Persistent homology has become an important tool for extracting geometric and
topological features from data, whose multi-scale features are summarized in a
persistence diagram. From a statistical perspective, however, persistence
diagrams are very sensitive to perturbations in the input space. In this work,
we develop a framework for constructing robust persistence diagrams from
superlevel filtrations of robust density estimators constructed using
reproducing kernels. Using an analogue of the influence function on the space
of persistence diagrams, we establish the proposed framework to be less
sensitive to outliers. The robust persistence diagrams are shown to be
consistent estimators in bottleneck distance, with the convergence rate
controlled by the smoothness of the kernel. This, in turn, allows us to
construct uniform confidence bands in the space of persistence diagrams.
Finally, we demonstrate the superiority of the proposed approach on benchmark
datasets.
"
"stat.TH","  In this paper, a new mixture family of multivariate normal distributions,
formed by mixing multivariate normal distribution and skewed distribution, is
constructed. Some properties of this family, such as characteristic function,
moment generating function, and the first four moments are derived. The
distributions of affine transformations and canonical forms of the model are
also derived. An EM type algorithm is developed for the maximum likelihood
estimation of model parameters. We have considered in detail, some special
cases of the family, using standard gamma and standard exponential mixture
distributions, denoted by MMNG and MMNE, respectively. For the proposed family
of distributions, different multivariate measures of skewness are computed. In
order to examine the performance of the developed estimation method, some
simulation studies are carried out to show that the maximum likelihood
estimates based on the EM type algorithm do provide good performance. For
different choices of parameters of MMNE distribution, several multivariate
measures of skewness are computed and compared. Because some measures of
skewness are scalar and some are vectors, in order to evaluate them properly,
we have carried out a simulation study to determine the power of tests, based
on sample versions of skewness measures as test statistics to test the fit of
the MMNE distribution. Finally, two real data sets are used to illustrate the
usefulness of the proposed family of distributions and the associated
inferential method.
"
"stat.TH","  This paper studies the variation diminishing property of $k$-positive
systems, which map inputs with $k-1$ sign changes to outputs with at most the
same variation. We characterize this property for the Toeplitz and Hankel
operators of finite-dimensional linear time invariant systems. Our main result
is that these operators have a dominant approximation in the form of series or
parallel interconnections of $k$ first order positive systems. This is shown by
expressing the $k$-positivity of a LTI system as the external positivity (that
is, $1$-positivity) of $k$ compound LTI systems. Our characterization
generalizes well known properties of externally positive systems ($k=1$) and
totally positive systems ($k=\infty$).
"
"stat.TH","  The copulas of random vectors with standard uniform univariate margins
truncated from the right are considered and a general formula for such
right-truncated conditional copulas is derived. This formula is analytical for
copulas that can be inverted analytically as functions of each single argument.
This is the case, for example, for Archimedean and related copulas. The
resulting right-truncated Archimedean copulas are not only analytically
tractable but can also be characterized as tilted Archimedean copulas. This
finding allows one, for example, to more easily derive analytical properties
such as the coefficients of tail dependence or sampling procedures of
right-truncated Archimedean copulas. As another result, one can easily obtain a
limiting Clayton copula for a general vector of truncation points converging to
zero; this is an important property for (re)insurance and a fact already known
in the special case of equal truncation points, but harder to prove without
aforementioned characterization. Furthermore, right-truncated Archimax copulas
with logistic stable tail dependence functions are characterized as tilted
outer power Archimedean copulas and an analytical form of right-truncated
nested Archimedean copulas is also derived.
"
"stat.TH","  Fisher's method prescribes a way to combine p-values from multiple
experiments into a single p-value. However, the original method can only
determine a combined p-value analytically if all constituent p-values are
weighted equally. Here we present, with proof, a method to combine p-values
with arbitrary weights.
"
"stat.TH","  This article is motivated by studying multisensory effects on brain
activities in intracranial electroencephalography (iEEG) experiments.
Differential brain activities to multisensory stimulus presentations are zero
in most regions and non-zero in some local regions, yielding locally sparse
functions. Such studies are essentially a function-on-scalar regression
problem, with interest being focused not only on estimating nonparametric
functions but also on recovering the function supports. We propose a weighted
group bridge approach for simultaneous function estimation and support recovery
in function-on-scalar mixed effect models, while accounting for heterogeneity
present in functional data. We use B-splines to transform sparsity of functions
to its sparse vector counterpart of increasing dimension, and propose a fast
non-convex optimization algorithm using nested alternative direction method of
multipliers (ADMM) for estimation. Large sample properties are established. In
particular, we show that the estimated coefficient functions are rate optimal
in the minimax sense under the $L_2$ norm and resemble a phase transition
phenomenon. For support estimation, we derive a convergence rate under the
$L_{\infty}$ norm that leads to a sparsistency property under
$\delta$-sparsity, and provide a simple sufficient regularity condition under
which a strict sparsistency property is established. An adjusted extended
Bayesian information criterion is proposed for parameter tuning. The developed
method is illustrated through simulation and an application to a novel iEEG
dataset to study multisensory integration. We integrate the proposed method
into RAVE, an R package that gains increasing popularity in the iEEG community.
"
"stat.TH","  The recent success of high-dimensional models, such as deep neural networks
(DNNs), has led many to question the validity of the bias-variance tradeoff
principle in high dimensions. We reexamine it with respect to two key choices:
the model class and the complexity measure. We argue that failing to suitably
specify either one can falsely suggest that the tradeoff does not hold. This
observation motivates us to seek a valid complexity measure, defined with
respect to a reasonably good class of models. Building on Rissanen's principle
of minimum description length (MDL), we propose a novel MDL-based complexity
(MDL-COMP). We focus on the context of linear models, which have been recently
used as a stylized tractable approximation to DNNs in high-dimensions. MDL-COMP
is defined via an optimality criterion over the encodings induced by a good
Ridge estimator class. We derive closed-form expressions for MDL-COMP and show
that for a dataset with $n$ observations and $d$ parameters it is \emph{not
always} equal to $d/n$, and is a function of the singular values of the design
matrix and the signal-to-noise ratio. For random Gaussian design, we find that
while MDL-COMP scales linearly with $d$ in low-dimensions ($d<n$), for
high-dimensions ($d>n$) the scaling is exponentially smaller, scaling as $\log
d$. We hope that such a slow growth of complexity in high-dimensions can help
shed light on the good generalization performance of several well-tuned
high-dimensional models. Moreover, via an array of simulations and real-data
experiments, we show that a data-driven Prac-MDL-COMP can inform
hyper-parameter tuning for ridge regression in limited data settings, sometimes
improving upon cross-validation.
"
"stat.TH","  We consider the problem of inference for local parameters of a convex
regression function $f_0: [0,1] \to \mathbb{R}$ based on observations from a
standard nonparametric regression model, using the convex least squares
estimator (LSE) $\widehat{f}_n$. For $x_0 \in (0,1)$, the local parameters
include the pointwise function value $f_0(x_0)$, the pointwise derivative
$f_0'(x_0)$, and the anti-mode (i.e., the smallest minimizer) of $f_0$. The
existing limiting distribution of the estimation error $(\widehat{f}_n(x_0) -
f_0(x_0), \widehat{f}_n'(x_0) - f_0'(x_0) )$ depends on the unknown second
derivative $f_0''(x_0)$, and is therefore not directly applicable for
inference. To circumvent this impasse, we show that the following locally
normalized errors (LNEs) enjoy pivotal limiting behavior: Let
$[\widehat{u}(x_0), \widehat{v}(x_0)]$ be the maximal interval containing $x_0$
where $\widehat{f}_n$ is linear. Then, under standard conditions, $$\binom{
\sqrt{n(\widehat{v}(x_0)-\widehat{u}(x_0))}(\widehat{f}_n(x_0)-f_0(x_0)) }{
\sqrt{n(\widehat{v}(x_0)-\widehat{u}(x_0))^3}(\widehat{f}_n'(x_0)-f_0'(x_0))}
\rightsquigarrow \sigma \cdot \binom{\mathbb{L}^{(0)}_2}{\mathbb{L}^{(1)}_2},$$
where $n$ is the sample size, $\sigma$ is the standard deviation of the errors,
and $\mathbb{L}^{(0)}_2, \mathbb{L}^{(1)}_2$ are universal random variables.
This asymptotically pivotal LNE theory instantly yields a simple tuning-free
procedure for constructing CIs with asymptotically exact coverage and optimal
length for $f_0(x_0)$ and $f_0'(x_0)$. We also construct an asymptotically
pivotal LNE for the anti-mode of $f_0$, and its limiting distribution does not
even depend on $\sigma$. These asymptotically pivotal LNE theories are further
extended to other convexity/concavity constrained models (e.g., log-concave
density estimation) for which a limit distribution theory is available for
problem-specific estimators.
"
"stat.TH","  We study a variant of the sparse PCA (principal component analysis) problem
in the ""hard"" regime, where the inference task is possible yet no
polynomial-time algorithm is known to exist. Prior work, based on the
low-degree likelihood ratio, has conjectured a precise expression for the best
possible (sub-exponential) runtime throughout the hard regime. Following
instead a statistical physics inspired point of view, we show bounds on the
depth of free energy wells for various Gibbs measures naturally associated to
the problem. These free energy wells imply hitting time lower bounds that
corroborate the low-degree conjecture: we show that a class of natural MCMC
(Markov chain Monte Carlo) methods (with worst-case initialization) cannot
solve sparse PCA with less than the conjectured runtime. These lower bounds
apply to a wide range of values for two tuning parameters: temperature and
sparsity misparametrization. Finally, we prove that the Overlap Gap Property
(OGP), a structural property that implies failure of certain local search
algorithms, holds in a significant part of the hard regime.
"
"stat.TH","  We study the problem of {\em list-decodable mean estimation} for bounded
covariance distributions. Specifically, we are given a set $T$ of points in
$\mathbb{R}^d$ with the promise that an unknown $\alpha$-fraction of points in
$T$, where $0< \alpha < 1/2$, are drawn from an unknown mean and bounded
covariance distribution $D$, and no assumptions are made on the remaining
points. The goal is to output a small list of hypothesis vectors such that at
least one of them is close to the mean of $D$. We give the first practically
viable estimator for this problem. In more detail, our algorithm is sample and
computationally efficient, and achieves information-theoretically near-optimal
error. While the only prior algorithm for this setting inherently relied on the
ellipsoid method, our algorithm is iterative and only uses spectral techniques.
Our main technical innovation is the design of a soft outlier removal procedure
for high-dimensional heavy-tailed datasets with a majority of outliers.
"
"stat.TH","  Stochastic Gradient Descent (SGD) has become the method of choice for solving
a broad range of machine learning problems. However, some of its learning
properties are still not fully understood. We consider least squares learning
in reproducing kernel Hilbert spaces (RKHSs) and extend the classical SGD
analysis to a learning setting in Hilbert scales, including Sobolev spaces and
Diffusion spaces on compact Riemannian manifolds. We show that even for
well-specified models, violation of a traditional benchmark smoothness
assumption has a tremendous effect on the learning rate. In addition, we show
that for miss-specified models, preconditioning in an appropriate Hilbert scale
helps to reduce the number of iterations, i.e. allowing for ""earlier stopping"".
"
"stat.TH","  Pure-jump L\'evy processes are popular classes of stochastic processes which
have found many applications in finance, statistics or machine learning. In
this paper, we propose a novel family of self-decomposable L\'evy processes
where one can control separately the tail behavior and the jump activity of the
process, via two different parameters. Crucially, we show that one can sample
exactly increments of this process, at any time scale; this allows the
implementation of likelihood-free Markov chain Monte Carlo algorithms for
(asymptotically) exact posterior inference. We use this novel process in
L\'evy-based stochastic volatility models to predict the returns of stock
market data, and show that the proposed class of models leads to superior
predictive performances compared to classical alternatives.
"
"stat.TH","  This paper considers endogenous selection models, in particular nonparametric
ones. Estimating the unconditional law of the outcomes is possible when one
uses instrumental variables. Using a selection equation which is additively
separable in a one dimensional unobservable has the sometimes undesirable
property of instrument monotonicity. We present models which allow for
nonmonotonicity and are based on nonparametric random coefficients indices. We
discuss their nonparametric identification and apply these results to inference
on nonlinear statistics such as the Gini index in surveys when the nonresponse
is not missing at random.
"
"stat.TH","  Mixture autoregressive (MAR) models provide a flexible way to model time
series with predictive distributions which depend on the recent history of the
process and are able to accommodate asymmetry and multimodality. Bayesian
inference for such models offers the additional advantage of incorporating the
uncertainty in the estimated models into the predictions. We introduce a new
way of sampling from the posterior distribution of the parameters of MAR models
which allows for covering the complete parameter space of the models, unlike
previous approaches. We also propose a relabelling algorithm to deal a
posteriori with label switching. We apply our new method to simulated and real
datasets, discuss the accuracy and performance of our new method, as well as
its advantages over previous studies. The idea of density forecasting using
MCMC output is also introduced.
"
"stat.TH","  Conditional correlation networks, within Gaussian Graphical Models (GGM), are
widely used to describe the direct interactions between the components of a
random vector. In the case of an unlabelled Heterogeneous population,
Expectation Maximisation (EM) algorithms for Mixtures of GGM have been proposed
to estimate both each sub-population's graph and the class labels. However, we
argue that, with most real data, class affiliation cannot be described with a
Mixture of Gaussian, which mostly groups data points according to their
geometrical proximity. In particular, there often exists external co-features
whose values affect the features' average value, scattering across the feature
space data points belonging to the same sub-population. Additionally, if the
co-features' effect on the features is Heterogeneous, then the estimation of
this effect cannot be separated from the sub-population identification. In this
article, we propose a Mixture of Conditional GGM (CGGM) that subtracts the
heterogeneous effects of the co-features to regroup the data points into
sub-population corresponding clusters. We develop a penalised EM algorithm to
estimate graph-sparse model parameters. We demonstrate on synthetic and real
data how this method fulfils its goal and succeeds in identifying the
sub-populations where the Mixtures of GGM are disrupted by the effect of the
co-features.
"
"stat.TH","  Partial orderings and measures of information for continuous univariate
random variables with special roles of Gaussian and uniform distributions are
discussed. The information measures and measures of non-Gaussianity including
third and fourth cumulants are generally used as projection indices in the
projection pursuit approach for the independent component analysis. The
connections between information, non-Gaussianity and statistical independence
in the context of independent component analysis is discussed in detail.
"
"stat.TH","  We generalize the notion of minimax convergence rate. In contrast to the
standard definition, we do not assume that the sample size is fixed in advance.
Allowing for varying sample size results in time-robust minimax rates and
estimators. These can be either strongly adversarial, based on the worst-case
over all sample sizes, or weakly adversarial, based on the worst-case over all
stopping times. We show that standard and time-robust rates usually differ by
at most a logarithmic factor, and that for some (and we conjecture for all)
exponential families, they differ by exactly an iterated logarithmic factor. In
many situations, time-robust rates are arguably more natural to consider. For
example, they allow us to simultaneously obtain strong model selection
consistency and optimal estimation rates, thus avoiding the ""AIC-BIC dilemma"".
"
"stat.TH","  Recent work, motivated by anonymous messaging platforms, has introduced
adaptive diffusion protocols which can obfuscate the source of a rumor: a
""snapshot adversary"" with access to the subgraph of ""infected"" nodes can do no
better than randomly guessing the entity of the source node. What happens if
the adversary has access to multiple independent snapshots? We study this
question when the underlying graph is the infinite $d$-regular tree. We show
that (1) a weak form of source obfuscation is still possible in the case of two
independent snapshots, but (2) already with three observations there is a
simple algorithm that finds the rumor source with constant probability,
regardless of the adaptive diffusion protocol. We also characterize the
tradeoff between local spreading and source obfuscation for adaptive diffusion
protocols (under a single snapshot). These results raise questions about the
robustness of anonymity guarantees when spreading information in social
networks.
"
"stat.TH","  We address the following question in this paper: ""What are the most robust
statistical methods for social choice?'' By leveraging the theory of uniformly
least favorable distributions in the Neyman-Pearson framework to finite models
and randomized tests, we characterize uniformly most powerful (UMP) tests,
which is a well-accepted statistical optimality w.r.t. robustness, for testing
whether a given alternative is the winner under Mallows' model and under
Condorcet's model, respectively.
"
"stat.TH","  A fundamental problem in high-dimensional testing is that of global null
testing: testing whether the null holds simultaneously in all of $n$
hypotheses. The max test, which uses the smallest of the $n$ marginal p-values
as its test statistic, enjoys widespread popularity for its simplicity and
robustness. However, its theoretical performance relative to other tests has
been called into question. In the Gaussian sequence version of the global
testing problem, Donoho and Jin (2004) discovered a so-called ""weak, sparse""
asymptotic regime in which the higher criticism and Berk-Jones tests achieve a
better detection boundary than the max test when all of the nonzero signal
strengths are identical. We study a more general model in which the non-null
means are drawn from a generic distribution, and show that the detection
boundary for the max test is optimal in the ""weak, sparse"" regime, provided
that the distribution's tail is no lighter than Gaussian. Further, we show
theoretically and in simulation that the modified higher criticism of Donoho
and Jin (2004) can have very low power when the distribution of non-null means
has a polynomial tail.
"
"stat.TH","  In dose-finding trials, due to staggered enrollment, it might be desirable to
make dose assignment decisions in real-time in the presence of pending toxicity
outcomes, for example, when patient accrual is fast or the dose-limiting
toxicity is late-onset. Patients' time-to-event information may be utilized to
facilitate such decisions. We propose a unified statistical framework for
time-to-event modeling in dose-finding trials, which leads to two classes of
time-to-event designs: TITE deigns and POD designs. TITE designs are based on
inference on toxicity probabilities, while POD designs are based on inference
on dose-finding decisions. These two classes of designs contain existing
designs as special cases and also give rise to new designs. We discuss and
study the theoretical properties of these designs, including large-sample
convergence properties, coherence principles, and the underlying decision
rules. To facilitate the use of time-to-event designs in practice, we introduce
efficient computational algorithms and review common practical considerations,
such as safety rules and suspension rules. Finally, the operating
characteristics of several designs are evaluated and compared through computer
simulations.
"
"stat.TH","  The causal effect of an intervention (treatment/exposure) on an outcome can
be estimated by: i) specifying knowledge about the data-generating process; ii)
assessing under what assumptions a target quantity, such as for example a
causal odds ratio, can be identified given the specified knowledge (and given
the measured data); and then, iii) using appropriate statistical estimation
techniques to estimate the desired parameter of interest. As regression is the
cornerstone of statistical analysis, it seems obvious to ask: is it appropriate
to use estimated regression parameters for causal effect estimation? It turns
out that using regression for effect estimation is possible, but typically
requires more assumptions than competing methods. This manuscript provides a
comprehensive summary of the assumptions needed to identify and estimate a
causal parameter using regression and, equally important, discusses the
resulting implications for statistical practice.
"
"stat.TH","  In this paper, we study the asymptotic properties (bias, variance, mean
squared error) of Bernstein estimators for cumulative distribution functions
and density functions near and on the boundary of the $d$-dimensional simplex.
The simplex is an important case as it is the natural domain of compositional
data and has been neglected in the literature. Our results generalize those
found in Leblanc (2012), who treated the case $d=1$, and complement the results
from Ouimet (2020) in the interior of the simplex. Different parts of the
boundary having different dimensions makes the analysis more complex.
"
"stat.TH","  We are interested in reconstructing the initial condition of a non-linear
partial differential equation (PDE), namely the Fokker-Planck equation, from
the observation of a Dyson Brownian motion at a given time $t>0$. The
Fokker-Planck equation describes the evolution of electrostatic repulsive
particle systems, and can be seen as the large particle limit of correctly
renormalized Dyson Brownian motions. The solution of the Fokker-Planck equation
can be written as the free convolution of the initial condition and the
semi-circular distribution. We propose a nonparametric estimator for the
initial condition obtained by performing the free deconvolution via the
subordination functions method. This statistical estimator is original as it
involves the resolution of a fixed point equation, and a classical
deconvolution by a Cauchy distribution. This is due to the fact that, in free
probability, the analogue of the Fourier transform is the R-transform, related
to the Cauchy transform. In past literature, there has been a focus on the
estimation of the initial conditions of linear PDEs such as the heat equation,
but to the best of our knowledge, this is the first time that the problem is
tackled for a non-linear PDE. The convergence of the estimator is proved and
the integrated mean square error is computed, providing rates of convergence
similar to the ones known for non-parametric deconvolution methods. Finally, a
simulation study illustrates the good performances of our estimator.
"
"stat.TH","  In this article, we study a robust estimation method for a general class of
integer-valued time series models.
  The conditional distribution of the process belongs to a broad class of
distribution and unlike classical autoregressive framework, the conditional
mean of the process also depends on some multivariate exogenous covariate.
  We derive a robust inference procedure based on the minimum density power
divergence.
  Under certain regularity conditions, we establish that the proposed estimator
is consistent and asymptotically normal.
  Simulation experiments are conducted to illustrate the empirical performances
of the estimator. An application to the number of transactions per minute for
the stock Ericsson B is also provided.
"
"stat.TH","  We establish finite-sample guarantees for a polynomial-time algorithm for
learning a nonlinear, nonparametric directed acyclic graphical (DAG) model from
data. The analysis is model-free and does not assume linearity, additivity,
independent noise, or faithfulness. Instead, we impose a condition on the
residual variances that is closely related to previous work on linear models
with equal variances. Compared to an optimal algorithm with oracle knowledge of
the variable ordering, the additional cost of the algorithm is linear in the
dimension $d$ and the number of samples $n$. Finally, we compare the proposed
algorithm to existing approaches in a simulation study.
"
"stat.TH","  We consider sensitivity of a generic stochastic optimization problem to model
uncertainty. We take a non-parametric approach and capture model uncertainty
using Wasserstein balls around the postulated model. We provide explicit
formulae for the first order correction to both the value function and the
optimizer and further extend our results to optimization under linear
constraints. We present applications to statistics, machine learning,
mathematical finance and uncertainty quantification. In particular, we provide
explicit first-order approximation for square-root LASSO regression
coefficients and deduce coefficient shrinkage compared to the ordinary least
squares regression. We consider robustness of call option pricing and deduce a
new Black-Scholes sensitivity, a non-parametric version of the so-called Vega.
We also compute sensitivities of optimized certainty equivalents in finance and
propose measures to quantify robustness of neural networks to adversarial
examples.
"
"stat.TH","  In this paper, we aim to provide a statistical theory for object matching
based on the Gromov-Wasserstein distance. To this end, we model general objects
as metric measure spaces. Based on this, we propose a simple and efficiently
computable asymptotic statistical test for pose invariant object
discrimination. This is based on an empirical version of a $\beta$-trimmed
lower bound of the Gromov-Wasserstein distance. We derive for $\beta\in[0,1/2)$
distributional limits of this test statistic. To this end, we introduce a novel
$U$-type process indexed in $\beta$ and show its weak convergence. Finally, the
theory developed is investigated in Monte Carlo simulations and applied to
structural protein comparisons.
"
"stat.TH","  Optimal transport (OT) distances are increasingly used as loss functions for
statistical inference, notably in the learning of generative models or
supervised learning. Yet, the behavior of minimum Wasserstein estimators is
poorly understood, notably in high-dimensional regimes or under model
misspecification. In this work we adopt the viewpoint of projection robust (PR)
OT, which seeks to maximize the OT cost between two measures by choosing a
$k$-dimensional subspace onto which they can be projected. Our first
contribution is to establish several fundamental statistical properties of PR
Wasserstein distances, complementing and improving previous literature that has
been restricted to one-dimensional and well-specified cases. Next, we propose
the integral PR Wasserstein (IPRW) distance as an alternative to the PRW
distance, by averaging rather than optimizing on subspaces. Our complexity
bounds can help explain why both PRW and IPRW distances outperform Wasserstein
distances empirically in high-dimensional inference tasks. Finally, we consider
parametric inference using the PRW distance. We provide an asymptotic guarantee
of two types of minimum PRW estimators and formulate a central limit theorem
for max-sliced Wasserstein estimator under model misspecification. To enable
our analysis on PRW with projection dimension larger than one, we devise a
novel combination of variational analysis and statistical theory.
"
"stat.TH","  Ubiquitous linear Gaussian exploratory tools such as principle component
analysis (PCA) and factor analysis (FA) remain widely used as tools for:
exploratory analysis, pre-processing, data visualization and related tasks.
However, due to their rigid assumptions including crowding of high dimensional
data, they have been replaced in many settings by more flexible and still
interpretable latent feature models. The Feature allocation is usually modelled
using discrete latent variables assumed to follow either parametric
Beta-Bernoulli distribution or Bayesian nonparametric prior. In this work we
propose a simple and tractable parametric feature allocation model which can
address key limitations of current latent feature decomposition techniques. The
new framework allows for explicit control over the number of features used to
express each point and enables a more flexible set of allocation distributions
including feature allocations with different sparsity levels. This approach is
used to derive a novel adaptive Factor analysis (aFA), as well as, an adaptive
probabilistic principle component analysis (aPPCA) capable of flexible
structure discovery and dimensionality reduction in a wide case of scenarios.
We derive both standard Gibbs sampler, as well as, an expectation-maximization
inference algorithms that converge orders of magnitude faster to a reasonable
point estimate solution. The utility of the proposed aPPCA model is
demonstrated for standard PCA tasks such as feature learning, data
visualization and data whitening. We show that aPPCA and aFA can infer
interpretable high level features both when applied on raw MNIST and when
applied for interpreting autoencoder features. We also demonstrate an
application of the aPPCA to more robust blind source separation for functional
magnetic resonance imaging (fMRI).
"
"stat.TH","  We study the problem of PAC learning one-hidden-layer ReLU networks with $k$
hidden units on $\mathbb{R}^d$ under Gaussian marginals in the presence of
additive label noise. For the case of positive coefficients, we give the first
polynomial-time algorithm for this learning problem for $k$ up to
$\tilde{O}(\sqrt{\log d})$. Previously, no polynomial time algorithm was known,
even for $k=3$. This answers an open question posed by~\cite{Kliv17}.
Importantly, our algorithm does not require any assumptions about the rank of
the weight matrix and its complexity is independent of its condition number. On
the negative side, for the more general task of PAC learning one-hidden-layer
ReLU networks with arbitrary real coefficients, we prove a Statistical Query
lower bound of $d^{\Omega(k)}$. Thus, we provide a separation between the two
classes in terms of efficient learnability. Our upper and lower bounds are
general, extending to broader families of activation functions.
"
"stat.TH","  Non linear regression models are a standard tool for modeling real phenomena,
with several applications in machine learning, ecology, econometry...
Estimating the parameters of the model has garnered a lot of attention during
many years. We focus here on a recursive method for estimating parameters of
non linear regressions. Indeed, these kinds of methods, whose most famous are
probably the stochastic gradient algorithm and its averaged version, enable to
deal efficiently with massive data arriving sequentially. Nevertheless, they
can be, in practice, very sensitive to the case where the eigen-values of the
Hessian of the functional we would like to minimize are at different scales. To
avoid this problem, we first introduce an online Stochastic Gauss-Newton
algorithm. In order to improve the estimates behavior in case of bad
initialization, we also introduce a new Averaged Stochastic Gauss-Newton
algorithm and prove its asymptotic efficiency.
"
"stat.TH","  The class of inhomogeneous phase-type distributions (IPH) was recently
introduced in Albrecher and Bladt (2019) as an extension of the classical
phase-type (PH) distributions. Like PH distributions, the class of IPH is dense
in the class of distributions on the positive halfline, but leads to more
parsimonious models in the presence of heavy tails. In this paper we propose a
fitting procedure for this class to given data. We furthermore consider an
analogous extension of Kulkarni's multivariate phase-type class (Kulkarni,
1989) to the inhomogeneous framework and study parameter estimation for the
resulting new and flexible class of multivariate distributions. As a
by-product, we amend a previously suggested fitting procedure for the
homogeneous multivariate phase-type case and provide appropriate adaptations
for censored data. The performance of the algorithms is illustrated in several
numerical examples, both for simulated and real-life insurance data.
"
"stat.TH","  This paper considers a new bootstrap procedure to estimate the distribution
of high-dimensional $\ell_p$-statistics, i.e. the $\ell_p$-norms of the sum of
$n$ independent $d$-dimensional random vectors with $d \gg n$ and $p \in [1,
\infty]$. We provide a non-asymptotic characterization of the sampling
distribution of $\ell_p$-statistics based on Gaussian approximation and show
that the bootstrap procedure is consistent in the Kolmogorov-Smirnov distance
under mild conditions on the covariance structure of the data. As an
application of the general theory we propose a bootstrap hypothesis test for
simultaneous inference on high-dimensional mean vectors. We establish its
asymptotic correctness and consistency under high-dimensional alternatives, and
discuss the power of the test as well as the size of associated confidence
sets. We illustrate the bootstrap and testing procedure numerically on
simulated data.
"
"stat.TH","  Robust covariance estimation is the following, well-studied problem in high
dimensional statistics: given $N$ samples from a $d$-dimensional Gaussian
$\mathcal{N}(\boldsymbol{0}, \Sigma)$, but where an $\varepsilon$-fraction of
the samples have been arbitrarily corrupted, output $\widehat{\Sigma}$
minimizing the total variation distance between $\mathcal{N}(\boldsymbol{0},
\Sigma)$ and $\mathcal{N}(\boldsymbol{0}, \widehat{\Sigma})$. This corresponds
to learning $\Sigma$ in a natural affine-invariant variant of the Frobenius
norm known as the \emph{Mahalanobis norm}. Previous work of Cheng et al
demonstrated an algorithm that, given $N = \Omega (d^2 / \varepsilon^2)$
samples, achieved a near-optimal error of $O(\varepsilon \log 1 /
\varepsilon)$, and moreover, their algorithm ran in time $\widetilde{O}(T(N, d)
\log \kappa / \mathrm{poly} (\varepsilon))$, where $T(N, d)$ is the time it
takes to multiply a $d \times N$ matrix by its transpose, and $\kappa$ is the
condition number of $\Sigma$. When $\varepsilon$ is relatively small, their
polynomial dependence on $1/\varepsilon$ in the runtime is prohibitively large.
In this paper, we demonstrate a novel algorithm that achieves the same
statistical guarantees, but which runs in time $\widetilde{O} (T(N, d) \log
\kappa)$. In particular, our runtime has no dependence on $\varepsilon$. When
$\Sigma$ is reasonably conditioned, our runtime matches that of the fastest
algorithm for covariance estimation without outliers, up to poly-logarithmic
factors, showing that we can get robustness essentially ""for free.""
"
"stat.TH","  We develop and analyze a projected particle Langevin optimization method to
learn the distribution in the Sch\""{o}nberg integral representation of the
radial basis functions from training samples. More specifically, we
characterize a distributionally robust optimization method with respect to the
Wasserstein distance to optimize the distribution in the Sch\""{o}nberg integral
representation. To provide theoretical performance guarantees, we analyze the
scaling limits of a projected particle online (stochastic) optimization method
in the mean-field regime. In particular, we prove that in the scaling limits,
the empirical measure of the Langevin particles converges to the law of a
reflected It\^{o} diffusion-drift process. Moreover, the drift is also a
function of the law of the underlying process. Using It\^{o} lemma for
semi-martingales and Grisanov's change of measure for the Wiener processes, we
then derive a Mckean-Vlasov type partial differential equation (PDE) with Robin
boundary conditions that describes the evolution of the empirical measure of
the projected Langevin particles in the mean-field regime. In addition, we
establish the existence and uniqueness of the steady-state solutions of the
derived PDE in the weak sense. We apply our learning approach to train radial
kernels in the kernel locally sensitive hash (LSH) functions, where the
training data-set is generated via a $k$-mean clustering method on a small
subset of data-base. We subsequently apply our kernel LSH with a trained kernel
for image retrieval task on MNIST data-set, and demonstrate the efficacy of our
kernel learning approach. We also apply our kernel learning approach in
conjunction with the kernel support vector machines (SVMs) for classification
of benchmark data-sets.
"
"stat.TH","  For a certain scaling of the initialization of stochastic gradient descent
(SGD), wide neural networks (NN) have been shown to be well approximated by
reproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed
that, for some classification tasks, RKHS methods can replace NNs without a
large loss in performance. On the other hand, two-layers NNs are known to
encode richer smoothness classes than RKHS and we know of special examples for
which SGD-trained NN provably outperform RKHS. This is true even in the wide
network limit, for a different scaling of the initialization.
  How can we reconcile the above claims? For which tasks do NNs outperform
RKHS? If feature vectors are nearly isotropic, RKHS methods suffer from the
curse of dimensionality, while NNs can overcome it by learning the best
low-dimensional representation. Here we show that this curse of dimensionality
becomes milder if the feature vectors display the same low-dimensional
structure as the target function, and we precisely characterize this tradeoff.
Building on these results, we present a model that can capture in a unified
framework both behaviors observed in earlier work.
  We hypothesize that such a latent low-dimensional structure is present in
image classification. We test numerically this hypothesis by showing that
specific perturbations of the training distribution degrade the performances of
RKHS methods much more significantly than NNs.
"
"stat.TH","  We consider machine learning, particularly regression, using
locally-differentially private datasets. The Wasserstein distance is used to
define an ambiguity set centered at the empirical distribution of the dataset
corrupted by local differential privacy noise. The ambiguity set is shown to
contain the probability distribution of unperturbed, clean data. The radius of
the ambiguity set is a function of the privacy budget, spread of the data, and
the size of the problem. Hence, machine learning with locally-differentially
private datasets can be rewritten as a distributionally-robust optimization.
For general distributions, the distributionally-robust optimization problem can
relaxed as a regularized machine learning problem with the Lipschitz constant
of the machine learning model as a regularizer. For linear and logistic
regression, this regularizer is the dual norm of the model parameters. For
Gaussian data, the distributionally-robust optimization problem can be solved
exactly to find an optimal regularizer. This approach results in an entirely
new regularizer for training linear regression models. Training with this novel
regularizer can be posed as a semi-definite program. Finally, the performance
of the proposed distributionally-robust machine learning training is
demonstrated on practical datasets.
"
"stat.TH","  We consider spatially dependent functional data collected under a
geostatistics setting, where spatial locations are irregular and random. The
functional response is the sum of a spatially dependent functional effect and a
spatially independent functional nugget effect. Observations on each function
are made on discrete time points and contaminated with measurement errors.
Under the assumption of spatial stationarity and isotropy, we propose a tensor
product spline estimator for the spatio-temporal covariance function. When a
coregionalization covariance structure is further assumed, we propose a new
functional principal component analysis method that borrows information from
neighboring functions. The proposed method also generates nonparametric
estimators for the spatial covariance functions, which can be used for
functional kriging. Under a unified framework for sparse and dense functional
data, infill and increasing domain asymptotic paradigms, we develop the
asymptotic convergence rates for the proposed estimators. Advantages of the
proposed approach are demonstrated through simulation studies and two real data
applications representing sparse and dense functional data, respectively.
"
"stat.TH","  We consider the problem of the estimation of the mean function of an
inhomogeneous Poisson process when its intensity function is periodic. For the
mean integrated squared error (MISE) there is a classical lower bound for all
estimators and the empirical mean function attains that lower bound, thus it is
asymptotically efficient. Following the ideas of the work by Golubev and Levit,
we compare asymptotically efficient estimators and propose an estimator which
is second order asymptotically efficient. Second order efficiency is done over
Sobolev ellipsoids, following the idea of Pinsker.
"
"stat.TH","  Stochastic volatility processes are used in multivariate time-series analysis
to track time-varying patterns in covariance structures. Uhlig extended and
beta-Bartlett processes are especially useful for analyzing high-dimensional
time-series because they are conjugate with Wishart likelihoods. In this
article, we show that Uhlig extended and beta-Bartlett processes are closely
related, but not equivalent: their hyperparameters can be matched so that they
have the same forward-filtered posteriors and one-step ahead forecasts, but
different joint (retrospective) posterior distributions. Under this
circumstance, Bayes factors cannot discriminate the models and alternative
approaches to model comparison are needed. We illustrate these issues in a
retrospective analysis of volatilities of returns of foreign exchange rates.
Additionally, we provide a backward sampling algorithm for the beta-Bartlett
process, for which retrospective analysis had not been developed.
"
"stat.TH","  We address the problem of estimating and comparing measures of concordance
that arise as Pearson's linear correlation coefficient between two random
variables transformed so that they follow the so-called concordance-inducing
distribution. The class of such transformed rank correlations includes
Spearman's rho, Blomqvist's beta and van der Waerden's coefficient as special
cases. To answer which transformed rank correlation is best to use, we propose
to compare them in terms of their best and worst asymptotic variances on a
given set of copulas. A criterion derived from this approach is that
concordance-inducing distributions with smaller variances of squared random
variables are more preferable. In particular, we show that Blomqvist's beta is
the optimal transformed rank correlation, and Spearman's rho outperforms van
der Waerden's coefficient. Moreover, we find that Kendall's tau also attains
the optimal asymptotic variances that Blomqvist's beta does, although Kendall's
tau is not a transformed rank correlation.
"
"stat.TH","  Anchor-based techniques reduce the computational complexity of spectral
clustering algorithms. Although empirical tests have shown promising results,
there is currently a lack of theoretical support for the anchoring approach. We
define a specific anchor-based algorithm and show that it is amenable to
rigorous analysis, as well as being effective in practice. We establish the
theoretical consistency of the method in an asymptotic setting where data is
sampled from an underlying continuous probability distribution. In particular,
we provide sharp asymptotic conditions for the algorithm parameters which
ensure that the anchor-based method can recover with high probability disjoint
clusters that are mutually separated by a positive distance. We illustrate the
performance of the algorithm on synthetic data and explain how the theoretical
convergence analysis can be used to inform the practical choice of parameter
scalings. We also test the accuracy and efficiency of the algorithm on two
large scale real data sets. We find that the algorithm offers clear advantages
over standard spectral clustering. We also find that it is competitive with the
state-of-the-art LSC method of Chen and Cai (Twenty-Fifth AAAI Conference on
Artificial Intelligence, 2011), while having the added benefit of a consistency
guarantee.
"
"stat.TH","  We study the problem of sampling from a probability distribution on $\mathbb
R^p$ defined via a convex and smooth potential function. We consider a
continuous-time diffusion-type process, termed Penalized Langevin dynamics
(PLD), the drift of which is the negative gradient of the potential plus a
linear penalty that vanishes when time goes to infinity. An upper bound on the
Wasserstein-2 distance between the distribution of the PLD at time $t$ and the
target is established. This upper bound highlights the influence of the speed
of decay of the penalty on the accuracy of the approximation. As a consequence,
considering the low-temperature limit we infer a new nonasymptotic guarantee of
convergence of the penalized gradient flow for the optimization problem.
"
"stat.TH","  Principal Component Analysis (PCA) is a powerful tool in statistics and
machine learning. While existing study of PCA focuses on the recovery of
principal components and their associated eigenvalues, there are few precise
characterizations of individual principal component scores that yield
low-dimensional embedding of samples. That hinders the analysis of various
spectral methods. In this paper, we first develop an $\ell_p$ perturbation
theory for a hollowed version of PCA in Hilbert spaces which provably improves
upon the vanilla PCA in the presence of heteroscedastic noises. Through a novel
$\ell_p$ analysis of eigenvectors, we investigate entrywise behaviors of
principal component score vectors and show that they can be approximated by
linear functionals of the Gram matrix in $\ell_p$ norm, which includes $\ell_2$
and $\ell_\infty$ as special examples. For sub-Gaussian mixture models, the
choice of $p$ giving optimal bounds depends on the signal-to-noise ratio, which
further yields optimality guarantees for spectral clustering. For contextual
community detection, the $\ell_p$ theory leads to a simple spectral algorithm
that achieves the information threshold for exact recovery. These also provide
optimal recovery results for Gaussian mixture and stochastic block models as
special cases.
"
"stat.TH","  In many instances, the application of approximate Bayesian methods is
hampered by two practical features: 1) the requirement to project the data down
to low-dimensional summary, including the choice of this projection, which
ultimately yields inefficient inference; 2) a possible lack of robustness to
deviations from the underlying model structure. Motivated by these efficiency
and robustness concerns, we construct a new Bayesian method that can deliver
efficient estimators when the underlying model is well-specified, and which is
simultaneously robust to certain forms of model misspecification. This new
approach bypasses the calculation of summaries by considering a norm between
empirical and simulated probability measures. For specific choices of the norm,
we demonstrate that this approach can deliver point estimators that are as
efficient as those obtained using exact Bayesian inference, while also
simultaneously displaying robustness to deviations from the underlying model
assumptions.
"
"stat.TH","  The paper analyses cointegration in vector autoregressive processes (VARs)
for the cases when both the number of coordinates, $N$, and the number of time
periods, $T$, are large and of the same order. We propose a way to examine a
VAR for the presence of cointegration based on a modification of the Johansen
likelihood ratio test. The advantage of our procedure over the original
Johansen test and its finite sample corrections is that our test does not
suffer from over-rejection. This is achieved through novel asymptotic theorems
for eigenvalues of matrices in the test statistic in the regime of
proportionally growing $N$ and $T$. Our theoretical findings are supported by
Monte Carlo simulations and an empirical illustration. Moreover, we find a
surprising connection with multivariate analysis of variance (MANOVA) and
explain why it emerges.
"
"stat.TH","  This paper considers the deconvolution problem in the case where the target
signal is multidimensional and no information is known about the noise
distribution. More precisely, no assumption is made on the noise distribution
and no samples are available to estimate it: the deconvolution problem is
solved based only on the corrupted signal observations. We establish the
identifiability of the model up to translation when the signal has a Laplace
transform with an exponential growth smaller than 2 and when it can be
decomposed into two dependent components. Then, we propose an estimator of the
probability density function of the signal without any assumption on the noise
distribution. As this estimator depends of the lightness of the tail of the
signal distribution which is usually unknown, a model selection procedure is
proposed to obtain an adaptive estimator in this parameter with the same rate
of convergence as the estimator with a known tail parameter. Finally, we
establish a lower bound on the minimax rate of convergence that matches the
upper bound.
"
"stat.TH","  This paper addresses inference in large panel data models in the presence of
both cross-sectional and temporal dependence of unknown form. We are interested
in making inferences that do not rely on the choice of any smoothing parameter
as is the case with the often employed ""HAC"" estimator for the covariance
matrix. To that end, we propose a cluster estimator for the asymptotic
covariance of the estimators and valid bootstrap schemes that do not require
the selection of a bandwidth or smoothing parameter and accommodate the
nonparametric nature of both temporal and cross-sectional dependence. Our
approach is based on the observation that the spectral representation of the
fixed effect panel data model is such that the errors become approximately
temporally uncorrelated. Our proposed bootstrap schemes can be viewed as wild
bootstraps in the frequency domain. We present some Monte-Carlo simulations to
shed some light on the small sample performance of our inferential procedure.
"
"stat.TH","  We consider the estimation of a scalar parameter, when two estimators are
available. The first is always consistent. The second is inconsistent in
general, but has a smaller asymptotic variance than the first, and may be
consistent if an assumption is satisfied. We propose to use the weighted sum of
the two estimators with the lowest estimated mean-squared error (MSE). We show
that this third estimator dominates the other two from a minimax-regret
perspective: the maximum asymptotic-MSE-gain one may incur by using this
estimator rather than one of the other estimators is larger than the maximum
asymptotic-MSE-loss.
"
"stat.TH","  Estimating the mixing density of a mixture distribution remains an
interesting problem in statistics literature. Using a stochastic approximation
method, Newton and Zhang (1999) introduced a fast recursive algorithm for
estimating the mixing density of a mixture. Under suitably chosen weights the
stochastic approximation estimator converges to the true solution. In Tokdar
et. al. (2009) the consistency of this recursive estimation method was
established. However, the proof of consistency of the resulting estimator used
independence among observations as an assumption. Here, we extend the
investigation of performance of Newton's algorithm to several dependent
scenarios. We first prove that the original algorithm under certain conditions
remains consistent when the observations are arising form a weakly dependent
process with fixed marginal with the target mixture as the marginal density.
For some of the common dependent structures where the original algorithm is no
longer consistent, we provide a modification of the algorithm that generates a
consistent estimator.
"
"stat.TH","  Component-wise MCMC algorithms, including Gibbs and conditional
Metropolis-Hastings samplers, are commonly used for sampling from multivariate
probability distributions. A long-standing question regarding Gibbs algorithms
is whether a deterministic-scan (systematic-scan) sampler converges faster than
its random-scan counterpart. We answer this question when the samplers involve
two components by establishing an exact quantitative relationship between the
$L^2$ convergence rates of the two samplers. The relationship shows that the
deterministic-scan sampler converges faster. We also establish qualitative
relations among the convergence rates of two-component Gibbs samplers and some
conditional Metropolis-Hastings variants. For instance, it is shown that if a
two-component conditional Metropolis-Hastings sampler is geometrically ergodic,
then so are the associated Gibbs samplers.
"
"stat.TH","  A multivariate errors-in-variables (EIV) model with an intercept term, and a
polynomial EIV model are considered. Focus is made on a structural
homoskedastic case, where vectors of covariates are i.i.d. and measurement
errors are i.i.d. as well. The covariates contaminated with errors are normally
distributed and the corresponding classical errors are also assumed normal. In
both models, it is shown that (inconsistent) ordinary least squares estimators
of regression parameters yield an a.s. approximation to the best prediction of
response given the values of observable covariates. Thus, not only in the
linear EIV, but in the polynomial EIV models as well, consistent estimators of
regression parameters are useless in the prediction problem, provided the size
and covariance structure of observation errors for the predicted subject do not
differ from those in the data used for the model fitting.
"
"stat.TH","  We introduce deep involutive generative models, a new architecture for deep
generative modeling, and use them to define Involutive Neural MCMC, a new
approach to fast neural MCMC. An involutive generative model represents a
probability kernel $G(\phi \mapsto \phi')$ as an involutive (i.e.,
self-inverting) deterministic function $f(\phi, \pi)$ on an enlarged state
space containing auxiliary variables $\pi$. We show how to make these models
volume preserving, and how to use deep volume-preserving involutive generative
models to make valid Metropolis-Hastings updates based on an auxiliary variable
scheme with an easy-to-calculate acceptance ratio. We prove that deep
involutive generative models and their volume-preserving special case are
universal approximators for probability kernels. This result implies that with
enough network capacity and training time, they can be used to learn
arbitrarily complex MCMC updates. We define a loss function and optimization
algorithm for training parameters given simulated data. We also provide initial
experiments showing that Involutive Neural MCMC can efficiently explore
multi-modal distributions that are intractable for Hybrid Monte Carlo, and can
converge faster than A-NICE-MC, a recently introduced neural MCMC technique.
"
"stat.TH","  We derive an asymptotic expansion for the log likelihood of Gaussian mixture
models (GMMs) with equal covariance matrices in the low signal-to-noise regime.
The expansion reveals an intimate connection between two types of algorithms
for parameter estimation: the method of moments and likelihood optimizing
algorithms such as Expectation-Maximization (EM). We show that likelihood
optimization in the low SNR regime reduces to a sequence of least squares
optimization problems that match the moments of the estimate to the ground
truth moments one by one. This connection is a stepping stone toward the
analysis of EM and maximum likelihood estimation in a wide range of models. A
motivating application for the study of low SNR mixture models is cryo-electron
microscopy data, which can be modeled as a GMM with algebraic constraints
imposed on the mixture centers. We discuss the application of our expansion to
algebraically constrained GMMs, among other example models of interest.
"
"stat.TH","  This article proposes a new filtering model for stationary Gaussian Markov
statistical experiments, given by diffusion-type difference stochastic
equations.
"
"stat.TH","  For multivariant Wright-Fisher models in population genetics, we introduce
equilibrium states, expressed by fluctuations of probability ratio, in contrast
to the traditionally used fluctuations, expressed by the difference between the
current value of the random process and its equilibrium value. Then the drift
component of the dynamic process of gene frequencies, primarily expressed as a
ratio of two quadratic forms, is transformed into a cubic parabola with a
certain normalization factor.
"
"stat.TH","  In this article, we study Bayesian inverse problems with multi-layered
Gaussian priors. We first describe the conditionally Gaussian layers in terms
of a system of stochastic partial differential equations. We build the
computational inference method using a finite-dimensional Galerkin method. We
show that the proposed approximation has a convergence-in-probability property
to the solution of the original multi-layered model. We then carry out Bayesian
inference using the preconditioned Crank--Nicolson algorithm which is modified
to work with multi-layered Gaussian fields. We show via numerical experiments
in signal deconvolution and computerized X-ray tomography problems that the
proposed method can offer both smoothing and edge preservation at the same
time.
"
"stat.TH","  Predicting the response at an unobserved location is a fundamental problem in
spatial statistics. Given the difficulty in modeling spatial dependence,
especially in non-stationary cases, model-based prediction intervals are at
risk of misspecification bias that can negatively affect their validity. Here
we present a new approach for model-free spatial prediction based on the {\em
conformal prediction} machinery. Our key observation is that spatial data can
be treated as exactly or approximately exchangeable in a wide range of
settings. For example, when the spatial locations are deterministic, we prove
that the response values are, in a certain sense, locally approximately
exchangeable for a broad class of spatial processes, and we develop a local
spatial conformal prediction algorithm that yields valid prediction intervals
without model assumptions. Numerical examples with both real and simulated data
confirm that the proposed conformal prediction intervals are valid and
generally more efficient than existing model-based procedures across a range of
non-stationary and non-Gaussian settings.
"
"stat.TH","  False negative errors are of major concern in applications where missing a
high proportion of true signals may cause serious consequences. False negative
control, however, raises a bottleneck challenge in high-dimensional inference
when signals are not identifiable at individual level. We propose a Dual
Control of Errors (DCOE) method that regulates not only false positive but also
false negative errors in measures that are highly relevant to high-dimensional
data analysis. DCOE is developed under general covariance dependence with a new
calibration procedure to measure the dependence effect. We specify how
dependence co-acts with signal sparsity to determine the difficulty level of
the dual control task and prove that DCOE is effective in retaining true
signals that are not identifiable at individual level. Simulation studies are
conducted to compare the new method with existing methods that focus on only
one type of error. DCOE is shown to be more powerful than FDR methods and less
aggressive than existing false negative control methods. DCOE is applied to a
fMRI dataset to identify voxels that are functionally relevant to saccadic eye
movements. The new method exhibits a nice balance in retaining signal voxels
and avoiding excessive noise voxels.
"
"stat.TH","  Researchers are often interested in treatment effects on outcomes that are
only defined conditional on a post-treatment event status. For example, in a
study of the effect of different cancer treatments on quality of life at end of
follow-up, the quality of life of individuals who die during the study is
undefined. In these settings, a naive contrast of outcomes conditional on the
post-treatment variable is not an average causal effect, even in a randomized
experiment. Therefore the effect in the principal stratum of those who would
have the same value of the post-treatment variable regardless of treatment,
such as the always survivors in a truncation by death setting, is often
advocated for causal inference. While this principal stratum effect is a well
defined causal contrast, it is often hard to justify that it is relevant to
scientists, patients or policy makers, and it cannot be identified without
relying on unfalsifiable assumptions. Here we formulate alternative estimands,
the conditional separable effects, that have a natural causal interpretation
under assumptions that can be falsified in a randomized experiment. We provide
identification results and introduce different estimators, including a doubly
robust estimator derived from the nonparametric influence function. As an
illustration, we estimate a conditional separable effect of chemotherapies on
quality of life in patients with prostate cancer, using data from a randomized
clinical trial.
"
"stat.TH","  Subgraph counts - in particular the number of occurrences of small shapes
such as triangles - characterize properties of random networks, and as a result
have seen wide use as network summary statistics. However, subgraphs are
typically counted globally, and existing approaches fail to describe
vertex-specific characteristics. On the other hand, rooted subgraph counts -
counts focusing on any given vertex's neighborhood - are fundamental
descriptors of local network properties. We derive the asymptotic joint
distribution of rooted subgraph counts in inhomogeneous random graphs, a model
which generalizes many popular statistical network models. This result enables
a shift in the statistical analysis of large graphs, from estimating network
summaries, to estimating models linking local network structure and
vertex-specific covariates. As an example, we consider a school friendship
network and show that local friendship patterns are significant predictors of
gender and race.
"
"stat.TH","  Multitask learning and related areas such as multi-source domain adaptation
address modern settings where datasets from $N$ related distributions $\{P_t\}$
are to be combined towards improving performance on any single such
distribution ${\cal D}$. A perplexing fact remains in the evolving theory on
the subject: while we would hope for performance bounds that account for the
contribution from multiple tasks, the vast majority of analyses result in
bounds that improve at best in the number $n$ of samples per task, but most
often do not improve in $N$. As such, it might seem at first that the
distributional settings or aggregation procedures considered in such analyses
might be somehow unfavorable; however, as we show, the picture happens to be
more nuanced, with interestingly hard regimes that might appear otherwise
favorable.
  In particular, we consider a seemingly favorable classification scenario
where all tasks $P_t$ share a common optimal classifier $h^*,$ and which can be
shown to admit a broad range of regimes with improved oracle rates in terms of
$N$ and $n$. Some of our main results are as follows:
  $\bullet$ We show that, even though such regimes admit minimax rates
accounting for both $n$ and $N$, no adaptive algorithm exists; that is, without
access to distributional information, no algorithm can guarantee rates that
improve with large $N$ for $n$ fixed.
  $\bullet$ With a bit of additional information, namely, a ranking of tasks
$\{P_t\}$ according to their distance to a target ${\cal D}$, a simple
rank-based procedure can achieve near optimal aggregations of tasks' datasets,
despite a search space exponential in $N$. Interestingly, the optimal
aggregation might exclude certain tasks, even though they all share the same
$h^*$.
"
"stat.TH","  Despite the popularism of Bayesian neural networks in recent years, its use
is somewhat limited in complex and big data situations due to the computational
cost associated with full posterior evaluations. Variational Bayes (VB)
provides a useful alternative to circumvent the computational cost and time
complexity associated with the generation of samples from the true posterior
using Markov Chain Monte Carlo (MCMC) techniques. The efficacy of the VB
methods is well established in machine learning literature. However, its
potential broader impact is hindered due to a lack of theoretical validity from
a statistical perspective. However there are few results which revolve around
the theoretical properties of VB, especially in non-parametric problems. In
this paper, we establish the fundamental result of posterior consistency for
the mean-field variational posterior (VP) for a feed-forward artificial neural
network model. The paper underlines the conditions needed to guarantee that the
VP concentrates around Hellinger neighborhoods of the true density function.
Additionally, the role of the scale parameter and its influence on the
convergence rates has also been discussed. The paper mainly relies on two
results (1) the rate at which the true posterior grows (2) the rate at which
the KL-distance between the posterior and variational posterior grows. The
theory provides a guideline of building prior distributions for Bayesian NN
models along with an assessment of accuracy of the corresponding VB
implementation.
"
"stat.TH","  Our main results are quantitative bounds in the multivariate normal
approximation of centred subgraph counts in random graphs generated by a
general graphon and independent vertex labels. The main motivation to
investigate these statistics is the fact that they are key to understanding
fluctuations of regular subgraph counts -- the cornerstone of dense graph limit
theory -- since they act as an orthogonal basis of a corresponding $L_2$ space.
We also identify the resulting limiting Gaussian stochastic measures by means
of the theory of generalised U-statistics and Gaussian Hilbert spaces, which we
think is a suitable framework to describe and understand higher-order
fluctuations in dense random graph models. With this article, we believe we
answer the question ""What is the central limit theorem of dense graph limit
theory?"".
"
"stat.TH","  Langevin diffusion (LD) is one of the main workhorses for sampling problems.
However, its convergence rate can be significantly reduced if the target
distribution is a mixture of multiple densities, especially when each component
concentrates around a different mode. Replica exchange Langevin diffusion
(ReLD) is a sampling method that can circumvent this issue. In particular, ReLD
adds another LD sampling a high-temperature version of the target density, and
exchange the locations of two LDs according to a Metropolis-Hasting type of
law. This approach can be further extended to multiple replica exchange
Langevin diffusion (mReLD), where $K$ additional LDs are added to sample
distributions at different temperatures and exchanges take place between
neighboring-temperature processes. While ReLD and mReLD have been used
extensively in statistical physics, molecular dynamics, and other applications,
there is little existing analysis on its convergence rate and choices of
temperatures. This paper closes these gaps assuming the target distribution is
a mixture of log-concave densities. We show ReLD can obtain constant or even
better convergence rates even when the density components of the mixture
concentrate around isolated modes. We also show using mReLD with $K$ additional
LDs can achieve the same result while the exchange frequency only needs to be
$(1/K)$-th power of the one in ReLD.
"
"stat.TH","  We study the fundamental problems of agnostically learning halfspaces and
ReLUs under Gaussian marginals. In the former problem, given labeled examples
$(\mathbf{x}, y)$ from an unknown distribution on $\mathbb{R}^d \times \{ \pm
1\}$, whose marginal distribution on $\mathbf{x}$ is the standard Gaussian and
the labels $y$ can be arbitrary, the goal is to output a hypothesis with 0-1
loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is the 0-1 loss of the
best-fitting halfspace. In the latter problem, given labeled examples
$(\mathbf{x}, y)$ from an unknown distribution on $\mathbb{R}^d \times
\mathbb{R}$, whose marginal distribution on $\mathbf{x}$ is the standard
Gaussian and the labels $y$ can be arbitrary, the goal is to output a
hypothesis with square loss $\mathrm{OPT}+\epsilon$, where $\mathrm{OPT}$ is
the square loss of the best-fitting ReLU. We prove Statistical Query (SQ) lower
bounds of $d^{\mathrm{poly}(1/\epsilon)}$ for both of these problems. Our SQ
lower bounds provide strong evidence that current upper bounds for these tasks
are essentially best possible.
"
"stat.TH","  Given partially observed pairwise comparison data generated by the
Bradley-Terry-Luce (BTL) model, we study the problem of top-$k$ ranking. That
is, to optimally identify the set of top-$k$ players. We derive the minimax
rate with respect to a normalized Hamming loss. This provides the first result
in the literature that characterizes the partial recovery error in terms of the
proportion of mistakes for top-$k$ ranking. We also derive the optimal signal
to noise ratio condition for the exact recovery of the top-$k$ set. The maximum
likelihood estimator (MLE) is shown to achieve both optimal partial recovery
and optimal exact recovery. On the other hand, we show another popular
algorithm, the spectral method, is in general sub-optimal. Our results
complement the recent work by Chen et al. (2019) that shows both the MLE and
the spectral method achieve the optimal sample complexity for exact recovery.
It turns out the leading constants of the sample complexity are different for
the two algorithms. Another contribution that may be of independent interest is
the analysis of the MLE without any penalty or regularization for the BTL
model. This closes an important gap between theory and practice in the
literature of ranking.
"
"stat.TH","  Estimation of the covariance matrix of asset returns is crucial to portfolio
construction. As suggested by economic theories, the correlation structure
among assets differs between emerging markets and developed countries. It is
therefore imperative to make rigorous statistical inference on correlation
matrix equality between the two groups of countries. However, if the
traditional vector-valued approach is undertaken, such inference is either
infeasible due to limited number of countries comparing to the relatively
abundant assets, or invalid due to the violations of temporal independence
assumption. This highlights the necessity of treating the observations as
matrix-valued rather than vector-valued. With matrix-valued observations, our
problem of interest can be formulated as statistical inference on covariance
structures under matrix normal distributions, i.e., testing independence and
correlation equality, as well as the corresponding support estimations. We
develop procedures that are asymptotically optimal under some regularity
conditions. Simulation results demonstrate the computational and statistical
advantages of our procedures over certain existing state-of-the-art methods.
Application of our procedures to stock market data validates several economic
propositions.
"
"stat.TH","  The subspace approximation problem with outliers, for given $n$ points in $d$
dimensions $x_{1},\ldots, x_{n} \in R^{d}$, an integer $1 \leq k \leq d$, and
an outlier parameter $0 \leq \alpha \leq 1$, is to find a $k$-dimensional
linear subspace of $R^{d}$ that minimizes the sum of squared distances to its
nearest $(1-\alpha)n$ points. More generally, the $\ell_{p}$ subspace
approximation problem with outliers minimizes the sum of $p$-th powers of
distances instead of the sum of squared distances. Even the case of robust PCA
is non-trivial, and previous work requires additional assumptions on the input.
Any multiplicative approximation algorithm for the subspace approximation
problem with outliers must solve the robust subspace recovery problem, a
special case in which the $(1-\alpha)n$ inliers in the optimal solution are
promised to lie exactly on a $k$-dimensional linear subspace. However, robust
subspace recovery is Small Set Expansion (SSE)-hard.
  We show how to extend dimension reduction techniques and bi-criteria
approximations based on sampling to the problem of subspace approximation with
outliers. To get around the SSE-hardness of robust subspace recovery, we assume
that the squared distance error of the optimal $k$-dimensional subspace summed
over the optimal $(1-\alpha)n$ inliers is at least $\delta$ times its
squared-error summed over all $n$ points, for some $0 < \delta \leq 1 -
\alpha$. With this assumption, we give an efficient algorithm to find a subset
of $poly(k/\epsilon) \log(1/\delta) \log\log(1/\delta)$ points whose span
contains a $k$-dimensional subspace that gives a multiplicative
$(1+\epsilon)$-approximation to the optimal solution. The running time of our
algorithm is linear in $n$ and $d$. Interestingly, our results hold even when
the fraction of outliers $\alpha$ is large, as long as the obvious condition $0
< \delta \leq 1 - \alpha$ is satisfied.
"
"stat.TH","  In this paper, we introduce a robust nonparametric density estimator
combining the popular Kernel Density Estimation method and the Median-of-Means
principle (MoM-KDE). This estimator is shown to achieve robustness to any kind
of anomalous data, even in the case of adversarial contamination. In
particular, while previous works only prove consistency results under known
contamination model, this work provides finite-sample high-probability
error-bounds without a priori knowledge on the outliers. Finally, when compared
with other robust kernel estimators, we show that MoM-KDE achieves competitive
results while having significant lower computational complexity.
"
"stat.TH","  In this work we introduce a general approach, based on the mar-tingale
representation of a sampling design and Azuma-Hoeffding's inequality , to
derive exponential inequalities for the difference between a Horvitz-Thompson
estimator and its expectation. Applying this idea, we establish such
inequalities for Chao's procedure, Till{\'e}'s elimination procedure, the
generalized Midzuno method as well as for Brewer's method. As a by-product, we
prove that the first three sampling designs are (conditionally) negatively
associated. For such sampling designs, we show that that the inequality we
obtain is usually sharper than the one obtained by applying known results for
negatively associated random variables.
"
"stat.TH","  Distributed machine learning systems have been receiving increasing
attentions for their efficiency to process large scale data. Many distributed
frameworks have been proposed for different machine learning tasks. In this
paper, we study the distributed kernel regression via the divide and conquer
approach. This approach has been proved asymptotically minimax optimal if the
kernel is perfectly selected so that the true regression function lies in the
associated reproducing kernel Hilbert space. However, this is usually, if not
always, impractical because kernels that can only be selected via prior
knowledge or a tuning process are hardly perfect. Instead it is more common
that the kernel is good enough but imperfect in the sense that the true
regression can be well approximated by but does not lie exactly in the kernel
space. We show distributed kernel regression can still achieves capacity
independent optimal rate in this case. To this end, we first establish a
general framework that allows to analyze distributed regression with response
weighted base algorithms by bounding the error of such algorithms on a single
data set, provided that the error bounds has factored the impact of the
unexplained variance of the response variable. Then we perform a leave one out
analysis of the kernel ridge regression and bias corrected kernel ridge
regression, which in combination with the aforementioned framework allows us to
derive sharp error bounds and capacity independent optimal rates for the
associated distributed kernel regression algorithms. As a byproduct of the
thorough analysis, we also prove the kernel ridge regression can achieve rates
faster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting
which, to our best knowledge, are first observed and novel in regression
learning.
"
"stat.TH","  Modelling wildfire occurrences is important for disaster management including
prevention, detection and suppression of large catastrophic events. We present
a spatial Poisson hurdle model for exploring geographical variation of monthly
counts of wildfire occurrences and apply it to Indonesia and Australia. The
model includes two a priori independent spatially structured latent effects
that account for residual spatial variation in the probability of wildfire
occurrence, and the positive count rate given an occurrence. Inference is
provided by empirical Bayes using the Laplace approximation to the marginal
posterior which provides fast inference for latent Gaussian models with sparse
structures. In both cases, our model matched several empirically known facts
about wildfires. We conclude that elevation, percentage tree cover, relative
humidity, surface temperature, and the interaction between humidity and
temperature to be important predictors of monthly counts of wildfire
occurrences. Further, our findings show opposing effects for surface
temperature and its interaction with relative humidity.
"
"stat.TH","  We in this paper consider Fr\'echet sufficient dimension reduction with
responses being complex random objects in a metric space and high dimension
Euclidean predictors. We propose a novel approach called weighted inverse
regression ensemble method for linear Fr\'echet sufficient dimension reduction.
The method is further generalized as a new operator defined on reproducing
kernel Hilbert spaces for nonlinear Fr\'echet sufficient dimension reduction.
We provide theoretical guarantees for the new method via asymptotic analysis.
Intensive simulation studies verify the performance of our proposals. And we
apply our methods to analyze the handwritten digits data to demonstrate its use
in real applications.
"
"stat.TH","  We investigate the generalisation performance of Distributed Gradient Descent
with Implicit Regularisation and Random Features in the homogenous setting
where a network of agents are given data sampled independently from the same
unknown distribution. Along with reducing the memory footprint, Random Features
are particularly convenient in this setting as they provide a common
parameterisation across agents that allows to overcome previous difficulties in
implementing Decentralised Kernel Regression. Under standard source and
capacity assumptions, we establish high probability bounds on the predictive
performance for each agent as a function of the step size, number of
iterations, inverse spectral gap of the communication matrix and number of
Random Features. By tuning these parameters, we obtain statistical rates that
are minimax optimal with respect to the total number of samples in the network.
The algorithm provides a linear improvement over single machine Gradient
Descent in memory cost and, when agents hold enough data with respect to the
network size and inverse spectral gap, a linear speed-up in computational
runtime for any network topology. We present simulations that show how the
number of Random Features, iterations and samples impact predictive
performance.
"
"stat.TH","  The periodogram is a widely used tool to analyze second order stationary time
series. An attractive feature of the periodogram is that the expectation of the
periodogram is approximately equal to the underlying spectral density of the
time series. However, this is only an approximation, and it is well known that
the periodogram has a finite sample bias, which can be severe in small samples.
In this paper, we show that the bias arises because of the finite boundary of
observation in one of the discrete Fourier transforms which is used in the
construction of the periodogram. Moreover, we show that by using the best
linear predictors of the time series over the boundary of observation we can
obtain a ""complete periodogram"" that is an unbiased estimator of the spectral
density. In practice, the ""complete periodogram"" cannot be evaluated as the
best linear predictors are unknown. We propose a method for estimating the best
linear predictors and prove that the resulting ""estimated complete periodogram""
has a smaller bias than the regular periodogram. The estimated complete
periodogram and a tapered version of it are used to estimate parameters, which
can be represented in terms of the integrated spectral density. We prove that
the resulting estimators have a smaller bias than their regular periodogram
counterparts. The proposed method is illustrated with simulations and real
data.
"
"stat.TH","  In high-dimensional linear regression, would increasing effect sizes always
improve model selection, while maintaining all the other conditions unchanged
(especially fixing the sparsity of regression coefficients)? In this paper, we
answer this question in the \textit{negative} in the regime of linear sparsity
for the Lasso method, by introducing a new notion we term effect size
heterogeneity. Roughly speaking, a regression coefficient vector has high
effect size heterogeneity if its nonzero entries have significantly different
magnitudes. From the viewpoint of this new measure, we prove that the false and
true positive rates achieve the optimal trade-off uniformly along the Lasso
path when this measure is maximal in a certain sense, and the worst trade-off
is achieved when it is minimal in the sense that all nonzero effect sizes are
roughly equal. Moreover, we demonstrate that the first false selection occurs
much earlier when effect size heterogeneity is minimal than when it is maximal.
The underlying cause of these two phenomena is, metaphorically speaking, the
""competition"" among variables with effect sizes of the same magnitude in
entering the model. Taken together, our findings suggest that effect size
heterogeneity shall serve as an important complementary measure to the sparsity
of regression coefficients in the analysis of high-dimensional regression
problems. Our proofs use techniques from approximate message passing theory as
well as a novel technique for estimating the rank of the first false variable.
"
"stat.TH","  In this paper, we propose Bayesian non-parametric tests for one-sample and
two-sample multivariate location problems. We model the underlying
distributions using a Dirichlet process prior. For the one-sample problem, we
compute a Bayesian credible set of the multivariate spatial median and accept
the null hypothesis if the credible set contains the null value. For the
two-sample problem, we form a credible set for the difference of the spatial
medians of the two samples and we accept the null hypothesis of equality if the
credible set contains zero. We derive the local asymptotic power of the tests
under shrinking alternatives, and also present a simulation study to compare
the finite-sample performance of our testing procedures with existing
parametric and non-parametric tests.
"
"stat.TH","  Let $X$ be a continuous-time strongly mixing or weakly dependent process and
$T$ a renewal process independent of $X$ with inter-arrival times $\{\tau_i\}$.
We show general conditions under which the sampled process
$(X_{T_i},\tau_i)^{\top}$ is strongly mixing or weakly dependent. Moreover, we
explicitly compute the strong mixing or weak dependence coefficients of the
renewal sampled process and show that exponential or power decay of the
coefficients of $X$ is preserved (at least asymptotically). Our results imply
that essentially all central limit theorems available in the literature for
strongly mixing or weakly dependent processes can be applied when renewal
sampled observations of the process $X$ are at disposal.
"
"stat.TH","  In this paper, we generalize the property of local asymptotic normality (LAN)
to an enlarged neighborhood, under the name of rescaled local asymptotic
normality (RLAN). We obtain sufficient conditions for a regular parametric
model to satisfy RLAN. We show that RLAN supports the construction of a
statistically efficient estimator which maximizes a cubic approximation to the
log-likelihood on this enlarged neighborhood. In the context of Monte Carlo
inference, we find that this maximum cubic likelihood estimator can maintain
its statistical efficiency in the presence of asymptotically increasing Monte
Carlo error in likelihood evaluation.
"
"stat.TH","  We consider so-called univariate unlinked (sometimes ""decoupled,"" or
""shuffled"") regression when the unknown regression curve is monotone. In
standard monotone regression, one observes a pair $(X,Y)$ where a response $Y$
is linked to a covariate $X$ through the model $Y= m_0(X) + \epsilon$, with
$m_0$ the (unknown) monotone regression function and $\epsilon$ the unobserved
error (assumed to be independent of $X$). In the unlinked regression setting
one gets only to observe a vector of realizations from both the response $Y$
and from the covariate $X$ where now $Y \stackrel{d}{=} m_0(X) + \epsilon$.
There is no (observed) pairing of $X$ and $Y$. Despite this, it is actually
still possible to derive a consistent non-parametric estimator of $m_0$ under
the assumption of monotonicity of $m_0$ and knowledge of the distribution of
the noise $\epsilon$. In this paper, we establish an upper bound on the rate of
convergence of such an estimator under minimal assumption on the distribution
of the covariate $X$. We discuss extensions to the case in which the
distribution of the noise is unknown. We develop a gradient-descent-based
algorithm for its computation, and we demonstrate its use on synthetic data.
Finally, we apply our method (in a fully data driven way, without knowledge of
the error distribution) on longitudinal data from the US Consumer Expenditure
Survey.
"
"stat.TH","  In many applications, hypothesis testing is based on an asymptotic
distribution of statistics. The aim of this paper is to clarify and extend
multiple correction procedures when the statistics are asymptotically Gaussian.
We propose a unified framework to prove their asymptotic behavior which is
valid in the case of highly correlated tests. We focus on correlation tests
where several test statistics are proposed. All these multiple testing
procedures on correlations are shown to control FWER. An extensive simulation
study on correlation-based graph estimation highlights finite sample behavior,
independence on the sparsity of graphs and dependence on the values of
correlations. Empirical evaluation of power provides comparisons of the
proposed methods. Finally validation of our procedures is proposed on real
dataset of rats brain connectivity measured by fMRI. We confirm our theoretical
findings by applying our procedures on a full null hypotheses with data from
dead rats. Data on alive rats show the performance of the proposed procedures
to correctly identify brain connectivity graphs with controlled errors.
"
"stat.TH","  We propose a new approach to the problem of high-dimensional multivariate
ANOVA via bootstrapping max statistics that involve the differences of sample
mean vectors, through constructing simultaneous confidence intervals for the
differences of population mean vectors. The proposed method is suited to
simultaneously test the equality of several pairs of mean vectors of
potentially more than two populations. By exploiting the variance decay
property that is a natural feature in relevant applications, we are able to
provide dimension-free and nearly-parametric convergence rates for Gaussian
approximation, bootstrap approximation, and the size of the test. We
demonstrate the proposed approach with ANOVA problems for functional data and
sparse count data. The proposed methodology is shown to work well in
simulations and several real data applications.
"
"stat.TH","  A well-known first-order method for sampling from log-concave probability
distributions is the Unadjusted Langevin Algorithm (ULA). This work proposes a
new annealing step-size schedule for ULA, which allows to prove new convergence
guarantees for sampling from a smooth log-concave distribution, which are not
covered by existing state-of-the-art convergence guarantees. To establish this
result, we derive a new theoretical bound that relates the Wasserstein distance
to total variation distance between any two log-concave distributions that
complements the reach of Talagrand T2 inequality. Moreover, applying this new
step size schedule to an existing constrained sampling algorithm, we show
state-of-the-art convergence rates for sampling from a constrained log-concave
distribution, as well as improved dimension dependence.
"
"stat.TH","  In this paper, we propose and analyze a model selection method for tree
tensor networks in an empirical risk minimization framework. Tree tensor
networks, or tree-based tensor formats, are prominent model classes for the
approximation of high-dimensional functions in numerical analysis and data
science. They correspond to sum-product neural networks with a sparse
connectivity associated with a dimension partition tree $T$, widths given by a
tuple $r$ of tensor ranks, and multilinear activation functions (or units). The
approximation power of these model classes has been proved to be near-optimal
for classical smoothness classes. However, in an empirical risk minimization
framework with a limited number of observations, the dimension tree $T$ and
ranks $r$ should be selected carefully to balance estimation and approximation
errors. In this paper, we propose a complexity-based model selection strategy
\`a la Barron, Birg\'e, Massart. Given a family of model classes, with
different trees, ranks and tensor product feature spaces, a model is selected
by minimizing a penalized empirical risk, with a penalty depending on the
complexity of the model class. After deriving bounds of the metric entropy of
tree tensor networks with bounded parameters, we deduce a form of the penalty
from bounds on suprema of empirical processes. This choice of penalty yields a
risk bound for the predictor associated with the selected model. For classical
smoothness spaces, we show that the proposed strategy is minimax optimal in a
least-squares setting. In practice, the amplitude of the penalty is calibrated
with a slope heuristics method. Numerical experiments in a least-squares
regression setting illustrate the performance of the strategy for the
approximation of multivariate functions and univariate functions identified
with tensors by tensorization (quantization).
"
"stat.TH","  A new approach in stochastic optimization via the use of stochastic gradient
Langevin dynamics (SGLD) algorithms, which is a variant of stochastic gradient
decent (SGD) methods, allows us to efficiently approximate global minimizers of
possibly complicated, high-dimensional landscapes. With this in mind, we extend
here the non-asymptotic analysis of SGLD to the case of discontinuous
stochastic gradients. We are thus able to provide theoretical guarantees for
the algorithm's convergence in (standard) Wasserstein distances for both convex
and non-convex objective functions. We also provide explicit upper estimates of
the expected excess risk associated with the approximation of global minimizers
of these objective functions. All these findings allow us to devise and present
a fully data-driven approach for the optimal allocation of weights for the
minimization of CVaR of portfolio of assets with complete theoretical
guarantees for its performance. Numerical results illustrate our main findings.
"
"stat.TH","  We study the parameter estimation problem of Vasicek Model driven by
sub-fractional Brownian processes from discrete observations, and let
{S_t^H,t>=0} denote a sub-fractional Brownian motion whose Hurst parameter
1/2<H<1 . The studies are as follows: firstly, two unknown parameters in the
model are estimated by the least squares method. Secondly, the strong
consistency and the asymptotic distribution of the estimators are studied
respectively. Finally, our estimators are validated by numerical simulation.
"
"stat.TH","  Best subset selection (BSS) is fundamental in statistics and machine
learning. Despite the intensive studies of it, the fundamental question of when
BSS is truly the ""best"", namely yielding the oracle estimator, remains
partially answered. In this paper, we address this important issue by giving a
weak sufficient condition and a strong necessary condition for BSS to exactly
recover the true model. We also give a weak sufficient condition for BSS to
achieve the sure screening property. On the optimization aspect, we find that
the exact combinatorial minimizer for BSS is unnecessary: all the established
statistical properties for the best subset carry over to any sparse model whose
residual sum of squares is close enough to that of the best subset. In
particular, we show that an iterative hard thresholding (IHT) algorithm can
find a sparse subset with the sure screening property within logarithmic steps;
another round of BSS within this set can recover the true model. The simulation
studies and real data examples show that IHT yields lower false discovery rates
and higher true positive rates than the competing approaches including LASSO,
SCAD and SIS.
"
"stat.TH","  In the fields of clinical trials, biomedical surveys, marketing, banking,
with dichotomous response variable, the logistic regression is considered as an
alternative convenient approach to linear regression. In this paper, we develop
a novel bootstrap technique based on perturbation resampling method for
approximating the distribution of the maximum likelihood estimator (MLE) of the
regression parameter vector. We establish second order correctness of the
proposed bootstrap method after proper studentization and smoothing. It is
shown that inferences drawn based on the proposed bootstrap method are more
accurate compared to that based on asymptotic normality. The main challenge in
establishing second order correctness remains in the fact that the response
variable being binary, the resulting MLE has a lattice structure. We show the
direct bootstrapping approach fails even after studentization. We adopt
smoothing technique developed in Lahiri (1993) to ensure that the smoothed
studentized version of the MLE has a density. Similar smoothing strategy is
employed to the bootstrap version also to achieve second order correct
approximation.
"
"stat.TH","  Three common classes of kernel regression estimators are considered: the
Nadaraya--Watson (NW) estimator, the Priestley--Chao (PC) estimator, and the
Gasser--M\""uller (GM) estimator. It is shown that (i) the GM estimator has a
certain monotonicity preservation property for any kernel $K$, (ii) the NW
estimator has this property if and only the kernel $K$ is log concave, and
(iii) the PC estimator does not have this property for any kernel $K$. Other
related properties of these regression estimators are discussed.
"
"stat.TH","  In light of recent work studying massive functional/longitudinal data, such
as the resulting data from the COVID-19 pandemic, we propose a novel
functional/longitudinal data model which is a combination of the popular
varying coefficient (VC) model and additive model. We call it Semi-VCAM in
which the response could be a functional/longitudinal variable, and the
explanatory variables could be a mixture of functional/longitudinal and scalar
variables. Notably some of the scalar variables could be categorical variables
as well. The Semi-VCAM simultaneously allows for both substantial flexibility
and the maintaining of one-dimensional rates of convergence. A local linear
smoothing with the aid of an initial B spline series approximation is developed
to estimate the unknown functional effects in the model. To avoid the
subjective choice between the sparse and dense cases of the data, we establish
the asymptotic theories of the resultant Pilot Estimation Based Local Linear
Estimators (PEBLLE) on a unified framework of sparse, dense and ultra-dense
cases of the data. Moreover, we construct unified consistent tests to justify
whether a parsimony submodel is sufficient or not. These test methods also
avoid the subjective choice between the sparse, dense and ultra dense cases of
the data. Extensive Monte Carlo simulation studies investigating the finite
sample performance of the proposed methodologies confirm our asymptotic
results. We further illustrate our methodologies via analyzing the COVID-19
data from China and the CD4 data.
"
"stat.TH","  Given two samples from possibly different discrete distributions over a
common set of size $N$, consider the problem of testing whether these
distributions are identical, vs. the following rare/weak perturbation
alternative: the frequencies of $N^{1-\beta}$ elements are perturbed by $r(\log
N)/2n$ in the Hellinger distance, where $n$ is the size of each sample. We
adapt the Higher Criticism (HC) test to this setting using P-values obtained
from $N$ exact binomial tests. We characterize the asymptotic performance of
the HC-based test in terms of the sparsity parameter $\beta$ and the
perturbation intensity parameter $r$. Specifically, we derive a region in the
$(\beta,r)$-plane where the test asymptotically has maximal power, while having
asymptotically no power outside this region. Our analysis distinguishes between
the cases of dense ($N\gg n$) and sparse ($N\ll n$) contingency tables. In the
dense case, the phase transition curve matches that of an analogous two-sample
normal means model.
"
"stat.TH","  Axially symmetric processes on spheres, for which the second-order dependency
structure may substantially vary with shifts in latitude, are a prominent
alternative to model the spatial uncertainty of natural variables located over
large portions of the Earth. In this paper, we focus on Karhunen-Lo\`eve
expansions of axially symmetric Gaussian processes. First, we investigate a
parametric family of Karhunen-Lo\`eve coefficients that allows for versatile
spatial covariance functions. The isotropy as well as the longitudinal
independence can be obtained as limit cases of our proposal. Second, we
introduce a strategy to render any longitudinally reversible process
irreversible, which means that its covariance function could admit certain
types of asymmetries along longitudes. Then, finitely truncated
Karhunen-Lo\`eve expansions are used to approximate axially symmetric
processes. For such approximations, bounds for the $L^2$-error are provided.
Numerical experiments are conducted to illustrate our findings.
"
"stat.TH","  The James-Stein estimator is an estimator of the multivariate normal mean and
dominates the maximum likelihood estimator (MLE) under squared error loss. The
original work inspired great interest in developing shrinkage estimators for a
variety of problems. Nonetheless, research on shrinkage estimation for
manifold-valued data is scarce. In this paper, we propose shrinkage estimators
for the parameters of the Log-Normal distribution defined on the manifold of $N
\times N$ symmetric positive-definite matrices. For this manifold, we choose
the Log-Euclidean metric as its Riemannian metric since it is easy to compute
and is widely used in applications. By using the Log-Euclidean distance in the
loss function, we derive a shrinkage estimator in an analytic form and show
that it is asymptotically optimal within a large class of estimators including
the MLE, which is the sample Fr\'echet mean of the data. We demonstrate the
performance of the proposed shrinkage estimator via several simulated data
experiments. Furthermore, we apply the shrinkage estimator to perform
statistical inference in diffusion magnetic resonance imaging problems.
"
"stat.TH","  Both observed and unobserved vertex heterogeneity can influence block
structure in graphs. To assess these effects on block recovery, we present a
comparative analysis of two model-based spectral algorithms for clustering
vertices in stochastic blockmodel graphs with vertex covariates. The first
algorithm directly estimates the induced block assignments by investigating the
estimated block connectivity probability matrix including the vertex covariate
effect. The second algorithm estimates the vertex covariate effect and then
estimates the induced block assignments after accounting for this effect. We
employ Chernoff information to analytically compare the algorithms' performance
and derive the Chernoff ratio formula for some special models of interest.
Analytic results and simulations suggest that, in general, the second algorithm
is preferred: we can better estimate the induced block assignments by first
estimating the vertex covariate effect. In addition, real data experiments on a
diffusion MRI connectome data set indicate that the second algorithm has the
advantages of revealing underlying block structure and taking observed vertex
heterogeneity into account in real applications. Our findings emphasize the
importance of distinguishing between observed and unobserved factors that can
affect block structure in graphs.
"
"stat.TH","  Rank correlations have found many innovative applications in the last decade.
In particular, suitable versions of rank correlations have been used for
consistent tests of independence between pairs of random variables. The use of
ranks is especially appealing for continuous data as tests become
distribution-free. However, the traditional concept of ranks relies on ordering
data and is, thus, tied to univariate observations. As a result it has long
remained unclear how one may construct distribution-free yet consistent tests
of independence between multivariate random vectors. This is the problem we
address in this paper, in which we lay out a general framework for designing
dependence measures that give tests of multivariate independence that are not
only consistent and distribution-free but which we also prove to be
statistically efficient. Our framework leverages the recently introduced
concept of center-outward ranks and signs, a multivariate generalization of
traditional ranks, and adopts a common standard form for dependence measures
that encompasses many popular measures from the literature. In a unified study,
we derive a general asymptotic representation of center-outward test statistics
under independence, extending to the multivariate setting the classical Hajek
asymptotic representation results. This representation permits a direct
calculation of limiting null distributions for the proposed test statistics.
Moreover, it facilitates a local power analysis that provides strong support
for the center-outward approach to multivariate ranks by establishing, for the
first time, the rate-optimality of center-outward tests within families of
Konijn alternatives.
"
"stat.TH","  We are interested to detect periodic signals in Hilbert space valued time
series when the length of the period is unknown. A natural test statistic is
the maximum Hilbert-Schmidt norm of the periodogram operator over all
fundamental frequencies. In this paper we analyze the asymptotic distribution
of this test statistic. We consider the case where the noise variables are
independent and then generalize our results to functional linear processes.
Details for implementing the test are provided for the class of functional
autoregressive processes. We illustrate the usefulness of our approach by
examining air quality data from Graz, Austria. The accuracy of the asymptotic
theory in finite samples is evaluated in a simulation experiment.
"
"stat.TH","  The concept of joint bivariate signature, introduced by Navarro et al.
(2013), is a useful tool for quantifying the reliability of two systems with
shared components. As with the univariate system signature, introduced by
Samaniego (2007), its applications are limited to systems with only one type of
components, which restricts its practical use. Coolen and Coolen-Maturi (2012)
introduced the survival signature, which generalizes Samaniego's signature and
can be used for systems with multiple types of components. This paper
introduces a joint survival signature for multiple systems with multiple types
of components and with some components shared between systems. A particularly
important feature is that the functioning of these systems can be considered at
different times, enabling computation of relevant conditional probabilities
with regard to a system's functioning conditional on the status of another
system with which it shares components. Several opportunities for practical
application and related challenges for further development of the presented
concept are briefly discussed, setting out an important direction for future
research.
"
"stat.TH","  Modern genomic studies are increasingly focused on identifying more and more
genes clinically associated with a health response. Commonly used Bayesian
shrinkage priors are designed primarily to detect only a handful of signals
when the dimension of the predictors is very high. In this article, we
investigate the performance of a popular continuous shrinkage prior in the
presence of relatively large number of true signals. We draw attention to an
undesirable phenomenon; the posterior mean is rendered very close to a null
vector, caused by a sharp underestimation of the global-scale parameter. The
phenomenon is triggered by the absence of a tail-index controlling mechanism in
the Bayesian shrinkage priors. We provide a remedy by developing a
global-local-tail shrinkage prior which can automatically learn the tail-index
and can provide accurate inference even in the presence of moderately large
number of signals. The collapsing behavior of the Horseshoe with its remedy is
exemplified in numerical examples and in two gene expression datasets.
"
"stat.TH","  Modelling and analysis of competing risks data with long-term survivors is an
important area of research in recent years. For example, in the study of cancer
patients treated for soft tissue sarcoma, patient may die due to different
causes. Considerable portion of the patients may remain cancer free after the
treatment. Accordingly, it is important to incorporate long-term survivors in
the analysis of competing risks data. Motivated by this, we propose a new
method for the analysis of competing risks data with long term survivors. The
new method enables us to estimate the overall survival probability without
estimating the cure fraction. We formulate the effects of covariates on
sub-distribution (cumulative incidence) functions using linear transformation
model. Estimating equations based on counting process are developed to find the
estimators of regression coefficients. The asymptotic properties of the
estimators are studied using martingale theory. An extensive Monte Carlo
simulation study is carried out to assess the finite sample performance of the
proposed estimators. Finally, we illustrate our method using a real data set.
"
"stat.TH","  Maximal ancestral graphs (MAGs) have many desirable properties; in particular
they can fully describe conditional independences from directed acyclic graphs
(DAGs) in the presence of latent and selection variables. However, different
MAGs may encode the same conditional independences, and are said to be
\emph{Markov equivalent}. Thus identifying necessary and sufficient conditions
for equivalence is essential for structure learning. Several criteria for this
already exist, but in this paper we give a new non-parametric characterization
in terms of the heads and tails that arise in the parameterization for discrete
models. We also provide a polynomial time algorithm ($O(ne^{2})$, where $n$ and
$e$ are the number of vertices and edges respectively) to verify equivalence.
Moreover, we extend our criterion to ADMGs and summary graphs and propose an
algorithm that converts an ADMG or summary graph to an equivalent MAG in
polynomial time ($O(n^{2}e)$). Hence by combining both algorithms, we can also
verify equivalence between two summary graphs or ADMGs.
"
"stat.TH","  We study the problem of estimating the parameters of a Boolean product
distribution in $d$ dimensions, when the samples are truncated by a set $S
\subset \{0, 1\}^d$ accessible through a membership oracle. This is the first
time that the computational and statistical complexity of learning from
truncated samples is considered in a discrete setting.
  We introduce a natural notion of fatness of the truncation set $S$, under
which truncated samples reveal enough information about the true distribution.
We show that if the truncation set is sufficiently fat, samples from the true
distribution can be generated from truncated samples. A stunning consequence is
that virtually any statistical task (e.g., learning in total variation
distance, parameter estimation, uniformity or identity testing) that can be
performed efficiently for Boolean product distributions, can also be performed
from truncated samples, with a small increase in sample complexity. We
generalize our approach to ranking distributions over $d$ alternatives, where
we show how fatness implies efficient parameter estimation of Mallows models
from truncated samples.
  Exploring the limits of learning discrete models from truncated samples, we
identify three natural conditions that are necessary for efficient
identifiability: (i) the truncation set $S$ should be rich enough; (ii) $S$
should be accessible through membership queries; and (iii) the truncation by
$S$ should leave enough randomness in all directions. By carefully adapting the
Stochastic Gradient Descent approach of (Daskalakis et al., FOCS 2018), we show
that these conditions are also sufficient for efficient learning of truncated
Boolean product distributions.
"
"stat.TH","  Popular guidance on observational data analysis states that outcomes should
be blinded when determining matching criteria or propensity scores. Such a
blinding is informally said to maintain the ""objectivity"" of the analysis
(Rubin et al., 2008). To explore these issues, we begin by proposing a
definition of objectivity based on the worst-case bias that can occur without
blinding, which we call ""added variable bias."" This bias is indeed severe, and
can diverge towards infinity as the sample size grows. However, we also show
that bias of the same order of magnitude can occur even if the outcomes are
blinded, so long as some prior knowledge is available that links covariates to
outcomes. Finally, we outline an alternative sample partitioning procedure for
estimating the average treatment effect on the controls, or the average
treatment effect on the treated, while avoiding added variable bias. This
procedure allows for the analysis to not be fully prespecified; uses all of the
the outcome data from all partitions in the final analysis step; and does not
require blinding. Together, these results illustrate that outcome blinding is
neither necessary nor sufficient for preventing added variable bias, and should
not be considered a requirement when evaluating novel causal inference methods.
"
"stat.TH","  We study a novel class of affine invariant and consistent tests for
multivariate normality. The tests are based on a characterization of the
standard $d$-variate normal distribution by means of the unique solution of an
initial value problem connected to a partial differential equation, which is
motivated by a multivariate Stein equation. The test criterion is a suitably
weighted $L^2$-statistic. We derive the limit distribution of the test
statistic under the null hypothesis as well as under contiguous and fixed
alternatives to normality. A consistent estimator of the limiting variance
under fixed alternatives as well as an asymptotic confidence interval of the
distance of an underlying alternative with respect to the multivariate normal
law is derived. In simulation studies, we show that the tests are strong in
comparison with prominent competitors, and that the empirical coverage rate of
the asymptotic confidence interval converges to the nominal level. We present a
real data example, and we outline topics for further research.
"
"stat.TH","  One fundamental problem when solving inverse problems is how to find
regularization parameters. This article considers solving this problem using
data-driven bilevel optimization, i.e. we consider the adaptive learning of the
regularization parameter from data by means of optimization. This approach can
be interpreted as solving an empirical risk minimization problem, and we
analyze its performance in the large data sample size limit for general
nonlinear problems. To reduce the associated computational cost, online
numerical schemes are derived using the stochastic gradient method. We prove
convergence of these numerical schemes under suitable assumptions on the
forward problem. Numerical experiments are presented illustrating the
theoretical results and demonstrating the applicability and efficiency of the
proposed approaches for various linear and nonlinear inverse problems,
including Darcy flow, the eikonal equation, and an image denoising example.
"
"stat.TH","  The problem of inferring the direct causal parents of a response variable
among a large set of explanatory variables is of high practical importance in
many disciplines. Recent work in the field of causal discovery exploits
invariance properties of models across different experimental conditions for
detecting direct causal links. However, these approaches generally do not scale
well with the number of explanatory variables, are difficult to extend to
nonlinear relationships, and require data across different experiments.
Inspired by {\em Debiased} machine learning methods, we study a
one-vs.-the-rest feature selection approach to discover the direct causal
parent of the response. We propose an algorithm that works for purely
observational data, while also offering theoretical guarantees, including the
case of partially nonlinear relationships. Requiring only one estimation for
each variable, we can apply our approach even to large graphs, demonstrating
significant improvements compared to established approaches.
"
"stat.TH","  Model complexity plays an essential role in its selection, namely, by
choosing a model that fits the data and is also succinct. Two-part codes and
the minimum description length have been successful in delivering procedures to
single out the best models, avoiding overfitting. In this work, we pursue this
approach and complement it by performing further assumptions in the parameter
space. Concretely, we assume that the parameter space is a smooth manifold, and
by using tools of Riemannian geometry, we derive a sharper expression than the
standard one given by the stochastic complexity, where the scalar curvature of
the Fisher information metric plays a dominant role. Furthermore, we derive the
minmax regret for general statistical manifolds and apply our results to derive
optimal dimensional reduction in the context of principal component analysis.
"
"stat.TH","  We analyze the finite sample mean squared error (MSE) performance of
regression trees and forests in the high dimensional regime with binary
features, under a sparsity constraint. We prove that if only $r$ of the $d$
features are relevant for the mean outcome function, then shallow trees built
greedily via the CART empirical MSE criterion achieve MSE rates that depend
only logarithmically on the ambient dimension $d$. We prove upper bounds, whose
exact dependence on the number relevant variables $r$ depends on the
correlation among the features and on the degree of relevance. For strongly
relevant features, we also show that fully grown honest forests achieve fast
MSE rates and their predictions are also asymptotically normal, enabling
asymptotically valid inference that adapts to the sparsity of the regression
function.
"
"stat.TH","  When neural network's parameters are initialized as i.i.d., neural networks
exhibit undesirable forward and backward properties as the number of layers
increases, e.g., vanishing dependency on the input, and perfectly correlated
outputs for any two inputs. To overcome these drawbacks Peluchetti and Favaro
(2020) considered fully connected residual networks (ResNets) with parameters'
distributions that shrink as the number of layers increases. In particular,
they established an interplay between infinitely deep ResNets and solutions to
stochastic differential equations, i.e. diffusion processes, showing that
infinitely deep ResNets does not suffer from undesirable forward properties. In
this paper, we review the forward-propagation results of Peluchetti and Favaro
(2020), extending them to the setting of convolutional ResNets. Then, we study
analogous backward-propagation results, which directly relate to the problem of
training deep ResNets. Finally, we extend our study to the doubly infinite
regime where both network's width and depth grow unboundedly. Within this novel
regime the dynamics of quantities of interest converge, at initialization, to
deterministic limits. This allow us to provide analytical expressions for
inference, both in the case of weakly trained and fully trained networks. These
results point to a limited expressive power of doubly infinite ResNets when the
unscaled parameters are i.i.d, and residual blocks are shallow.
"
"stat.TH","  We show how convergence to the Gumbel distribution in an extreme value
setting can be understood in an information-theoretic sense. We introduce a new
type of score function which behaves well under the maximum operation, and
which implies simple expressions for entropy and relative entropy. We show
that, assuming certain properties of the von Mises representation, convergence
to the Gumbel can be proved in the strong sense of relative entropy.
"
"stat.TH","  In this paper, a new natural discrete version of the one parameter polynomial
exponential family of distributions have been proposed and studied. The
distribution is named as Natural Discrete One Parameter Polynomial Exponential
(NDOPPE) distribution. Structural and reliability properties have been studied.
Estimation procedure of the parameter of the distribution have been mentioned.
Compound NDOPPE distribution in the context of collective risk model have been
obtained in closed form. The new compound distribution has been compared with
the classical compound Poisson, compound Negative binomial, compound discrete
Lindley, compound xgamma-I and compound xgamma-II distributions regarding
suitability of modelling extreme data with the help of some automobile claim.
"
"stat.TH","  Gradient descent yields zero training loss in polynomial time for deep neural
networks despite non-convex nature of the objective function. The behavior of
network in the infinite width limit trained by gradient descent can be
described by the Neural Tangent Kernel (NTK) introduced in
\cite{Jacot2018Neural}. In this paper, we study dynamics of the NTK for finite
width Deep Residual Network (ResNet) using the neural tangent hierarchy (NTH)
proposed in \cite{Huang2019Dynamics}. For a ResNet with smooth and Lipschitz
activation function, we reduce the requirement on the layer width $m$ with
respect to the number of training samples $n$ from quartic to cubic. Our
analysis suggests strongly that the particular skip-connection structure of
ResNet is the main reason for its triumph over fully-connected network.
"
"stat.TH","  We consider a testing problem for cross-sectional dependence for
high-dimensional panel data, where the number of cross-sectional units is
potentially much larger than the number of observations. The cross-sectional
dependence is described through a linear regression model. We study three tests
named the sum test, the max test and the max-sum test, where the latter two are
new. The sum test is initially proposed by Breusch and Pagan (1980). We design
the max and sum tests for sparse and non-sparse residuals in the linear
regressions, respectively.And the max-sum test is devised to compromise both
situations on the residuals. Indeed, our simulation shows that the max-sum test
outperforms the previous two tests. This makes the max-sum test very useful in
practice where sparsity or not for a set of data is usually vague. Towards the
theoretical analysis of the three tests, we have settled two conjectures
regarding the sum of squares of sample correlation coefficients asked by
Pesaran (2004 and 2008). In addition, we establish the asymptotic theory for
maxima of sample correlations coefficients appeared in the linear regression
model for panel data, which is also the first successful attempt to our
knowledge. To study the max-sum test, we create a novel method to show
asymptotic independence between maxima and sums of dependent random variables.
We expect the method itself is useful for other problems of this nature.
Finally, an extensive simulation study as well as a case study are carried out.
They demonstrate advantages of our proposed methods in terms of both empirical
powers and robustness for residuals regardless of sparsity or not.
"
"stat.TH","  Linear models have shown great effectiveness and flexibility in many fields
such as machine learning, signal processing and statistics. They can represent
rich spaces of functions while preserving the convexity of the optimization
problems where they are used, and are simple to evaluate, differentiate and
integrate. However, for modeling non-negative functions, which are crucial for
unsupervised learning, density estimation, or non-parametric Bayesian methods,
linear models are not applicable directly. Moreover, current state-of-the-art
models like generalized linear models either lead to non-convex optimization
problems, or cannot be easily integrated. In this paper we provide the first
model for non-negative functions which benefits from the same good properties
of linear models. In particular, we prove that it admits a representer theorem
and provide an efficient dual formulation for convex problems. We study its
representation power, showing that the resulting space of functions is strictly
richer than that of generalized linear models. Finally we extend the model and
the theoretical results to functions with outputs in convex cones. The paper is
complemented by an experimental evaluation of the model showing its
effectiveness in terms of formulation, algorithmic derivation and practical
results on the problems of density estimation, regression with heteroscedastic
errors, and multiple quantile regression.
"
"stat.TH","  Statisticians have warned us since the early days of their discipline that
experimental correlation between two observations by no means implies the
existence of a causal relation. The question about what clues exist in
observational data that could informs us about the existence of such causal
relations is nevertheless more that legitimate. It lies actually at the root of
any scientific endeavor. For decades however the only accepted method among
statisticians to elucidate causal relationships was the so called Randomized
Controlled Trial. Besides this notorious exception causality questions remained
largely taboo for many. One reason for this state of affairs was the lack of an
appropriate mathematical framework to formulate such questions in an
unambiguous way. Fortunately thinks have changed these last years with the
advent of the so called Causality Revolution initiated by Judea Pearl and
coworkers. The aim of this pedagogical paper is to present their ideas and
methods in a compact and self-contained fashion with concrete business examples
as illustrations.
"
"stat.TH","  The risk of occurrence of atypical phenomena is a cross-cutting concern in
several areas, such as engineering, climatology, finance, actuarial, among
others. Extreme value theory is the natural tool to approach this theme. Many
of these random phenomena carry variables defined in time and space, usually
modeled through random fields. Thus, the study of random fields in the context
of extreme values becomes imperative and has been developed especially in the
last decade. In this work, we propose a new random field, called pMAX, designed
for modeling extremes. We analyze its dependence and pre-asymptotic dependence
structure through the corresponding bivariate tail dependence coefficients.
Estimators for the model parameters are obtained and their finite sample
properties analyzed. Examples with simulations illustrate the results.
"
"stat.TH","  Standard approaches to stochastic gradient estimation, with only noisy
black-box function evaluations, use the finite-difference method or its
variants. While natural, it is open to our knowledge whether their statistical
accuracy is the best possible. This paper argues so by showing that central
finite-difference is a nearly minimax optimal zeroth-order gradient estimator
for a suitable class of objective functions and mean squared risk, among both
the class of linear estimators and the much larger class of all (nonlinear)
estimators.
"
"stat.TH","  Scientists and engineers are often interested in learning the number of
subpopulations (or components) present in a data set. A common suggestion is to
use a finite mixture model (FMM) with a prior on the number of components. Past
work has shown the resulting FMM component-count posterior is consistent; that
is, the posterior concentrates on the true generating number of components. But
existing results crucially depend on the assumption that the component
likelihoods are perfectly specified. In practice, this assumption is
unrealistic, and empirical evidence suggests that the FMM posterior on the
number of components is sensitive to the likelihood choice. In this paper, we
add rigor to data-analysis folk wisdom by proving that under even the slightest
model misspecification, the FMM component-count posterior diverges: the
posterior probability of any particular finite number of latent components
converges to 0 in the limit of infinite data. We illustrate practical
consequences of our theory on simulated and real data sets.
"
"stat.TH","  We prove an exponential decay concentration inequality to bound the tail
probability of the difference between the log-likelihood of discrete random
variables and the negative entropy. The concentration bound we derive holds
uniformly over all parameter values. The new result improves the convergence
rate in an earlier result of Zhao (2020), from $(K^2\log K)/n=o(1)$ to $ (\log
K)^2/n=o(1)$, where $n$ is the sample size and $K$ is the number of possible
values of the discrete variable. We further prove that the rate $(\log
K)^2/n=o(1)$ is optimal. The results are extended to misspecified
log-likelihoods for grouped random variables.
"
"stat.TH","  We introduce in this work a new method for online approximate Bayesian
learning, whose main idea is to approximate the sequence $(\pi_t)_{t\geq 1}$ of
posterior distributions by a sequence $(\tilde{\pi}_t)_{t\geq 1}$ which (i) can
be estimated in an online fashion using sequential Monte Carlo methods and (ii)
is shown to converge to the same distribution as the sequence $(\pi_t)_{t\geq
1}$, under weak assumptions on the statistical model at hand. In its simplest
version, the proposed approach amounts to take for $(\tilde{\pi}_t)_{t\geq 1}$
the sequence of filtering distributions associated to a particular state-space
model, and to approximate this sequence using a standard particle filter
algorithm. We illustrate on several challenging examples the benefits of this
procedure for online approximate Bayesian parameter inference, and with one
real data example we show that its online predictive performance can
significantly outperform that of stochastic gradient descent and of streaming
variational Bayes.
"
"stat.TH","  Using differential geometry, I derive a form of the Bayesian Cram\'er-Rao
bound that remains invariant under reparametrization. With the invariant
formulation at hand, I find the optimal and naturally invariant bound among the
Gill-Levit family of bounds. By assuming that the prior probability density is
the square of a wavefunction, I also express the bounds in terms of functionals
that are quadratic with respect to the wavefunction and its gradient. The
problem of finding an unfavorable prior to tighten the bound for minimax
estimation is shown, in a special case, to be equivalent to finding the ground
state of a Schr\""odinger equation, with the Fisher information playing the role
of the potential. To illustrate the theory, two quantum estimation problems,
namely, optomechanical waveform estimation and subdiffraction incoherent
optical imaging, are discussed.
"
"stat.TH","  In this paper, we investigate the age-limited capacity of the Gaussian many
channel with total $N$ users, out of which a random subset of $K_{a}$ users are
active in any transmission period and a large-scale antenna array at the base
station (BS). Motivated by IoT applications and promises of the massive MIMO
technology, we consider the setting in which both the number of users, $N$, and
the number of antennas at the BS, $M$, are allowed to grow large at a fixed
ratio $\zeta = \frac{M}{N}$. Assuming perfect channel state information (CSI)
at the receiver, we derive the achievability bound under maximal ratio
combining. As the number of active users, $K_{a}$, increases, the achievable
spectral efficiency is found to increase monotonically to a limit
$\log_2\left(1+\frac{M}{K_{a}}\right)$. Using the age of information (AoI)
metric, first coined in \cite{kaul2011minimizing}, as our measure of data
timeliness/freshness, we investigate the trade-offs between the AoI and
spectral efficiency in the context massive connectivity with large-scale
receiving antenna arrays. Based on our large system analysis, we provide an
accurate characterization of the asymptotic spectral efficiency as a function
of the number of antennas/users, the attempt probability, and the AoI. It is
found that while the spectral efficiency can be made large, the penalty is an
increase in the minimum AoI obtainable. The proposed achievability bound is
further compared against recent massive MIMO-based massive unsourced random
access (URA) schemes.
"
"stat.TH","  The significance of Marshall-Olkin distribution in reliability theory has
motivated us to introduce a generalized exponentiated Marshall-Olkin (GEMO), a
family of distributions.
"
"stat.TH","  This paper proposes a tool for dimension reduction where the dimension of the
original space is reduced: a Principal Loading Analysis (PLA). PLA is a tool to
reduce dimensions by discarding variables. The intuition is that variables are
dropped which distort the covariance matrix only by a little. Our method is
introduced and an algorithm for conducting PLA is provided. Further, we give
bounds for the noise arising in the sample case.
"
"stat.TH","  Monte Carlo (MC) dropout is one of the state-of-the-art approaches for
uncertainty estimation in neural networks (NNs). It has been interpreted as
approximately performing Bayesian inference. Based on previous work on the
approximation of Gaussian processes by wide and deep neural networks with
random weights, we study the limiting distribution of wide untrained NNs under
dropout more rigorously and prove that they as well converge to Gaussian
processes for fixed sets of weights and biases. We sketch an argument that this
property might also hold for infinitely wide feed-forward networks that are
trained with (full-batch) gradient descent. The theory is contrasted by an
empirical analysis in which we find correlations and non-Gaussian behaviour for
the pre-activations of finite width NNs. We therefore investigate how
(strongly) correlated pre-activations can induce non-Gaussian behavior in NNs
with strongly correlated weights.
"
"stat.TH","  The efficiency of a markov sampler based on the underdamped Langevin
diffusion is studied for high dimensionial targets with convex and smooth
potentials. We consider a classical second-order integrator which requires only
one gradient computation per iteration. Contrary to previous works on similar
samplers, a dimension-free contraction of Wasserstein distances and convergence
rate for the total variance distance are proved for the discrete time chain
itself. Non-asymptotic Wasserstein and total variation efficiency bounds and
concentration inequalities are obtained for both the Metropolis adjusted and
unadjusted chains. In terms of the dimension $d$ and the desired accuracy
$\varepsilon$, the Wasserstein efficiency bounds are of order $\sqrt d /
\varepsilon$ in the general case, $\sqrt{d/\varepsilon}$ if the Hessian of the
potential is Lipschitz, and $d^{1/4}/\sqrt\varepsilon$ in the case of a
separable target, in accordance with known results for other kinetic Langevin
or HMC schemes.
"
"stat.TH","  The evaluation of hyperparameters, neural architectures, or data augmentation
policies becomes a critical model selection problem in advanced deep learning
with a large hyperparameter search space. In this paper, we propose an
efficient and robust bandit-based algorithm called Sub-Sampling (SS) in the
scenario of hyperparameter search evaluation. It evaluates the potential of
hyperparameters by the sub-samples of observations and is theoretically proved
to be optimal under the criterion of cumulative regret. We further combine SS
with Bayesian Optimization and develop a novel hyperparameter optimization
algorithm called BOSS. Empirical studies validate our theoretical arguments of
SS and demonstrate the superior performance of BOSS on a number of
applications, including Neural Architecture Search (NAS), Data Augmentation
(DA), Object Detection (OD), and Reinforcement Learning (RL).
"
"stat.TH","  We consider different types of predictive intervals and ask whether they are
elicitable, i.e. are unique minimizers of a loss or scoring function in
expectation. The equal-tailed interval is elicitable, with a rich class of
suitable loss functions, though subject to translation invariance, or positive
homogeneity and differentiability, the Winkler interval score becomes a unique
choice. The modal interval also is elicitable, with a sole consistent scoring
function, up to equivalence. However, the shortest interval fails to be
elicitable relative to practically relevant classes of distributions. These
results provide guidance in interval forecast evaluation and support recent
choices of performance measures in forecast competitions.
"
"stat.TH","  We provide a framework for empirical process theory of locally stationary
processes using the functional dependence measure. Our results extend known
results for stationary mixing sequences by another common possibility to
measure dependence and allow for additional time dependence. We develop maximal
inequalities for expectations and provide functional limit theorems and
Bernstein-type inequalities. We show their applicability to a variety of
situations, for instance we prove the weak functional convergence of the
empirical distribution function and uniform convergence rates for kernel
density and regression estimation if the observations are locally stationary
processes.
"
"stat.TH","  Given two arbitrary sequences of denoisers for block lengths tending to
infinity we ask if it is possible to construct a third sequence of denoisers
with an asymptotically vanishing (in block length) excess expected loss
relative to the best expected loss of the two given denoisers for all clean
channel input sequences. As in the setting of DUDE [1], which solves this
problem when the given denoisers are sliding block denoisers, the construction
is allowed to depend on the two given denoisers and the channel transition
probabilities. We show that under certain restrictions on the two given
denoisers the problem can be solved using a straightforward application of a
known loss estimation paradigm. We then show by way of a counter-example that
the loss estimation approach fails in the general case. Finally, we show that
for the binary symmetric channel, combining the loss estimation with a
randomization step leads to a solution to the stated problem under no
restrictions on the given denoisers.
"
"stat.TH","  We resolve one of the major outstanding problems in robust statistics. In
particular, if $X$ is an evenly weighted mixture of two arbitrary
$d$-dimensional Gaussians, we devise a polynomial time algorithm that given
access to samples from $X$ an $\eps$-fraction of which have been adversarially
corrupted, learns $X$ to error $\poly(\eps)$ in total variation distance.
"
"stat.TH","  We state an exact simulation scheme for the first passage time of a Brownian
motion to a symmetric linear boundary.
"
"stat.TH","  Marcinkiewicz strong law of large numbers, ${n^{-\frac1p}}\sum_{k=1}^{n}
(d_{k}- d)\rightarrow 0\ $ almost surely with $p\in(1,2)$, are developed for
products $d_k=\prod_{r=1}^s x_k^{(r)}$, where the $x_k^{(r)} =
\sum_{l=-\infty}^{\infty}c_{k-l}^{(r)}\xi_l^{(r)}$ are two-sided linear process
with coefficients $\{c_l^{(r)}\}_{l\in \mathbb{Z}}$ and i.i.d. zero-mean
innovations $\{\xi_l^{(r)}\}_{l\in \mathbb{Z}}$. The decay of the coefficients
$c_l^{(r)}$ as $|l|\to\infty$, can be slow enough for $\{x_k^{(r)}\}$ to have
long memory while $\{d_k\}$ can have heavy tails. The long-range dependence and
heavy tails for $\{d_k\}$ are handled simultaneously and a decoupling property
shows the convergence rate is dictated by the worst of long-range dependence
and heavy tails, but not their combination. The results provide a means to
estimate how much (if any) long-range dependence and heavy tails a sequential
data set possesses, which is done for real financial data. All of the stocks we
considered had some degree of heavy tails. The majority also had long-range
dependence. The Marcinkiewicz strong law of large numbers is also extended to
the multivariate linear process case.
"
"stat.TH","  Sparse estimation methods capable of tolerating outliers have been broadly
investigated in the last decade. We contribute to this research considering
high-dimensional regression problems contaminated by multiple mean-shift
outliers which affect both the response and the design matrix. We develop a
general framework for this class of problems and propose the use of
mixed-integer programming to simultaneously perform feature selection and
outlier detection with provably optimal guarantees. We characterize the
theoretical properties of our approach, i.e. a necessary and sufficient
condition for the robustly strong oracle property, which allows the number of
features to exponentially increase with the sample size; the optimal estimation
of the parameters; and the breakdown point of the resulting estimates.
Moreover, we provide computationally efficient procedures to tune integer
constraints and to warm-start the algorithm. We show the superior performance
of our proposal compared to existing heuristic methods through numerical
simulations and an application investigating the relationships between the
human microbiome and childhood obesity.
"
"stat.TH","  We propose a new simulation-based estimation method, adversarial estimation,
for structural models. The estimator is formulated as the solution to a minimax
problem between a generator (which generates synthetic observations using the
structural model) and a discriminator (which classifies if an observation is
synthetic). The discriminator maximizes the accuracy of its classification
while the generator minimizes it. We show that, with a sufficiently rich
discriminator, the adversarial estimator attains parametric efficiency under
correct specification and the parametric rate under misspecification. We
advocate the use of a neural network as a discriminator that can exploit
adaptivity properties and attain fast rates of convergence. We apply our method
to the elderly's saving decision model and show that including gender and
health profiles in the discriminator uncovers the bequest motive as an
important source of saving across the wealth distribution, not only for the
rich.
"
"stat.TH","  We initiate a program of average smoothness analysis for efficiently learning
real-valued functions on metric spaces. Rather than using the Lipschitz
constant as the regularizer, we define a local slope at each point and gauge
the function complexity as the average of these values. Since the mean can be
dramatically smaller than the maximum, this complexity measure can yield
considerably sharper generalization bounds -- assuming that these admit a
refinement where the Lipschitz constant is replaced by our average of local
slopes.
  Our first major contribution is to obtain just such distribution-sensitive
bounds. This required overcoming a number of technical challenges, perhaps the
most formidable of which was bounding the {\em empirical} covering numbers,
which can be much worse-behaved than the ambient ones. Our combinatorial
results are accompanied by efficient algorithms for smoothing the labels of the
random sample, as well as guarantees that the extension from the sample to the
whole space will continue to be, with high probability, smooth on average.
Along the way we discover a surprisingly rich combinatorial and analytic
structure in the function class we define.
"
"stat.TH","  This article studies the finite sample behaviour of a number of estimators
for the integrated power volatility process of a Brownian semistationary
process in the non semi-martingale setting. We establish three consistent
feasible estimators for the integrated volatility, two derived from parametric
methods and one non-parametrically. We then use a simulation study to compare
the convergence properties of the estimators to one another, and to a benchmark
of an infeasible estimator. We further establish bounds for the asymptotic
variance of the infeasible estimator and assess whether a central limit theorem
which holds for the infeasible estimator can be translated into a feasible
limit theorem for the non-parametric estimator.
"
"stat.TH","  We describe a general class of ie-merging functions and pose the problem of
finding ie-merging functions outside this class.
"
"stat.TH","  Given observations from a circular random variable contaminated by an
additive measurement error, we consider the problem of minimax optimal
goodness-of-fit testing in a non-asymptotic framework. We propose direct and
indirect testing procedures using a projection approach. The structure of the
optimal tests depends on regularity and ill-posedness parameters of the model,
which are unknown in practice. Therefore, adaptive testing strategies that
perform optimally over a wide range of regularity and ill-posedness classes
simultaneously are investigated. Considering a multiple testing procedure, we
obtain adaptive i.e. assumption-free procedures and analyse their performance.
Compared with the non-adaptive tests, their radii of testing face a
deterioration by a log-factor. We show that for testing of uniformity this loss
is unavoidable by providing a lower bound. The results are illustrated
considering Sobolev spaces and ordinary or super smooth error densities.
"
"stat.TH","  We provide a strong uniform consistency result with the convergence rate for
the kernel density estimation on Riemannian manifolds with Riemann integrable
kernels (in the ambient Euclidean space). We also provide a strong uniform
consistency result for the kernel density estimation on Riemannian manifolds
with Lebesgue integrable kernels. The kernels considered in this paper are
different from the kernels in the Vapnik-Chervonenkis class that are frequently
considered in statistics society. We illustrate the difference when we apply
them to estimate probability density function. We also provide the necessary
and sufficient condition for a kernel to be Riemann integrable on a submanifold
in the Euclidean space.
"
"stat.TH","  We consider species tree estimation under a standard stochastic model of gene
tree evolution that incorporates incomplete lineage sorting (as modeled by a
coalescent process) and gene duplication and loss (as modeled by a branching
process). Through a probabilistic analysis of the model, we derive sample
complexity bounds for widely used quartet-based inference methods that
highlight the effect of the duplication and loss rates in both subcritical and
supercritical regimes.
"
"stat.TH","  Variational algorithms have gained prominence over the past two decades as a
scalable computational environment for Bayesian inference. In this article, we
explore tools from the dynamical systems literature to study convergence of
coordinate ascent algorithms for mean field variational inference. Focusing on
the Ising model defined on two nodes, we fully characterize the dynamics of the
sequential coordinate ascent algorithm and its parallel version. We observe
that in the regime where the objective function is convex, both the algorithms
are stable and exhibit convergence to the unique fixed point. Our analyses
reveal interesting {\em discordances} between these two versions of the
algorithm in the region when the objective function is non-convex. In fact, the
parallel version exhibits a periodic oscillatory behavior which is absent in
the sequential version. Drawing intuition from the Markov chain Monte Carlo
literature, we {\em empirically} show that a parameter expansion of the Ising
model, popularly called as the Edward--Sokal coupling, leads to an enlargement
of the regime of convergence to the global optima.
"
"stat.TH","  In this paper, the open problem of finding a closed analytical expression for
the distribution function of the length of the longest pure head run in coin
tosses of a possibly biased coin is solved by studying the closely related
Markov chain of current head runs.
"
"stat.TH","  Motivated by decentralized approaches to machine learning, we propose a
collaborative Bayesian learning algorithm taking the form of decentralized
Langevin dynamics in a non-convex setting. Our analysis show that the initial
KL-divergence between the Markov Chain and the target posterior distribution is
exponentially decreasing while the error contributions to the overall
KL-divergence from the additive noise is decreasing in polynomial time. We
further show that the polynomial-term experiences speed-up with number of
agents and provide sufficient conditions on the time-varying step-sizes to
guarantee convergence to the desired distribution. The performance of the
proposed algorithm is evaluated on a wide variety of machine learning tasks.
The empirical results show that the performance of individual agents with
locally available data is on par with the centralized setting with considerable
improvement in the convergence rate.
"
"stat.TH","  In this paper, we study the problem of early stopping for iterative learning
algorithms in a reproducing kernel Hilbert space (RKHS) in the nonparametric
regression framework. In particular, we work with the gradient descent and
(iterative) kernel ridge regression algorithms. We present a data-driven rule
to perform early stopping without a validation set that is based on the
so-called minimum discrepancy principle. This method enjoys only one assumption
on the regression function: it belongs to a reproducing kernel Hilbert space
(RKHS). The proposed rule is proved to be minimax-optimal over different types
of kernel spaces, including finite-rank and Sobolev smoothness classes. The
proof is derived from the fixed-point analysis of the localized Rademacher
complexities, which is a standard technique for obtaining optimal rates in the
nonparametric regression literature. In addition to that, we present simulation
results on artificial datasets that show the comparable performance of the
designed rule with respect to other stopping rules such as the one determined
by V-fold cross-validation.
"
"stat.TH","  Standard inference about a scalar parameter estimated via GMM amounts to
applying a t-test to a particular set of observations. If the number of
observations is not very large, then moderately heavy tails can lead to poor
behavior of the t-test. This is a particular problem under clustering, since
the number of observations then corresponds to the number of clusters, and
heterogeneity in cluster sizes induces a form of heavy tails. This paper
combines extreme value theory for the smallest and largest observations with a
normal approximation for the average of the remaining observations to construct
a more robust alternative to the t-test. The new test is found to control size
much more successfully in small samples compared to existing methods.
Analytical results in the canonical inference for the mean problem demonstrate
that the new test provides a refinement over the full sample t-test under more
than two but less than three moments, while the bootstrapped t-test does not.
"
"stat.TH","  We study quantile trend filtering, a recently proposed method for
nonparametric quantile regression with the goal of generalizing existing risk
bounds known for the usual trend filtering estimators which perform mean
regression. We study both the penalized and the constrained version (of order
$r \geq 1$) of quantile trend filtering. Our results show that both the
constrained and the penalized version (of order $r \geq 1$) attain the minimax
rate up to log factors, when the $(r-1)$th discrete derivative of the true
vector of quantiles belongs to the class of bounded variation signals. We also
show that if the true vector of quantiles is a discrete spline with a few
polynomial pieces then the constrained version attains a near parametric rate
of convergence. Corresponding results for the usual trend filtering estimators
are known to hold only when the errors are sub-Gaussian. In contrast, our risk
bounds are shown to hold under minimal assumptions on the error variables. In
particular, no moment assumptions are needed and our results hold under
heavy-tailed errors. On the other hand, we prove all our results for a Huber
type loss which can be smaller than the mean squared error loss employed for
showing risk bounds for usual trend filtering. Our proof techniques are general
and thus can potentially be used to study other nonparametric quantile
regression methods. To illustrate this generality we also employ our proof
techniques to obtain new results for multivariate quantile total variation
denoising and high dimensional quantile linear regression.
"
"stat.TH","  In this paper, we formally define and analyze the class of noisy permutation
channels. The noisy permutation channel model constitutes a standard discrete
memoryless channel (DMC) followed by an independent random permutation that
reorders the output codeword of the DMC. While coding theoretic aspects of this
model have been studied extensively, particularly in the context of reliable
communication in network settings where packets undergo transpositions, and
closely related models of DNA based storage systems have also been analyzed
recently, we initiate an information theoretic study of this model by defining
an appropriate notion of noisy permutation channel capacity. Specifically, on
the achievability front, we prove a lower bound on the noisy permutation
channel capacity of any DMC in terms of the rank of the stochastic matrix of
the DMC. On the converse front, we establish two upper bounds on the noisy
permutation channel capacity of any DMC whose stochastic matrix is strictly
positive (entry-wise). Together, these bounds yield coding theorems that
characterize the noisy permutation channel capacities of every strictly
positive and ""full rank"" DMC, and our achievability proof yields a conceptually
simple, computationally efficient, and capacity achieving coding scheme for
such DMCs. Furthermore, we also demonstrate the relation between the output
degradation preorder over channels and noisy permutation channel capacity. In
fact, the proof of one of our converse bounds exploits a degradation result
that constructs a symmetric channel for any DMC such that the DMC is a degraded
version of the symmetric channel. Finally, we illustrate some examples such as
the special cases of binary symmetric channels and (general) erasure channels.
Somewhat surprisingly, our results suggest that noisy permutation channel
capacities are generally quite agnostic to the parameters that define the DMCs.
"
"stat.TH","  The first motivation of this paper is to study stationarity and ergodic
properties for a general class of time series models defined conditional on an
exogenous covariates process. The dynamic of these models is given by an
autoregressive latent process which forms a Markov chain in random
environments. Contrarily to existing contributions in the field of Markov
chains in random environments, the state space is not discrete and we do not
use small set type assumptions or uniform contraction conditions for the random
Markov kernels. Our assumptions are quite general and allows to deal with
models that are not fully contractive, such as threshold autoregressive
processes. Using a coupling approach, we study the existence of a limit, in
Wasserstein metric, for the backward iterations of the chain. We also derive
ergodic properties for the corresponding skew-product Markov chain. Our results
are illustrated with many examples of autoregressive processes widely used in
statistics or in econometrics, including GARCH type processes, count
autoregressions and categorical time series.
"
"stat.TH","  In this article, we propose a novel Bayesian multiple testing formulation for
model and variable selection in inverse setups, judiciously embedding the idea
of inverse reference distributions proposed by Bhattacharya (2013) in a mixture
framework consisting of the competing models. We develop the theory and methods
in the general context encompassing parametric and nonparametric competing
models, dependent data, as well as misspecifications. Our investigation shows
that asymptotically the multiple testing procedure almost surely selects the
best possible inverse model that minimizes the minimum Kullback-Leibler
divergence from the true model. We also show that the error rates, namely,
versions of the false discovery rate and the false non-discovery rate converge
to zero almost surely as the sample size goes to infinity. Asymptotic
{\alpha}-control of versions of the false discovery rate and its impact on the
convergence of false non-discovery rate versions, are also investigated.
  Our simulation experiments involve small sample based selection among inverse
Poisson log regression and inverse geometric logit and probit regression, where
the regressions are either linear or based on Gaussian processes. Additionally,
variable selection is also considered. Our multiple testing results turn out to
be very encouraging in the sense of selecting the best models in all the
non-misspecified and misspecified cases.
"
"stat.TH","  The principle of optimism in the face of uncertainty is one of the most
widely used and successful ideas in multi-armed bandits and reinforcement
learning. However, existing optimistic algorithms (primarily UCB and its
variants) are often unable to deal with large context spaces. Essentially all
existing well performing algorithms for general contextual bandit problems rely
on weighted action allocation schemes; and theoretical guarantees for
optimism-based algorithms are only known for restricted formulations. In this
paper we study general contextual bandits under the realizability condition,
and propose a simple generic principle to design optimistic algorithms, dubbed
""Upper Counterfactual Confidence Bounds"" (UCCB). We show that these algorithms
are provably optimal and efficient in the presence of large context spaces. Key
components of UCCB include: 1) a systematic analysis of confidence bounds in
policy space rather than in action space; and 2) the potential function
perspective that is used to express the power of optimism in the contextual
setting. We further show how the UCCB principle can be extended to infinite
action spaces, by constructing confidence bounds via the newly introduced
notion of ""counterfactual action divergence.""
"
"stat.TH","  Anomaly estimation, or the problem of finding a subset of a dataset that
differs from the rest of the dataset, is a classic problem in machine learning
and data mining. In both theoretical work and in applications, the anomaly is
assumed to have a specific structure defined by membership in an
$\textit{anomaly family}$. For example, in temporal data the anomaly family may
be time intervals, while in network data the anomaly family may be connected
subgraphs. The most prominent approach for anomaly estimation is to compute the
Maximum Likelihood Estimator (MLE) of the anomaly. However, it was recently
observed that for some anomaly families, the MLE is an asymptotically
$\textit{biased}$ estimator of the anomaly. Here, we demonstrate that the bias
of the MLE depends on the size of the anomaly family. We prove that if the
number of sets in the anomaly family that contain the anomaly is
sub-exponential, then the MLE is asymptotically unbiased. At the same time, we
provide empirical evidence that the converse is also true: if the number of
such sets is exponential, then the MLE is asymptotically biased. Our analysis
unifies a number of earlier results on the bias of the MLE for specific anomaly
families, including intervals, submatrices, and connected subgraphs. Next, we
derive a new anomaly estimator using a mixture model, and we empirically
demonstrate that our estimator is asymptotically unbiased regardless of the
size of the anomaly family. We illustrate the benefits of our estimator on both
simulated disease outbreak data and a real-world highway traffic dataset.
"
"stat.TH","  We study efficiency of non-parametric estimation of diffusions (stochastic
differential equations driven by Brownian motion) from long stationary
trajectories. First, we introduce estimators based on conditional expectation
which is motivated by the definition of drift and diffusion coefficients. These
estimators involve time- and space-discretization parameters for computing
expected values from discretely-sampled stationary data. Next, we analyze
consistency and mean squared error of these estimators depending on
computational parameters. We derive relationships between the number of
observational points, time- and space-discretization parameters in order to
achieve the optimal speed of convergence and minimize computational complexity.
We illustrate our approach with numerical simulations.
"
"stat.TH","  The aim of online monitoring is to issue an alarm as soon as there is
significant evidence in the collected observations to suggest that the
underlying data generating mechanism has changed. This work is concerned with
open-end, nonparametric procedures that can be interpreted as statistical
tests. The proposed monitoring schemes consist of computing the so-called
retrospective CUSUM statistic (or minor variations thereof) after the arrival
of each new observation. After proposing suitable threshold functions for the
chosen detectors, the asymptotic validity of the procedures is investigated in
the special case of monitoring for changes in the mean, both under the null
hypothesis of stationarity and relevant alternatives. To carry out the
sequential tests in practice, an approach based on an asymptotic regression
model is used to estimate high quantiles of relevant limiting distributions.
Monte Carlo experiments demonstrate the good finite-sample behavior of the
proposed monitoring schemes and suggest that they are superior to existing
competitors as long as changes do not occur at the very beginning of the
monitoring. Extensions to statistics exhibiting an asymptotic mean-like
behavior are briefly discussed. Finally, the application of the derived
sequential change-point detection tests is succinctly illustrated on
temperature anomaly data.
"
"stat.TH","  We study the problem of estimating the surface area of the boundary of a
sufficiently smooth set when the available information is only a set of points
(random or not) that becomes dense (with respect to Hausdorff distance) in the
set or the trajectory of a reflected diffusion. We obtain consistency results
in this general setup, and we derive rates of convergence for the iid case or
when the data corresponds to the trajectory of a reflected Brownian motion. We
propose an algorithm based on Crofton's formula, which estimates the number of
intersections of random lines with the boundary of the set by counting, in a
suitable way (given by the proposed algorithm), the number of intersections
with the boundary of two different estimators: the Devroye-Wise estimator and
the $\alpha$-convex hull of the data. As a by-product, our results also cover
the convex case, for any dimension.
"
"stat.TH","  This paper presents a unified framework for supervised learning and inference
procedures using the divide-and-conquer approach for high-dimensional
correlated outcomes. We propose a general class of estimators that can be
implemented in a fully distributed and parallelized computational scheme.
Modelling, computational and theoretical challenges related to high-dimensional
correlated outcomes are overcome by dividing data at both outcome and subject
levels, estimating the parameter of interest from blocks of data using a broad
class of supervised learning procedures, and combining block estimators in a
closed-form meta-estimator asymptotically equivalent to estimates obtained by
Hansen (1982)'s generalized method of moments (GMM) that does not require the
entire data to be reloaded on a common server. We provide rigorous theoretical
justifications for the use of distributed estimators with correlated outcomes
by studying the asymptotic behaviour of the combined estimator with fixed and
diverging number of data divisions. Simulations illustrate the finite sample
performance of the proposed method, and we provide an R package for ease of
implementation.
"
"stat.TH","  The asymptotic behaviour of Linear Spectral Statistics (LSS) of the smoothed
periodogram estimator of the spectral coherency matrix of a complex Gaussian
high-dimensional time series (yn) n$\in$Z with independent components is
studied under the asymptotic regime where both the dimension M of y and the
smoothing span of the estimator grow to infinity at the same rate. It is
established that the estimated spectral coherency matrix is close from the
sample covariance matrix of an independent identically N C (0, I M) distributed
sequence, and that its empirical eigenvalue distribution converges towards the
Marcenko-Pastur distribution. This allows to conclude that each LSS has a
deterministic behaviour that can be evaluated explicitely. Using concentration
inequalities, it is shown that the order of magnitude of the deviation of each
LSS from its deterministic approximation is of the order of M N where N is the
sample size. Numerical simulations suggest that these results can be used to
test whether a large number of time series are uncorrelated or not. MSC 2010
subject classifications: Primary 60B20, 62H15; secondary 62M15.
"
"stat.TH","  In this paper, we address the problem of detection, in the frequency domain,
of a M-dimensional time series modeled as the output of a M x K MIMO filter
driven by a K-dimensional Gaussian white noise, and disturbed by an additive
M-dimensional Gaussian colored noise. We consider the study of test statistics
based of the Spectral Coherence Matrix (SCM) obtained as renormalization of the
smoothed periodogram matrix of the observed time series over N samples, and
with smoothing span B. To that purpose, we consider the asymptotic regime in
which M, B, N all converge to infinity at certain specific rates, while K
remains fixed. We prove that the SCM may be approximated in operator norm by a
correlated Wishart matrix, for which Random Matrix Theory (RMT) provides a
precise description of the asymptotic behaviour of the eigenvalues. These
results are then exploited to study the consistency of a test based on the
largest eigenvalue of the SCM, and provide some numerical illustrations to
evaluate the statistical performance of such a test.
"
"stat.TH","  This paper aims at comparing two coupling approaches as basic layers for
building clustering criteria, suited for modularizing very large graphs.
Although the scientific literature is not sparing with clustering criteria
dedicated to graphs and networks decomposition, we shall nevertheless rework
this subject, in this paper, by proposing a new symmetric and dual approach
based on coupling functions, allowing to compare and calibrate them. To
elaborate those coupling maps, we will briefly use ""optimal transport theory""
as a starting point, then we will derive two main families of criteria: those
based upon ""statistical independence"" versus those based upon ""logical
indetermina-tion"". Among others, we will use the so called ""Monge's
properties"", applied to contingency matrices context, as specific tricks for
putting forward some key features about those criteria. A further and deeper
study is proposed, highlighting ""logical indetermination"", because it is, by
far, lesser known. Those dual and parallel criteria are perfectly suited for
graphs clustering, this will be illustrated and shown on various types of
graphs within this paper.
"
"stat.TH","  We give two asymptotic results for the distance covariance on separable
metric spaces without any iid assumptions. In particular, we show the almost
sure convergence of the empirical distance covariance under ergodicity for any
measure with finite first moments. We further give a result concerning the
asymptotic distribution of the empirical distance covariance under the
assumption of absolute regularity and extend these results to certain types of
pseudometric spaces. In the process, we derive a general theorem concerning the
asymptotic distribution of degenerate V-statistics of order 2 under a strong
mixing condition.
"
"stat.TH","  We develop deterministic perturbation bounds for singular values and vectors
of orthogonally decomposable tensors, in a spirit similar to classical results
for matrices. Our bounds exhibit intriguing differences between matrices and
higher-order tensors. Most notably, they indicate that for higher-order tensors
perturbation affects each singular value/vector in isolation. In particular,
its effect on a singular vector does not depend on the multiplicity of its
corresponding singular value or its distance from other singular values. Our
results can be readily applied and provide a unified treatment to many
different problems involving higher-order orthogonally decomposable tensors. In
particular, we illustrate the implications of our bounds through three
connected yet seemingly different high dimensional data analysis tasks: tensor
SVD, tensor regression and estimation of latent variable models, leading to new
insights in each of these settings.
"
"stat.TH","  We study posterior concentration properties of Bayesian procedures for
estimating finite Gaussian mixtures in which the number of components is
unknown and allowed to grow with the sample size. Under this general setup, we
derive a series of new theoretical results. More specifically, we first show
that under mild conditions on the prior, the posterior distribution
concentrates around the true mixing distribution at a near optimal rate with
respect to the Wasserstein distance. Under a separation condition on the true
mixing distribution, we further show that a better and adaptive convergence
rate can be achieved, and the number of components can be consistently
estimated. Furthermore, we derive optimal convergence rates for the
higher-order mixture models where the number of components diverges arbitrarily
fast. In addition, we consider the fractional posterior and investigate its
posterior contraction rates, which are also shown to be minimax optimal in
estimating the mixing distribution under mild conditions. We also investigate
Bayesian estimation of general mixtures with strong identifiability conditions,
and derive the optimal convergence rates when the number of components is
fixed. Lastly, we study theoretical properties of the posterior of the popular
Dirichlet process (DP) mixture prior, and show that such a model can provide a
reasonable estimate for the number of components while only guaranteeing a slow
convergence rate of the mixing distribution estimation.
"
"stat.TH","  Inspired by Stein's lemma, we derive two expressions for the joint moments of
elliptical distributions. We use two different methods to derive
$E[X_{1}^{2}f(\mathbf{X})]$ for any measurable function $f$ satisfying some
regularity conditions. Then, by applying this result, we obtain new formulae
for expectations of product of normally distributed random variables, and also
present simplified expressions of $E[X_{1}^{2}f(\mathbf{X})]$ for multivariate
Student-$t$, logistic and Laplace distributions.
"
"stat.TH","  We present general results on the univariate tail conditional expectation
(TCE) and multivariate tail conditional expectation for location-scale mixture
of elliptical distributions. Examples include the location-scale mixture of
normal distributions, location-scale mixture of Student-$t$ distributions,
location-scale mixture of Logistic distributions and location-scale mixture of
Laplace distributions. We also consider portfolio risk decomposition with TCE
for location-scale mixture of elliptical distributions.
"
"stat.TH","  General linear models (GLM) are often constructed and used in statistical
inference at the voxel level in brain imaging. In this paper, we explore the
basics of random fields and the multiple comparisons on the random fields,
which are necessary to properly threshold statistical maps for the whole image
at specific statistical significance level. The multiple comparisons are
crucial in determining overall statistical significance in correlated test
statistics over the whole brain. In practice, t- or F-statistics in adjacent
voxels are correlated. So there is the problem of multiple comparisons, which
we have simply neglected up to now. For multiple comparisons that account for
spatially correlated test statistics, various methods were proposed: Bonferroni
correction, random field theory, false discovery rates and permutation tests.
Among them, we will explore the random field approach.
"
"stat.TH","  Given $n$ independent and identically distributed observations and measuring
the value of obtaining an additional observation in terms of Le Cam's notion of
deficiency between experiments we show for certain types of non-parametric
experiments that the value of an additional observation decreases at a rate of
$1/\sqrt{n}$. This is distinct from the known typical decrease at a rate of
$1/n$ for parametric experiments and non-decreasing value in the case of very
large experiments. In particular the rate of $1/\sqrt{n}$ holds for the
experiment given by observing samples from a density about which we know only
that it is bounded from below by some fixed constant. Thus there exists an
experiment where the value of additional observations tends to zero but for
which no consistent, in total variation distance, estimator exists.
"
"stat.TH","  We give two new simple characterizations of the Cauchy distribution by using
the M\""obius and Mellin transforms. They also yield characterizations of the
circular Cauchy distribution and the mixture Cauchy model.
"
"stat.TH","  The linear regression model can be used even when the true regression
function is not linear. The resulting estimated linear function is the best
linear approximation to the regression function and the vector $\beta$ of the
coefficients of this linear approximation are the projection parameter. We
provide finite sample bounds on the Normal approximation to the law of the
least squares estimator of the projection parameters normalized by the
sandwich-based standard error. Our results hold in the increasing dimension
setting and under minimal assumptions on the distribution of the response
variable. Furthermore, we construct confidence sets for $\beta$ in the form of
hyper-rectangles and establish rates on their coverage accuracy. We provide
analogous results for partial correlations among the entries of sub-Gaussian
vectors.
"
"stat.TH","  An individualized dose rule recommends a dose level within a continuous safe
dose range based on patient level information such as physical conditions,
genetic factors and medication histories. Traditionally, personalized dose
finding process requires repeating clinical visits of the patient and frequent
adjustments of the dosage. Thus the patient is constantly exposed to the risk
of underdosing and overdosing during the process. Statistical methods for
finding an optimal individualized dose rule can lower the costs and risks for
patients. In this article, we propose a kernel assisted learning method for
estimating the optimal individualized dose rule. The proposed methodology can
also be applied to all other continuous decision-making problems. Advantages of
the proposed method include robustness to model misspecification and capability
of providing statistical inference for the estimated parameters. In the
simulation studies, we show that this method is capable of identifying the
optimal individualized dose rule and produces favorable expected outcomes in
the population. Finally, we illustrate our approach using data from a warfarin
dosing study for thrombosis patients.
"
"stat.TH","  Recently, there has been significant work studying distribution testing under
the Conditional Sampling model. In this model, a query specifies a subset $S$
of the domain, and the output received is a sample drawn from the distribution
conditioned on being in $S$. In this paper, we improve query complexity bounds
for several classic distribution testing problems in this model.
  First, we prove that tolerant uniformity testing in the conditional sampling
model can be solved using $\tilde{O}(\varepsilon^{-2})$ queries, which is
optimal and improves upon the $\tilde{O}(\varepsilon^{-20})$-query algorithm of
Canonne et al. [CRS15]. This bound even holds under a restricted version of the
conditional sampling model called the Pair Conditional Sampling model. Next, we
prove that tolerant identity testing in the conditional sampling model can be
solved in $\tilde{O}(\varepsilon^{-4})$ queries, which is the first known bound
independent of the support size of the distribution for this problem. Next, we
use our algorithm for tolerant uniformity testing to get an
$\tilde{O}(\varepsilon^{-4})$-query algorithm for monotonicity testing in the
conditional sampling model, improving on the
$\tilde{O}(\varepsilon^{-22})$-query algorithm of Canonne [Can15]. Finally, we
study (non-tolerant) identity testing under the pair conditional sampling
model, and provide a tight bound of $\tilde{\Theta}(\sqrt{\log N} \cdot
\varepsilon^{-2})$ for the query complexity, where the domain of the
distribution has size $N$. This improves upon both the known upper and lower
bounds in [CRS15].
"
"stat.TH","  In this paper we are concerned with a sequence of univariate random variables
with piecewise polynomial means and independent sub-Gaussian noise. The
underlying polynomials are allowed to be of arbitrary but fixed degrees. All
the other model parameters are allowed to vary depending on the sample size.
  We propose a two-step estimation procedure based on the $\ell_0$-penalisation
and provide upper bounds on the localisation error. We complement these results
by deriving a global information-theoretic lower bounds, which show that our
two-step estimators are nearly minimax rate-optimal. We also show that our
estimator enjoys near optimally adaptive performance by attaining individual
localisation errors depending on the level of smoothness at individual change
points of the underlying signal. In addition, under a special smoothness
constraint, we provide a minimax lower bound on the localisation errors. This
lower bound is independent of the polynomial orders and is sharper than the
global minimax lower bound.
"
"stat.TH","  This work is devoted to the finite sample prediction risk analysis of a class
of linear predictors of a response $Y\in \mathbb{R}$ from a high-dimensional
random vector $X\in \mathbb{R}^p$ when $(X,Y)$ follows a latent factor
regression model generated by a unobservable latent vector $Z$ of dimension
less than $p$. Our primary contribution is in establishing finite sample risk
bounds for prediction with the ubiquitous Principal Component Regression (PCR)
method, under the factor regression model, with the number of principal
components adaptively selected from the data---a form of theoretical guarantee
that is surprisingly lacking from the PCR literature. To accomplish this, we
prove a master theorem that establishes a risk bound for a large class of
predictors, including the PCR predictor as a special case. This approach has
the benefit of providing a unified framework for the analysis of a wide range
of linear prediction methods, under the factor regression setting. In
particular, we use our main theorem to recover known risk bounds for the
minimum-norm interpolating predictor, which has received renewed attention in
the past two years, and a prediction method tailored to a subclass of factor
regression models with identifiable parameters. This model-tailored method can
be interpreted as prediction via clusters with latent centers. To address the
problem of selecting among a set of candidate predictors, we analyze a simple
model selection procedure based on data-splitting, providing an oracle
inequality under the factor model to prove that the performance of the selected
predictor is close to the optimal candidate. We conclude with a detailed
simulation study to support and complement our theoretical results.
"
"stat.TH","  In this paper, we study the log-likelihood function and Maximum Likelihood
Estimate (MLE) for the matrix normal model for both real and complex models. We
describe the exact number of samples needed to achieve (almost surely) three
conditions, namely a bounded log-likelihood function, existence of MLEs, and
uniqueness of MLEs. As a consequence, we observe that almost sure boundedness
of log-likelihood function guarantees almost sure existence of an MLE, thereby
proving a conjecture of Drton, Kuriki and Hoff. The main tools we use are from
the theory of quiver representations, in particular, results of Kac, King and
Schofield on canonical decomposition and stability.
"
"stat.TH","  Missing data and confounding are two problems researchers face in
observational studies for comparative effectiveness. Williamson et al. (2012)
recently proposed a unified approach to handle both issues concurrently using a
multiply-robust (MR) methodology under the assumption that confounders are
missing at random. Their approach considers a union of models in which any
submodel has a parametric component while the remaining models are
unrestricted. We show that while their estimating function is MR in theory, the
possibility for multiply robust inference is complicated by the fact that
parametric models for different components of the union model are not variation
independent and therefore the MR property is unlikely to hold in practice. To
address this, we propose an alternative transparent parametrization of the
likelihood function, which makes explicit the model dependencies between
various nuisance functions needed to evaluate the MR efficient score. The
proposed method is genuinely doubly-robust (DR) in that it is consistent and
asymptotic normal if one of two sets of modeling assumptions holds. We evaluate
the performance and doubly robust property of the DR method via a simulation
study.
"
"stat.TH","  We introduce a new class of methods for finite-sample false discovery rate
(FDR) control in multiple testing problems with dependent test statistics where
the dependence is fully or partially known. Our approach separately calibrates
a data-dependent p-value rejection threshold for each hypothesis, relaxing or
tightening the threshold as appropriate to target exact FDR control. In
addition to our general framework we propose a concrete algorithm, the
dependence-adjusted Benjamini-Hochberg (dBH) procedure, which adaptively
thresholds the q-value for each hypothesis. Under positive regression
dependence the dBH procedure uniformly dominates the standard BH procedure, and
in general it uniformly dominates the Benjamini-Yekutieli (BY) procedure (also
known as BH with log correction). Simulations and real data examples illustrate
power gains over competing approaches to FDR control under dependence.
"
"stat.TH","  Population quantiles are important parameters in many applications.
Enthusiasm for the development of effective statistical inference procedures
for quantiles and their functions has been high for the past decade. In this
article, we study inference methods for quantiles when multiple samples from
linked populations are available. The research problems we consider have a wide
range of applications. For example, to study the evolution of the economic
status of a country, economists monitor changes in the quantiles of annual
household incomes, based on multiple survey datasets collected annually. Even
with multiple samples, a routine approach would estimate the quantiles of
different populations separately. Such approaches ignore the fact that these
populations are linked and share some intrinsic latent structure. Recently,
many researchers have advocated the use of the density ratio model (DRM) to
account for this latent structure and have developed more efficient procedures
based on pooled data. The nonparametric empirical likelihood (EL) is
subsequently employed. Interestingly, there has been no discussion in this
context of the EL-based likelihood ratio test (ELRT) for population quantiles.
We explore the use of the ELRT for hypotheses concerning quantiles and
confidence regions under the DRM. We show that the ELRT statistic has a
chi-square limiting distribution under the null hypothesis. Simulation
experiments show that the chi-square distributions approximate the
finite-sample distributions well and lead to accurate tests and confidence
regions. The DRM helps to improve statistical efficiency. We also give a
real-data example to illustrate the efficiency of the proposed method.
"
"stat.TH","  We develop Edgeworth expansion theory for spot volatility estimator under
general assumptions on the log-price process that allow for drift and leverage
effect. The result is based on further estimation of skewness and kurtosis,
when compared with existing second order asymptotic normality result. Thus our
theory can provide with a refinement result for the finite sample distribution
of spot volatility. We also construct feasible confidence intervals (one-sided
and two-sided) for spot volatility by using Edgeworth expansion. The Monte
Carlo simulation study we conduct shows that the intervals based on Edgeworth
expansion perform better than the conventional intervals based on normal
approximation, which justifies the correctness of our theoretical conclusion.
"
"stat.TH","  Regression models with crossed random effect error models can be very
expensive to compute. The cost of both generalized least squares and Gibbs
sampling can easily grow as $N^{3/2}$ (or worse) for $N$ observations.
Papaspiliopoulos et al. (2020) present a collapsed Gibbs sampler that costs
$O(N)$, but under an extremely stringent sampling model. We propose a
backfitting algorithm to compute a generalized least squares estimate and prove
that it costs $O(N)$ under greatly relaxed though still strict sampling
assumptions. Empirically, the backfitting algorithm costs $O(N)$ under further
relaxed assumptions. We illustrate the new algorithm on a ratings data set from
Stitch Fix.
"
"stat.TH","  Wasserstein geometry and information geometry are two important structures to
be introduced in a manifold of probability distributions. Wasserstein geometry
is defined by using the transportation cost between two distributions, so it
reflects the metric of the base manifold on which the distributions are
defined. Information geometry is defined to be invariant under reversible
transformations of the base space. Both have their own merits for applications.
In particular, statistical inference is based upon information geometry, where
the Fisher metric plays a fundamental role, whereas Wasserstein geometry is
useful in computer vision and AI applications. In this study, we analyze
statistical inference based on the Wasserstein geometry in the case that the
base space is one-dimensional. By using the location-scale model, we further
derive the W-estimator that explicitly minimizes the transportation cost from
the empirical distribution to a statistical model and study its asymptotic
behaviors. We show that the W-estimator is consistent and explicitly give its
asymptotic distribution. The W-estimator is Fisher efficient only in the
Gaussian case. We further prove that the maximum likelihood estimator minimizes
the transportation cost from the true distribution to the estimated one.
"
"stat.TH","  Majorisation, also called rearrangement inequalities, yields a type of
stochastic ordering in which two or more distributions can be then compared.
This method provides a representation of the peakedness of probability
distributions and is also independent of the location of probabilities. These
properties make majorisation a good candidate as a theory for uncertainty. We
demonstrate that this approach is also dimension free by obtaining univariate
decreasing rearrangements from multivariate distributions, thus we can consider
the ordering of two, or more, distributions with different support. We present
operations including inverse mixing and maximise/minimise to combine and
analyse uncertainties associated with different distribution functions. We
illustrate these methods for empirical examples with applications to scenario
analysis and simulations.
"
"stat.TH","  We obtain central limit theorems for stationary random fields which are based
on the use of a novel measure of dependence called $\theta$-lex weak
dependence. We discuss hereditary properties for $\theta$-lex and $\eta$-weak
dependence and illustrate the possible applications of the weak dependence
notions to the study of the asymptotic properties of stationary random fields.
Our general results are applied to mixed moving average fields (MMAF in short)
and ambit fields. We show general conditions such that MMAF and ambit fields,
with the volatility field being an MMAF or a $p$-dependent random field, are
weakly dependent. For all the aforementioned models, we give a complete
characterization of their weak dependence coefficients and sufficient
conditions to obtain asymptotic normality of their sample moments. Finally, we
give explicit computations of the weak dependence coefficients in the case of
MSTOU and CARMA fields.
"
"stat.TH","  In this paper we develop valid inference for high-dimensional time series. We
extend the desparsified lasso to a time series setting under Near-Epoch
Dependence (NED) assumptions allowing for non-Gaussian, serially correlated and
heteroskedastic processes, where the number of regressors can possibly grow
faster than the time dimension. We first derive an oracle inequality for the
(regular) lasso, relaxing the commonly made exact sparsity assumption to a
weaker alternative, which permits many small but non-zero parameters. The weak
sparsity coupled with the NED assumption means this inequality can also be
applied to the (inherently misspecified) nodewise regressions performed in the
desparsified lasso. This allows us to establish the uniform asymptotic
normality of the desparsified lasso under general conditions. Additionally, we
show consistency of a long-run variance estimator, thus providing a complete
set of tools for performing inference in high-dimensional linear time series
models. Finally, we perform a simulation exercise to demonstrate the small
sample properties of the desparsified lasso in common time series settings.
"
"stat.TH","  We consider distributed inference using sequentially interactive protocols.
We obtain lower bounds on the minimax sample complexity of interactive
protocols under local information constraints, a broad family of resource
constraints which captures communication constraints, local differential
privacy, and noisy binary queries as special cases. We focus on the inference
tasks of learning (density estimation) and identity testing (goodness-of-fit)
for discrete distributions under total variation distance, and establish
general lower bounds that take into account the local constraints modeled as a
channel family.
  Our main technical contribution is an approach to handle the correlation that
builds due to interactivity and quantifies how effectively one can exploit this
correlation in spite of the local constraints. Using this, we fill gaps in some
prior works and characterize the optimal sample complexity of learning and
testing discrete distributions under total variation distance, for both
communication and local differential privacy constraints. Prior to our work,
this was known only for the problem of testing under local privacy constraints
(Amin, Joseph, and Mao (2020); Berrett and Butucea (2020)). Our results show
that interactivity does not help for learning or testing under these two
constraints. Finally, we provide the first instance of a natural family of
""leaky query"" local constraints under which interactive protocols strictly
outperform the noninteractive ones for distribution testing.
"
"stat.TH","  A Bayesian decision analysis perspective on problems of constrained
forecasting is presented and developed, motivated by increasing interest in
problems of aggregate and hierarchical forecasting coupled with short-comings
of traditional, purely inferential approaches. Foundational and pedagogic
developments underlie new methodological approaches to such problems, explored
and exemplified in contexts of total-constrained forecasting linked to
motivating applications in commercial forecasting. The new perspective is
complementary and integrated with traditional Bayesian inference approaches,
while offering new practical methodology when the traditional view is
challenged. Examples explore ranges of practically relevant loss functions in
simple, illustrative contexts that highlight the opportunities for methodology
as well as practically important questions of how constrained forecasting is
impacted by dependencies among outcomes being predicted. The paper couples this
core development with arguments in support of a broader view of Bayesian
decision analysis than is typically adopted, involving studies of predictive
distributions of loss function values under putative optimal decisions.
Additional examples highlight the practical importance of this broader view in
the constrained forecasting context. Extensions to more general constrained
forecasting problems, and connections with broader interests in forecast
reconciliation and aggregation are noted along with other broader
considerations.
"
"stat.TH","  In this paper, we provide general bounds on the mean absolute difference and
difference of moments of a random variable $X$ and a perturbation rd$(X)$, when
$|{\rm rd}(x)-x| \leq \epsilon |x|$ or $|{\rm rd}(x)-x| \leq \delta$ which
depend linearly on $\epsilon$ and $\delta$. We then show that if the
perturbation corresponds to rounding to the nearest point in some fixed
discrete set, the bounds on the difference of moments can be improved to
quadratic in many cases. When the points in this fixed set are uniformly
spaced, our analysis can be viewed as a generalization of Sheppard's
corrections. We discuss how our bounds can be used to balance measurement error
with sample error in a rigorous way, as well as how they can be used to
generalize classical numerical analysis results. The frameworks developed in
our analysis can be applied to a wider range of applications than those studied
in this paper and may be of general interest.
"
"stat.TH","  This paper concerns the parameter estimation problem for the quadratic
potential energy in interacting particle systems from continuous-time and
single-trajectory data. Even though such dynamical systems are
high-dimensional, we show that the vanilla maximum likelihood estimator
(without regularization) is able to estimate the interaction potential
parameter with optimal rate of convergence simultaneously in mean-field limit
and in long-time dynamics. This to some extend avoids the
curse-of-dimensionality for estimating large dynamical systems under symmetry
of the particle interaction.
"
"stat.TH","  A fundamental problem in the high-dimensional regression is to understand the
tradeoff between type I and type II errors or, equivalently, false discovery
rate (FDR) and power in variable selection. To address this important problem,
we offer the first complete tradeoff diagram that distinguishes all pairs of
FDR and power that can be asymptotically realized by the Lasso with some choice
of its penalty parameter from the remaining pairs, in a regime of linear
sparsity under random designs. The tradeoff between the FDR and power
characterized by our diagram holds no matter how strong the signals are. In
particular, our results improve on the earlier Lasso tradeoff diagram of
arXiv:1511.01957 by recognizing two simple but fundamental constraints on the
pairs of FDR and power. The improvement is more substantial when the regression
problem is above the Donoho--Tanner phase transition. Finally, we present
extensive simulation studies to confirm the sharpness of the complete Lasso
tradeoff diagram.
"
"stat.TH","  We study the statistical problem of estimating a rank-one sparse tensor
corrupted by additive Gaussian noise, a model also known as sparse tensor PCA.
We show that for Bernoulli and Bernoulli-Rademacher distributed signals and
\emph{for all} sparsity levels which are sublinear in the dimension of the
signal, the sparse tensor PCA model exhibits a phase transition called the
\emph{all-or-nothing phenomenon}. This is the property that for some
signal-to-noise ratio (SNR) $\mathrm{SNR_c}$ and any fixed $\epsilon>0$, if the
SNR of the model is below $\left(1-\epsilon\right)\mathrm{SNR_c}$, then it is
impossible to achieve any arbitrarily small constant correlation with the
hidden signal, while if the SNR is above $\left(1+\epsilon
\right)\mathrm{SNR_c}$, then it is possible to achieve almost perfect
correlation with the hidden signal. The all-or-nothing phenomenon was initially
established in the context of sparse linear regression, and over the last year
also in the context of sparse 2-tensor (matrix) PCA, Bernoulli group testing,
and generalized linear models. Our results follow from a more general result
showing that for any Gaussian additive model with a discrete uniform prior, the
all-or-nothing phenomenon follows as a direct outcome of an appropriately
defined ""near-orthogonality"" property of the support of the prior distribution.
"
"stat.TH","  We introduce stochastic sequences $\zeta(k)$ with periodically stationary
generalized multiple increments of fractional order which combines
cyclostationary, multi-seasonal, integrated and fractionally integrated
patterns. We solve the problem of optimal estimation of linear functionals
constructed from unobserved values of stochastic sequences $\zeta(k)$ based on
their observations at points $ k<0$. For sequences with known spectral
densities, we obtain formulas for calculating values of the mean square errors
and the spectral characteristics of the optimal estimates of functionals.
Formulas that determine the least favorable spectral densities and minimax
(robust) spectral characteristics of the optimal linear estimates of
functionals are proposed in the case where spectral densities of sequences are
not exactly known while some sets of admissible spectral densities are given.
"
"stat.TH","  Multi-reference alignment entails estimating a signal in $\mathbb{R}^L$ from
its circularly-shifted and noisy copies. This problem has been studied
thoroughly in recent years, focusing on the finite-dimensional setting (fixed
$L$). Motivated by single-particle cryo-electron microscopy, we analyze the
sample complexity of the problem in the high-dimensional regime $L\to\infty$.
Our analysis uncovers a phase transition phenomenon governed by the parameter
$\alpha = L/(\sigma^2\log L)$, where $\sigma^2$ is the variance of the noise.
When $\alpha>2$, the impact of the unknown circular shifts on the sample
complexity is minor. Namely, the number of measurements required to achieve a
desired accuracy $\varepsilon$ approaches $\sigma^2/\varepsilon$ for small
$\varepsilon$; this is the sample complexity of estimating a signal in additive
white Gaussian noise, which does not involve shifts. In sharp contrast, when
$\alpha\leq 2$, the problem is significantly harder and the sample complexity
grows substantially quicker with $\sigma^2$.
"
"stat.TH","  We consider bivariate observations $(X_1,Y_1), \ldots, (X_n,Y_n)\subset
\mathfrak{X}\times\mathbb{R}$ with a real set $\mathfrak{X}$ such that,
conditional on the $X_i$, the $Y_i$ are independent random variables with
distribution $P_{X_i}$, where $(P_x)_{x\in\mathfrak{X}}$ is unknown. Using an
empirical likelihood approach, we devise an algorithm to estimate the unknown
family of distributions $(P_x)_{x\in\mathfrak{X}}$ under the sole assumption
that this family is increasing with respect to likelihood ratio order. We
review the latter concept and realize that no further assumption such as all
distributions $P_x$ having densities or having a common countable support is
needed.
  The benefit of the stronger regularization imposed by likelihood ratio
ordering over the usual stochastic ordering is evaluated in terms of estimation
and predictive performances on simulated as well as real data.
"
"stat.TH","  We study statistical inference for small-noise-perturbed multiscale dynamical
systems where the slow motion is driven by fractional Brownian motion. We
develop statistical estimators for both the Hurst index as well as a vector of
unknown parameters in the model based on a single time series of observations
from the slow process only. We prove that these estimators are both consistent
and asymptotically normal as the amplitude of the perturbation and the
time-scale separation parameter go to zero. Numerical simulations illustrate
the theoretical results.
"
"stat.TH","  We consider the batch (off-line) policy learning problem in the infinite
horizon Markov Decision Process. Motivated by mobile health applications, we
focus on learning a policy that maximizes the long-term average reward. We
propose a doubly robust estimator for the average reward and show that it
achieves semiparametric efficiency given multiple trajectories collected under
some behavior policy. Based on the proposed estimator, we develop an
optimization algorithm to compute the optimal policy in a parameterized
stochastic policy class. The performance of the estimated policy is measured by
the difference between the optimal average reward in the policy class and the
average reward of the estimated policy and we establish a finite-sample regret
guarantee. To the best of our knowledge, this is the first regret bound for
batch policy learning in the infinite time horizon setting. The performance of
the method is illustrated by simulation studies.
"
"stat.TH","  Identifying directions where severe events occur is a major challenge in
multivariate Extreme Value Analysis. The support of the spectral measure of
regularly varying vectors brings out which coordinates contribute to the
extremes. This measure is defined via weak convergence which fails at providing
an estimator of its support, especially in high dimension. The estimation of
the support is all the more challenging since it relies on a threshold above
which the data are considered to be extreme and the choice of such a threshold
is still an open problem. In this paper we extend the framework of sparse
regular variation introduced by Meyer and Wintenberger (2020) to infer tail
dependence. This approach relies on the Euclidean projection onto the simplex
which exhibits sparsity and reduces the dimension of the extremes' analysis. We
provide an algorithmic approach based on model selection to tackle both the
choice of an optimal threshold and the learning of relevant directions on which
extreme events appear. We apply our method on numerical experiments to
highlight the relevance of our findings. Finally we illustrate our approach
with financial return data.
"
"stat.TH","  The non-parametric estimation of covariance lies at the heart of functional
data analysis, whether for curve or surface-valued data. The case of a
two-dimensional domain poses both statistical and computational challenges,
which are typically alleviated by assuming separability. However, separability
is often questionable, sometimes even demonstrably inadequate. We propose a
framework for the analysis of covariance operators of random surfaces that
generalises separability, while retaining its major advantages. Our approach is
based on the additive decomposition of the covariance into a series of
separable components. The decomposition is valid for any covariance over a
two-dimensional domain. Leveraging the key notion of the partial inner product,
we generalise the power iteration method to general Hilbert spaces and show how
the aforementioned decomposition can be efficiently constructed in practice.
Truncation of the decomposition and retention of the principal separable
components automatically induces a non-parametric estimator of the covariance,
whose parsimony is dictated by the truncation level. The resulting estimator
can be calculated, stored and manipulated with little computational overhead
relative to separability. The framework and estimation method are genuinely
non-parametric, since the considered decomposition holds for any covariance.
Consistency and rates of convergence are derived under mild regularity
assumptions, illustrating the trade-off between bias and variance regulated by
the truncation level. The merits and practical performance of the proposed
methodology are demonstrated in a comprehensive simulation study.
"
"stat.TH","  We investigate the problem of sensor clusterization in optimal experimental
design for infinite-dimensional Bayesian inverse problems. We suggest an
analytically tractable model for such designs and reason how it may lead to
sensor clusterization in the case of iid measurement noise. We also show that
in the case of spatially correlated measurement error clusterization does not
occur. As a part of the analysis we prove a matrix determinant lemma analog in
infinite dimensions, as well as a lemma for calculating derivatives of $\log
\det$ of operators.
"
"stat.TH","  In the linear regression model with possibly autoregressive errors, we
propose a family of nonparametric tests for regression under a nuisance
autoregression. The tests avoid the estimation of nuisance parameters, in
contrast to the tests proposed in the literature.
"
"stat.TH","  Heyde proved that a Gaussian distribution on the real line is characterized
by the symmetry of the conditional distribution of one linear statistic given
another. The present article is devoted to a group analogue of the Heyde
theorem. We describe distributions of independent random variables $\xi_1$,
$\xi_2$ with values in a group $X=\mathbb{R}^n\times D$, where $D$ is a
discrete Abelian group, which are characterized by the symmetry of the
conditional distribution of the linear statistic $L_2 = \xi_1 + \delta\xi_2$
given $L_1 = \xi_1 + \xi_2$, where $\delta$ is a topological automorphism of
$X$ such that ${Ker}(I+\delta)=\{0\}$.
"
"stat.TH","  We consider a high-dimensional linear regression problem. Unlike many papers
on the topic, we do not require sparsity of the regression coefficients;
instead, our main structural assumption is a decay of eigenvalues of the
covariance matrix of the data. We propose a new family of estimators, called
the canonical thresholding estimators, which pick largest regression
coefficients in the canonical form. The estimators admit an explicit form and
can be linked to LASSO and Principal Component Regression (PCR). A theoretical
analysis for both fixed design and random design settings is provided. Obtained
bounds on the mean squared error and the prediction error of a specific
estimator from the family allow to clearly state sufficient conditions on the
decay of eigenvalues to ensure convergence. In addition, we promote the use of
the relative errors, strongly linked with the out-of-sample $R^2$. The study of
these relative errors leads to a new concept of joint effective dimension,
which incorporates the covariance of the data and the regression coefficients
simultaneously, and describes the complexity of a linear regression problem.
Numerical simulations confirm good performance of the proposed estimators
compared to the previously developed methods.
"
"stat.TH","  Various methods of combining individual p-values into one p-value are widely
used in many areas of statistical applications. We say that a combining method
is valid for arbitrary dependence (VAD) if it does not require any assumption
on the dependence structure of the p-values, whereas it is valid for some
dependence (VSD) if it requires some specific, perhaps realistic but
unjustifiable, dependence structures. The trade-off between validity and
efficiency of these methods is studied via analyzing the choices of critical
values under different dependence assumptions. We introduce the notions of
independence-comonotonicity balance (IC-balance) and the price for validity. In
particular, IC-balanced methods always produce an identical critical value for
independent and perfectly positively dependent p-values, a specific type of
insensitivity to a family of dependence assumptions. We show that, among two
very general classes of merging methods commonly used in practice, the Cauchy
combination method and the Simes method are the only IC-balanced ones.
Simulation studies and a real data analysis are conducted to analyze the sizes
and powers of various combining methods in the presence of weak and strong
dependence.
"
"stat.TH","  Sensitivity indices are commonly used to quantity the relative inuence of any
specic group of input variables on the output of a computer code. In this
paper, we focus both on computer codes the output of which is a cumulative
distribution function and on stochastic computer codes. We propose a way to
perform a global sensitivity analysis for these kinds of computer codes. In the
rst setting, we dene two indices: the rst one is based on Wasserstein
Fr{\'e}chet means while the second one is based on the Hoeding decomposition of
the indicators of Wasserstein balls. Further, when dealing with the stochastic
computer codes, we dene an ideal version of the stochastic computer code thats
ts into the frame of the rst setting. Finally, we deduce a procedure to realize
a second level global sensitivity analysis, namely when one is interested in
the sensitivity related to the input distributions rather than in the
sensitivity related to the inputs themselves. Several numerical studies are
proposed as illustrations in the dierent settings.
"
"stat.TH","  This paper obtains asymptotic results for parametric inference using
prediction-based estimating functions when the data are high frequency
observations of a diffusion process with an infinite time horizon.
Specifically, the data are observations of a diffusion process at $n$
equidistant time points $\Delta_n i$, and the asymptotic scenario is $\Delta_n
\to 0$ and $n\Delta_n \to \infty$. For a useful and tractable classes of
prediction-based estimating functions, existence of a consistent estimator is
proved under standard weak regularity conditions on the diffusion process and
the estimating function. Asymptotic normality of the estimator is established
under the additional rate condition $n\Delta_n^3 \to 0$. The prediction-based
estimating functions are approximate martingale estimating functions to a
smaller order than what has previously been studied, and new non-standard
asymptotic theory is needed. A Monte Carlo method for calculating the
asymptotic variance of the estimators is proposed.
"
"stat.TH","  This work develops central limit theorems for cross-validation and consistent
estimators of its asymptotic variance under weak stability conditions on the
learning algorithm. Together, these results provide practical,
asymptotically-exact confidence intervals for $k$-fold test error and valid,
powerful hypothesis tests of whether one learning algorithm has smaller
$k$-fold test error than another. These results are also the first of their
kind for the popular choice of leave-one-out cross-validation. In our real-data
experiments with diverse learning algorithms, the resulting intervals and tests
outperform the most popular alternative methods from the literature.
"
"stat.TH","  Bayesian model selection is premised on the assumption that the data are
generated from one of the postulated models, however, in many applications, all
of these models are incorrect. When two or more models provide a nearly equally
good fit to the data, Bayesian model selection can be highly unstable,
potentially leading to self-contradictory findings. In this paper, we explore
using bagging on the posterior distribution (""BayesBag"") when performing model
selection -- that is, averaging the posterior model probabilities over many
bootstrapped datasets. We provide theoretical results characterizing the
asymptotic behavior of the standard posterior and the BayesBag posterior under
misspecification, in the model selection setting. We empirically assess the
BayesBag approach on synthetic and real-world data in (i) feature selection for
linear regression and (ii) phylogenetic tree reconstruction. Our theory and
experiments show that in the presence of misspecification, BayesBag provides
(a) greater reproducibility and (b) greater accuracy in selecting the correct
model, compared to the standard Bayesian posterior; on the other hand, under
correct specification, BayesBag is slightly more conservative than the standard
posterior. Overall, our results demonstrate that BayesBag provides an
easy-to-use and widely applicable approach that improves upon standard Bayesian
model selection by making it more stable and reproducible.
"
"stat.TH","  Jointly using data from multiple similar sources for the training of
prediction models is increasingly becoming an important task in many fields of
science. In this paper, we propose a framework for generalist and specialist
predictions that leverages multiple datasets, with potential heterogenity in
the relationships between predictors and outcomes. Our framework uses
ensembling with stacking, and includes three major components: 1) training of
the ensemble members using one or more datasets, 2) a no-data-reuse technique
for stacking weights estimation and 3) task-specific utility functions. We
prove that under certain regularity conditions, our framework produces a
stacked prediction function with oracle property. We also provide analytically
the conditions under which the proposed no-data-reuse technique will increase
the prediction accuracy of the stacked prediction function compared to using
the full data. We perform a simulation study to numerically verify and
illustrate these results and apply our framework to predicting mortality based
on a collection of variables including long-term exposure to common air
pollutants.
"
"stat.TH","  Modern neural networks are often operated in a strongly overparametrized
regime: they comprise so many parameters that they can interpolate the training
set, even if actual labels are replaced by purely random ones. Despite this,
they achieve good prediction error on unseen data: interpolating the training
set does not induce overfitting. Further, overparametrization appears to be
beneficial in that it simplifies the optimization landscape. Here we study
these phenomena in the context of two-layers neural networks in the neural
tangent (NT) regime. We consider a simple data model, with isotropic feature
vectors in $d$ dimensions, and $N$ hidden neurons. Under the assumption $N \le
Cd$ (for $C$ a constant), we show that the network can exactly interpolate the
data as soon as the number of parameters is significantly larger than the
number of samples: $Nd\gg n$. Under these assumptions, we show that the
empirical NT kernel has minimum eigenvalue bounded away from zero, and
characterize the generalization error of min-$\ell_2$ norm interpolants, when
the target function is linear. In particular, we show that the network
approximately performs ridge regression in the raw features, with a strictly
positive `self-induced' regularization.
"
"stat.TH","  We construct a Bayesian sequential test of two simple hypotheses about the
value of the unobservable drift coefficient of a Brownian motion, with a
possibility to change the initial decision at subsequent moments of time for
some penalty. Such a testing procedure allows to correct the initial decision
if it turns out to be wrong. The test is based on observation of the posterior
mean process and makes the initial decision and, possibly, changes it later,
when this process crosses certain thresholds. The solution of the problem is
obtained by reducing it to joint optimal stopping and optimal switching
problems.
"
"stat.TH","  Two recently introduced model based bias corrected estimators for proportion
of true null hypotheses ($\pi_0$) under multiple hypotheses testing scenario
have been restructured for exponentially distributed random observations
available for each of the common hypotheses. Based on stochastic ordering, a
new motivation behind formulation of some related estimators for $\pi_0$ is
given. The reduction of bias for the model based estimators are theoretically
justified and algorithms for computing the estimators are also presented. The
estimators are also used to formulate a popular adaptive multiple testing
procedure. Extensive numerical study supports superiority of the bias corrected
estimators. We also point out the adverse effect of using the model based bias
correction method without proper assessment of the underlying distribution. A
case-study is done with a synthetic dataset in connection with reliability and
warranty studies to demonstrate the applicability of the procedure, under a
non-Gaussian set up. The results obtained are in line with the intuition and
experience of the subject expert. An intriguing discussion has been attempted
to conclude the article that also indicates the future scope of study.
"
"stat.TH","  The network interference model for causal inference places all experimental
units at the vertices of an undirected exposure graph, such that treatment
assigned to one unit may affect the outcome of another unit if and only if
these two units are connected by an edge. This model has recently gained
popularity as means of incorporating interference effects into the
Neyman--Rubin potential outcomes framework; and several authors have considered
estimation of various causal targets, including the direct and indirect effects
of treatment. In this paper, we consider large-sample asymptotics for treatment
effect estimation under network interference in a setting where the exposure
graph is a random draw from a graphon. When targeting the direct effect, we
show that---in our setting---popular estimators are considerably more accurate
than existing results suggest, and provide a central limit theorem in terms of
moments of the graphon. Meanwhile, when targeting the indirect effect, we
leverage our generative assumptions to propose a consistent estimator in a
setting where no other consistent estimators are currently available. We also
show how our results can be used to conduct a practical assessment of the
sensitivity of randomized study inference to potential interference effects.
Overall, our results highlight the promise of random graph asymptotics in
understanding the practicality and limits of causal inference under network
interference.
"
"stat.TH","  We consider a general linear program in standard form whose right-hand side
constraint vector is subject to random perturbations. This defines a stochastic
linear program for which, under general conditions, we characterize the
fluctuations of the corresponding empirical optimal solution by a central
limit-type theorem. Our approach relies on the combinatorial nature and the
concept of degeneracy inherent in linear programming, in strong contrast to
well-known results for smooth stochastic optimization programs. In particular,
if the corresponding dual linear program is degenerate the asymptotic limit law
might not be unique and is determined from the way the empirical optimal
solution is chosen. Furthermore, we establish consistency and convergence rates
of the Hausdorff distance between the empirical and the true optimality sets.
As a consequence, we deduce a limit law for the empirical optimal value
characterized by the set of all dual optimal solutions which turns out to be a
simple consequence of our general proof techniques.
  Our analysis is motivated from recent findings in statistical optimal
transport that will be of special focus here. In addition to the asymptotic
limit laws for optimal transport solutions, we obtain results linking
degeneracy of the dual transport problem to geometric properties of the
underlying ground space, and prove almost sure uniqueness statements that may
be of independent interest.
"
"stat.TH","  The Lasso is a method for high-dimensional regression, which is now commonly
used when the number of covariates $p$ is of the same order or larger than the
number of observations $n$. Classical asymptotic normality theory is not
applicable for this model due to two fundamental reasons: $(1)$ The regularized
risk is non-smooth; $(2)$ The distance between the estimator $\bf
\widehat{\theta}$ and the true parameters vector $\bf \theta^\star$ cannot be
neglected. As a consequence, standard perturbative arguments that are the
traditional basis for asymptotic normality fail.
  On the other hand, the Lasso estimator can be precisely characterized in the
regime in which both $n$ and $p$ are large, while $n/p$ is of order one. This
characterization was first obtained in the case of standard Gaussian designs,
and subsequently generalized to other high-dimensional estimation procedures.
Here we extend the same characterization to Gaussian correlated designs with
non-singular covariance structure. This characterization is expressed in terms
of a simpler ``fixed design'' model. We establish non-asymptotic bounds on the
distance between distributions of various quantities in the two models, which
hold uniformly over signals $\bf \theta^\star$ in a suitable sparsity class,
and values of the regularization parameter.
  As applications, we study the distribution of the debiased Lasso, and show
that a degrees-of-freedom correction is necessary for computing valid
confidence intervals.
"
"stat.TH","  This paper considers linear rational expectations models in the frequency
domain. Two classical results drive the entire theory: the
Kolmogorov-Cram\'{e}r spectral representation theorem and Wiener-Hopf
factorization. The paper develops necessary and sufficient conditions for
existence and uniqueness of particular and generic systems. The space of all
solutions is characterized as an affine space in the frequency domain, which
sheds light on the variety of solution methods considered in the literature. It
is shown that solutions are not generally continuous with respect to the
parameters of the models. This motivates regularized solutions with
theoretically guaranteed smoothness properties. As an application, the limiting
Gaussian likelihood functions of solutions is derived analytically and its
properties are studied. The paper finds that non-uniqueness leads to highly
irregular likelihood functions and recommends either restricting the parameter
space to the region of uniqueness or employing regularization.
"
"stat.TH","  A class of multivariate spectral representations for real-valued
nonstationary random variables is introduced, which is characterised by a
general complex Gaussian distribution. In this way, the temporal signal
properties -- harmonicity, wide-sense stationarity and cyclostationarity -- are
designated respectively by the mean, Hermitian variance and pseudo-variance of
the associated time-frequency representation (TFR). For rigour, the estimators
of the TFR distribution parameters are derived within a maximum likelihood
framework and are shown to be statistically consistent, owing to the
statistical identifiability of the proposed distribution parametrization. By
virtue of the assumed probabilistic model, a generalised likelihood ratio test
(GLRT) for nonstationarity detection is also proposed. Intuitive examples
demonstrate the utility of the derived probabilistic framework for spectral
analysis in low-SNR environments.
"
"stat.TH","  We consider together the retrospective and the sequential change-point
detection in a general class of integer-valued time series.
  The conditional mean of the process depends on a parameter $\theta^*$ which
may change over time. We propose procedures which are based on the Poisson
quasi-maximum likelihood estimator of the parameter, and where the updated
estimator is computed without the historical observations in the sequential
framework. For both the retrospective and the sequential detection, the test
statistics converge to some distributions obtained from the standard Brownian
motion under the null hypothesis of no change and diverge to infinity under the
alternative; that is, these procedures are consistent.
  Some results of simulations as well as real data application are provided.
"
"stat.TH","  Methods of merging several p-values into a single p-value are important in
their own right and widely used in multiple hypothesis testing. This paper is
the first to systematically study the admissibility (in Wald's sense) of
p-merging functions and their domination structure, without any assumptions on
the dependence structure of the input p-values. As a technical tool we use the
notion of e-values, which are alternatives to p-values recently promoted by
several authors. We obtain several results on the representation of admissible
p-merging functions via e-values and on (in)admissibility of existing p-merging
functions. By introducing new admissible p-merging functions, we show that some
classic merging methods can be strictly improved to enhance power without
compromising validity under arbitrary dependence.
"
"stat.TH","  We propose a theoretical framework for the problem of learning a real-valued
function which meets fairness requirements. This framework is built upon the
notion of $\alpha$-relative (fairness) improvement of the regression function
which we introduce using the theory of optimal transport. Setting $\alpha = 0$
corresponds to the regression problem under the Demographic Parity constraint,
while $\alpha = 1$ corresponds to the classical regression problem without any
constraints. For $\alpha \in (0, 1)$ the proposed framework allows to
continuously interpolate between these two extreme cases and to study partially
fair predictors. Within this framework we precisely quantify the cost in risk
induced by the introduction of the fairness constraint. We put forward a
statistical minimax setup and derive a general problem-dependent lower bound on
the risk of any estimator satisfying $\alpha$-relative improvement constraint.
We illustrate our framework on a model of linear regression with Gaussian
design and systematic group-dependent bias, deriving matching (up to absolute
constants) upper and lower bounds on the minimax risk under the introduced
constraint. Finally, we perform a simulation study of the latter setup.
"
"stat.TH","  We propose a general framework for modelling network data that is designed to
describe aspects of non-exchangeable networks. Conditional on latent
(unobserved) variables, the edges of the network are generated by their finite
growth history (with latent orders) while the marginal probabilities of the
adjacency matrix are modeled by a generalization of a graph limit function (or
a graphon). In particular, we study the estimation, clustering and degree
behavior of the network in our setting. We determine (i) the minimax estimator
of a composite graphon with respect to squared error loss; (ii) that spectral
clustering is able to consistently detect the latent membership when the
block-wise constant composite graphon is considered under additional
conditions; and (iii) we are able to construct models with heavy-tailed
empirical degrees under specific scenarios and parameter choices. This explores
why and under which general conditions non-exchangeable network data can be
described by a stochastic block model. The new modelling framework is able to
capture empirically important characteristics of network data such as sparsity
combined with heavy tailed degree distribution, and add understanding as to
what generative mechanisms will make them arise.
  Keywords: statistical network analysis, exchangeable arrays, stochastic block
model, nonlinear stochastic processes.
"
"stat.TH","  The problem of sequentially detecting a moving anomaly which affects
different parts of a sensor network with time is studied. Each network sensor
is characterized by a non-anomalous and anomalous distribution, governing the
generation of sensor data. Initially, the observations of each sensor are
generated according to the corresponding non-anomalous distribution. After some
unknown but deterministic time instant, a moving anomaly emerges, affecting
different sets of sensors as time progresses. As a result, the observations of
the affected sensors are generated according to the corresponding anomalous
distribution. Our goal is to design a stopping procedure to detect the
emergence of the anomaly as quickly as possible, subject to constraints on the
frequency of false alarms. The problem is studied in a quickest change
detection framework where it is assumed that the evolution of the anomaly is
unknown but deterministic. To this end, we propose a modification of Lorden's
worst average detection delay metric to account for the trajectory of the
anomaly that maximizes the detection delay of a candidate detection procedure.
We establish that a Cumulative Sum-type test solves the resulting sequential
detection problem exactly when the sensors are homogeneous. For the case of
heterogeneous sensors, the proposed detection scheme can be modified to provide
a first-order asymptotically optimal algorithm. We conclude by presenting
numerical simulations to validate our theoretical analysis.
"
"stat.TH","  There is a well-known equivalence between avoiding accuracy dominance and
having probabilistically coherent credences (see, e.g., de Finetti 1974, Joyce
2009, Predd et al. 2009, Schervish et al. 2009, Pettigrew 2016). However, this
equivalence has been established only when the set of propositions on which
credence functions are defined is finite. In this paper, we establish
connections between accuracy dominance and coherence when credence functions
are defined on an infinite set of propositions. In particular, we establish the
necessary results to extend the classic accuracy argument for probabilism
originally due to Joyce (1998) to certain classes of infinite sets of
propositions including countably infinite partitions.
"
"stat.TH","  As in standard linear regression, in truncated linear regression, we are
given access to observations $(A_i, y_i)_i$ whose dependent variable equals
$y_i= A_i^{\rm T} \cdot x^* + \eta_i$, where $x^*$ is some fixed unknown vector
of interest and $\eta_i$ is independent noise; except we are only given an
observation if its dependent variable $y_i$ lies in some ""truncation set"" $S
\subset \mathbb{R}$. The goal is to recover $x^*$ under some favorable
conditions on the $A_i$'s and the noise distribution. We prove that there
exists a computationally and statistically efficient method for recovering
$k$-sparse $n$-dimensional vectors $x^*$ from $m$ truncated samples, which
attains an optimal $\ell_2$ reconstruction error of $O(\sqrt{(k \log n)/m})$.
As a corollary, our guarantees imply a computationally efficient and
information-theoretically optimal algorithm for compressed sensing with
truncation, which may arise from measurement saturation effects. Our result
follows from a statistical and computational analysis of the Stochastic
Gradient Descent (SGD) algorithm for solving a natural adaptation of the LASSO
optimization problem that accommodates truncation. This generalizes the works
of both: (1) [Daskalakis et al. 2018], where no regularization is needed due to
the low-dimensionality of the data, and (2) [Wainright 2009], where the
objective function is simple due to the absence of truncation. In order to deal
with both truncation and high-dimensionality at the same time, we develop new
techniques that not only generalize the existing ones but we believe are of
independent interest.
"
"stat.TH","  Quantum hypothesis testing is a central task in the entire field of quantum
information theory. Understanding its ultimate limits will give insight into a
wide range of quantum protocols and applications, from sensing to
communication. Although the limits of hypothesis testing between quantum states
have been completely clarified by the pioneering works of Helstrom in the 70s,
the more difficult problem of hypothesis testing with quantum channels, i.e.,
channel discrimination, is less understood. This is mainly due to the
complications coming from the use of input entanglement and the possibility of
employing adaptive strategies. In this paper, we establish a lower limit for
the ultimate error probability affecting the discrimination of an arbitrary
number of quantum channels. We also show that this lower bound is achievable
when the channels have certain symmetries. As an example, we apply our results
to the problem of channel position finding, where the goal is to identify the
location of a target channel among multiple background channels. In this
general setting, we find that the use of entanglement offers a great advantage
over strategies without entanglement, with non-trivial implications for data
readout, target detection and quantum spectroscopy.
"
"stat.TH","  This paper is devoted to two different two-time-scale stochastic
approximation algorithms for superquantile estimation. We shall investigate the
asymptotic behavior of a Robbins-Monro estimator and its convexified version.
Our main contribution is to establish the almost sure convergence, the
quadratic strong law and the law of iterated logarithm for our estimates via a
martingale approach. A joint asymptotic normality is also provided. Our
theoretical analysis is illustrated by numerical experiments on real datasets.
"
"stat.TH","  Cokriging is the common method of spatial interpolation (best linear unbiased
prediction) in multivariate geostatistics. While best linear prediction has
been well understood in univariate spatial statistics, the literature for the
multivariate case has been elusive so far. The new challenges provided by
modern spatial datasets, being typically multivariate, call for a deeper study
of cokriging. In particular, we deal with the problem of misspecified cokriging
prediction within the framework of fixed domain asymptotics. Specifically, we
provide conditions for equivalence of measures associated with multivariate
Gaussian random fields, with index set in a compact set of a d-dimensional
Euclidean space. Such conditions have been elusive for over about 50 years of
spatial statistics.
  We then focus on the multivariate Mat\'ern and Generalized Wendland classes
of matrix valued covariance functions, that have been very popular for having
parameters that are crucial to spatial interpolation, and that control the mean
square differentiability of the associated Gaussian process. We provide
sufficient conditions, for equivalence of Gaussian measures, relying on the
covariance parameters of these two classes. This enables to identify the
parameters that are crucial to asymptotically equivalent interpolation in
multivariate geostatistics. Our findings are then illustrated through
simulation studies.
"
"stat.TH","  This paper investigates noisy graph-based semi-supervised learning or
community detection. We consider the Stochastic Block Model (SBM), where, in
addition to the graph observation, an oracle gives a non-perfect information
about some nodes' cluster assignment. We derive the Maximum A Priori (MAP)
estimator, and show that a continuous relaxation of the MAP performs almost
exact recovery under non-restrictive conditions on the average degree and
amount of oracle noise. In particular, this method avoids some pitfalls of
several graph-based semi-supervised learning methods such as the flatness of
the classification functions, appearing in the problems with a very large
amount of unlabeled data.
"
"stat.TH","  Ratios of central order statistics seem to be very useful for estimating the
tail of the distributions and therefore, quantiles outside the range of the
data. In 1995 Isabel Fraga Alves investigated the rate of convergence of three
semi-parametric estimators of the parameter of the tail index in case when the
cumulative distribution function of the observed random variable belongs to the
max-domain of attraction of a fixed Generalized Extreme Value Distribution.
They are based on ratios of specific linear transformations of two extreme
order statistics. In 2019 we considered Pareto case and found two very simple
and unbiased estimators of the index of regular variation. Then, using the
central order statistics we showed that these estimators have many good
properties. Then, we observed that although the assumptions are different, one
of them is equivalent to one of Alves's estimators. Using central order
statistics we proved unbiasedness, asymptotic consistency, asymptotic normality
and asymptotic efficiency. Here we use again central order statistics and a
parametric approach and obtain distribution sensitive estimators of the index
of regular variation in some particular cases. Then, we find conditions which
guarantee that these estimators are unbiased, consistent and asymptotically
normal. The results are depicted via simulation study.
"
"stat.TH","  In the present paper we consider design criteria which depend on several
designs simultaneously. We formulate equivalence theorems based on moment
matrices (if criteria depend on designs via moment matrices) or with respect to
the designs themselves (for finite design regions). We apply the obtained
optimality conditions to the multiple-group random coefficient regression
models and illustrate the results by simple examples.
"
"stat.TH","  In this paper, we compute doubly truncated moments for the selection
elliptical (SE) class of distributions, which includes some multivariate
asymmetric versions of well-known elliptical distributions, such as, the
normal, Student's t, slash, among others. We address the moments for doubly
truncated members of this family, establishing neat formulation for high order
moments as well as for its first two moments. We establish sufficient and
necessary conditions for the existence of these truncated moments. Further, we
propose optimized methods able to deal with extreme setting of the parameters,
partitions with almost zero volume or no truncation which are validated with a
brief numerical study. Finally, we present some results useful in interval
censoring models. All results has been particularized to the unified skew-t
(SUT) distribution, a complex multivariate asymmetric heavy-tailed distribution
which includes the extended skew-t (EST), extended skew-normal (ESN), skew-t
(ST) and skew-normal (SN) distributions as particular and limiting cases.
"
"stat.TH","  We consider stochastic systems of interacting particles or agents, with
dynamics determined by an interaction kernel which only depends on pairwise
distances. We study the problem of inferring this interaction kernel from
observations of the positions of the particles, in either continuous or
discrete time, along multiple independent trajectories. We introduce a
nonparametric inference approach to this inverse problem, based on a
regularized maximum likelihood estimator constrained to suitable hypothesis
spaces adaptive to data. We show that a coercivity condition enables us to
control the condition number of this problem and prove the consistency of our
estimator, and that in fact it converges at a near-optimal learning rate, equal
to the min-max rate of $1$-dimensional non-parametric regression. In
particular, this rate is independent of the dimension of the state space, which
is typically very high. We also analyze the discretization errors in the case
of discrete-time observations, showing that it is of order $1/2$ in terms of
the time gaps between observations. This term, when large, dominates the
sampling error and the approximation error, preventing convergence of the
estimator. Finally, we exhibit an efficient parallel algorithm to construct the
estimator from data, and we demonstrate the effectiveness of our algorithm with
numerical tests on prototype systems including stochastic opinion dynamics and
a Lennard-Jones model.
"
"stat.TH","  We study the problem of high-dimensional covariance estimation under the
constraint that the partial correlations are nonnegative. The sign constraints
dramatically simplify estimation: the Gaussian maximum likelihood estimator is
well defined with only two observations regardless of the number of variables.
We analyze its performance in the setting where the dimension may be much
larger than the sample size. We establish that the estimator is both
high-dimensionally consistent and minimax optimal in the symmetrized Stein
loss. We also prove a negative result which shows that the sign-constraints can
introduce substantial bias for estimating the top eigenvalue of the covariance
matrix.
"
"stat.TH","  In this paper we present a parametric estimation method for certain
multi-parameter heavy-tailed L\'evy-driven moving averages. The theory relies
on recent multivariate central limit theorems obtained in [3] via Malliavin
calculus on Poisson spaces. Our minimal contrast approach is related to the
papers [14, 15], which propose to use the marginal empirical characteristic
function to estimate the one-dimensional parameter of the kernel function and
the stability index of the driving L\'evy motion. We extend their work to allow
for a multi-parametric framework that in particular includes the important
examples of the linear fractional stable motion, the stable Ornstein-Uhlenbeck
process, certain CARMA(2, 1) models and Ornstein-Uhlenbeck processes with a
periodic component among other models. We present both the consistency and the
associated central limit theorem of the minimal contrast estimator.
Furthermore, we demonstrate numerical analysis to uncover the finite sample
performance of our method.
"
"stat.TH","  In a linear model with possibly many predictors, we consider variable
selection procedures given by $$ \{1\leq j\leq p: |\widehat{\beta}_j(\lambda)|
> t\}, $$ where $\widehat{\beta}(\lambda)$ is the Lasso estimate of the
regression coefficients, and where $\lambda$ and $t$ may be data dependent.
Ordinary Lasso selection is captured by using $t=0$, thus allowing to control
only $\lambda$, whereas thresholded-Lasso selection allows to control both
$\lambda$ and $t$. The potential advantages of the latter over the former in
terms of power---figuratively, opening up the possibility to look further down
the Lasso path---have been quantified recently leveraging advances in
approximate message-passing (AMP) theory, but the implications are actionable
only when assuming substantial knowledge of the underlying signal.
  In this work we study theoretically the power of a knockoffs-calibrated
counterpart of thresholded-Lasso that enables us to control FDR in the
realistic situation where no prior information about the signal is available.
Although the basic AMP framework remains the same, our analysis requires a
significant technical extension of existing theory in order to handle the
pairing between original variables and their knockoffs. Relying on this
extension we obtain exact asymptotic predictions for the true positive
proportion achievable at a prescribed type I error level. In particular, we
show that the knockoffs version of thresholded-Lasso can perform much better
than ordinary Lasso selection if $\lambda$ is chosen by cross-validation on the
augmented matrix.
"
"stat.TH","  Random forest (RF) is one of the most popular methods for estimating
regression functions. The local nature of the RF algorithm, based on intra-node
means and variances, is ideal when errors are i.i.d. For dependent error
processes like time series and spatial settings where data in all the nodes
will be correlated, operating locally ignores this dependence. Also, RF will
involve resampling of correlated data, violating the principles of bootstrap.
Theoretically, consistency of RF has been established for i.i.d. errors, but
little is known about the case of dependent errors.
  We propose RF-GLS, a novel extension of RF for dependent error processes in
the same way Generalized Least Squares (GLS) fundamentally extends Ordinary
Least Squares (OLS) for linear models under dependence. The key to this
extension is the equivalent representation of the local decision-making in a
regression tree as a global OLS optimization which is then replaced with a GLS
loss to create a GLS-style regression tree. This also synergistically addresses
the resampling issue, as the use of GLS loss amounts to resampling uncorrelated
contrasts (pre-whitened data) instead of the correlated data. For spatial
settings, RF-GLS can be used in conjunction with Gaussian Process correlated
errors to generate kriging predictions at new locations. RF becomes a special
case of RF-GLS with an identity working covariance matrix.
  We establish consistency of RF-GLS under beta- (absolutely regular) mixing
error processes and show that this general result subsumes important cases like
autoregressive time series and spatial Matern Gaussian Processes. As a
byproduct, we also establish consistency of RF for beta-mixing processes, which
to our knowledge, is the first such result for RF under dependence.
  We empirically demonstrate the improvement achieved by RF-GLS over RF for
both estimation and prediction under dependence.
"
"stat.TH","  Extending rank-based inference to a multivariate setting such as
multiple-output regression or MANOVA with unspecified d-dimensional error
density has remained an open problem for more than half a century. None of the
many solutions proposed so far is enjoying the combination of
distribution-freeness and efficiency that makes rank-based inference a
successful tool in the univariate setting. A concept of center-outward
multivariate ranks and signs based on measure transportation ideas has been
introduced recently. Center-outward ranks and signs are not only
distribution-free but achieve in dimension d > 1 the (essential) maximal
ancillarity property of traditional univariate ranks, hence carry all the
""distribution-free information"" available in the sample. We derive here the
H\'ajek representation and asymptotic normality results required in the
construction of center-outward rank tests for multiple-output regression and
MANOVA. When based on appropriate spherical scores, these fully
distribution-free tests achieve parametric efficiency in the corresponding
models.
"
"stat.TH","  A two-class mixture model, where the density of one of the components is
known, is considered. We address the issue of the nonparametric adaptive
estimation of the unknown probability density of the second component. We
propose a randomly weighted kernel estimator with a fully data-driven bandwidth
selection method, in the spirit of the Goldenshluger and Lepski method. An
oracle-type inequality for the pointwise quadratic risk is derived as well as
convergence rates over Holder smoothness classes. The theoretical results are
illustrated by numerical simulations.
"
"stat.TH","  We study the problem of outlier robust high-dimensional mean estimation under
a finite covariance assumption, and more broadly under finite low-degree moment
assumptions. We consider a standard stability condition from the recent robust
statistics literature and prove that, except with exponentially small failure
probability, there exists a large fraction of the inliers satisfying this
condition. As a corollary, it follows that a number of recently developed
algorithms for robust mean estimation, including iterative filtering and
non-convex gradient descent, give optimal error estimators with
(near-)subgaussian rates. Previous analyses of these algorithms gave
significantly suboptimal rates. As a corollary of our approach, we obtain the
first computationally efficient algorithm with subgaussian rate for
outlier-robust mean estimation in the strong contamination model under a finite
covariance assumption.
"
"stat.TH","  We introduce the I-prior methodology as a unifying framework for estimating a
variety of regression models, including varying coefficient, multilevel,
longitudinal models, and models with functional covariates and responses. It
can also be used for multi-class classification, with low or high dimensional
covariates.
  The I-prior is generally defined as a maximum entropy prior. For a regression
function, the I-prior is Gaussian with covariance kernel proportional to the
Fisher information on the regression function, which is estimated by its
posterior distribution under the I-prior. The I-prior has the intuitively
appealing property that the more information is available on a linear
functional of the regression function, the larger the prior variance, and the
smaller the influence of the prior mean on the posterior distribution.
  Advantages compared to competing methods, such as Gaussian process regression
or Tikhonov regularization, are ease of estimation and model comparison. In
particular, we develop an EM algorithm with a simple E and M step for
estimating hyperparameters, facilitating estimation for complex models. We also
propose a novel parsimonious model formulation, requiring a single scale
parameter for each (possibly multidimensional) covariate and no further
parameters for interaction effects. This simplifies estimation because fewer
hyperparameters need to be estimated, and also simplifies model comparison of
models with the same covariates but different interaction effects; in this
case, the model with the highest estimated likelihood can be selected.
  Using a number of widely analyzed real data sets we show that predictive
performance of our methodology is competitive. An R-package implementing the
methodology is available (Jamil, 2019).
"
"stat.TH","  We study the problem of estimating the mean of a distribution in high
dimensions when either the samples are adversarially corrupted or the
distribution is heavy-tailed. Recent developments in robust statistics have
established efficient and (near) optimal procedures for both settings. However,
the algorithms developed on each side tend to be sophisticated and do not
directly transfer to the other, with many of them having ad-hoc or complicated
analyses.
  In this paper, we provide a meta-problem and a duality theorem that lead to a
new unified view on robust and heavy-tailed mean estimation in high dimensions.
We show that the meta-problem can be solved either by a variant of the Filter
algorithm from the recent literature on robust estimation or by the quantum
entropy scoring scheme (QUE), due to Dong, Hopkins and Li (NeurIPS '19). By
leveraging our duality theorem, these results translate into simple and
efficient algorithms for both robust and heavy-tailed settings. Furthermore,
the QUE-based procedure has run-time that matches the fastest known algorithms
on both fronts.
  Our analysis of Filter is through the classic regret bound of the
multiplicative weights update method. This connection allows us to avoid the
technical complications in previous works and improve upon the run-time
analysis of a gradient-descent-based algorithm for robust mean estimation by
Cheng, Diakonikolas, Ge and Soltanolkotabi (ICML '20).
"
"stat.TH","  We study the bootstrap for the maxima of the sums of independent random
variables, a problem of high relevance to many applications in modern
statistics. Since the consistency of bootstrap was justified by Gaussian
approximation in Chernozhukov et al. (2013), quite a few attempts have been
made to sharpen the error bound for bootstrap and reduce the sample size
requirement for bootstrap consistency. In this paper, we show that the sample
size requirement can be dramatically improved when we make the inference
slightly conservative, that is, to inflate the bootstrap quantile
$t_{\alpha}^*$ by a small fraction, e.g. by $1\%$ to $1.01\,t^*_\alpha$. This
simple procedure yields error bounds for the coverage probability of
conservative bootstrap at as fast a rate as $\sqrt{(\log p)/n}$ under suitable
conditions, so that not only the sample size requirement can be reduced to
$\log p \ll n$ but also the overall convergence rate is nearly parametric.
Furthermore, we improve the error bound for the coverage probability of the
standard non-conservative bootstrap to $[(\log (np))^3 (\log p)^2/n]^{1/4}$
under general assumptions on data. These results are established for the
empirical bootstrap and the multiplier bootstrap with third moment match. An
improved coherent Lindeberg interpolation method, originally proposed in Deng
and Zhang (2017), is developed to derive sharper comparison bounds, especially
for the maxima.
"
"stat.TH","  Bayesian inference and uncertainty quantification in a general class of
non-linear inverse regression models is considered. Analytic conditions on the
regression model $\{\mathscr G(\theta): \theta \in \Theta\}$ and on Gaussian
process priors for $\theta$ are provided such that semi-parametrically
efficient inference is possible for a large class of linear functionals of
$\theta$. A general semi-parametric Bernstein-von Mises theorem is proved that
shows that the (non-Gaussian) posterior distributions are approximated by
certain Gaussian measures centred at the posterior mean. As a consequence
posterior-based credible sets are shown to be valid and optimal from a
frequentist point of view. The theory is demonstrated to cover two prototypical
applications with PDEs that arise in non-linear tomography problems: the first
concerns an elliptic inverse problem for the Schr\""odinger equation, and the
second the inversion of non-Abelian $X$-ray transforms. New PDE techniques are
developed to show that the relevant Fisher information operators are invertible
between suitable function spaces.
"
"stat.TH","  Several regularization methods have been considered over the last decade for
sparse high-dimensional linear regression models, but the most common ones use
the least square (quadratic) or likelihood loss and hence are not robust
against data contamination. Some authors have overcome the problem of
non-robustness by considering suitable loss function based on divergence
measures (e.g., density power divergence, gamma-divergence, etc.) instead of
the quadratic loss. In this paper we shall consider a loss function based on
the R\'enyi's pseudodistance jointly with non-concave penalties in order to
simultaneously perform variable selection and get robust estimators of the
parameters in a high-dimensional linear regression model of non-polynomial
dimensionality. The desired oracle properties of our proposed method are
derived theoretically and its usefulness is illustustrated numerically through
simulations and real data examples.
"
"stat.TH","  In high-dimensions, the prior tails can have a significant effect on both
posterior computation and asymptotic concentration rates. To achieve optimal
rates while keeping the posterior computations relatively simple, an empirical
Bayes approach has recently been proposed, featuring thin-tailed conjugate
priors with data-driven centers. While conjugate priors ease some of the
computational burden, Markov chain Monte Carlo methods are still needed, which
can be expensive when dimension is high. In this paper, we develop a
variational approximation to the empirical Bayes posterior that is fast to
compute and retains the optimal concentration rate properties of the original.
In simulations, our method is shown to have superior performance compared to
existing variational approximations in the literature across a wide range of
high-dimensional settings.
"
"stat.TH","  In this paper, we establish the almost sure convergence of two-timescale
stochastic gradient descent algorithms in continuous time under general noise
and stability conditions, extending well known results in discrete time. We
analyse algorithms with additive noise and those with non-additive noise. In
the non-additive case, our analysis is carried out under the assumption that
the noise is a continuous-time Markov process, controlled by the algorithm
states. The algorithms we consider can be applied to a broad class of bilevel
optimisation problems. We study one such problem in detail, namely, the problem
of joint online parameter estimation and optimal sensor placement for a
partially observed diffusion process. We demonstrate how this can be formulated
as a bilevel optimisation problem, and propose a solution in the form of a
continuous-time, two-timescale, stochastic gradient descent algorithm.
Furthermore, under suitable conditions on the latent signal, the filter, and
the filter derivatives, we establish almost sure convergence of the online
parameter estimates and optimal sensor placements to the stationary points of
the asymptotic log-likelihood and asymptotic filter covariance, respectively.
We also provide numerical examples, illustrating the application of the
proposed methodology to a partially observed Bene\v{s} equation, and a
partially observed stochastic advection-diffusion equation.
"
"stat.TH","  Marginal polytopes are important geometric objects that arise in statistics
as the polytopes underlying hierarchical log-linear models. These polytopes can
be used to answer geometric questions about these models, such as determining
the existence of maximum likelihood estimates or the normality of the
associated semigroup. Cut polytopes of graphs have been useful in analyzing
binary marginal polytopes in the case where the simplicial complex underlying
the hierarchical model is a graph. We introduce a generalized cut polytope that
is isomorphic to the binary marginal polytope of an arbitrary simplicial
complex via a generalized covariance map. This polytope is full dimensional in
its ambient space and has a natural switching operation among its facets that
can be used to deduce symmetries between the facets of the correlation and
binary marginal polytopes. We find complete H-representations of the
generalized cut polytope for some important families of simplicial complexes.
We also compute the volume of these polytopes in some instances.
"
"stat.TH","  We introduce a new class of multivariate elliptically symmetric distributions
including elliptically symmetric logistic distributions and Kotz type
distributions. We investigate the various probabilistic properties including
marginal distributions, conditional distributions, linear transformations,
characteristic functions and dependence measure in the perspective of the
inconsistency property. In addition, we provide a real data example to show
that the new distributions have reasonable flexibility.
"
"stat.TH","  Sparse Bayesian learning models are typically used for prediction in datasets
with significantly greater number of covariates than observations. Among the
class of sparse Bayesian learning models, relevance vector machines (RVM) is
very popular. Its popularity is demonstrated by a large number of citations of
the original RVM paper of Tipping (2001)[JMLR, 1, 211 - 244]. In this article
we show that RVM and some other sparse Bayesian learning models with
hyperparameter values currently used in the literature are based on improper
posteriors. Further, we also provide necessary and sufficient conditions for
posterior propriety of RVM.
"
"stat.TH","  Finding the best model that describes a high dimensional dataset, is a
daunting task. For binary data, we show that this becomes feasible, if the
search is restricted to simple models. These models -- that we call Minimally
Complex Models (MCMs) -- are simple because they are composed of independent
components of minimal complexity, in terms of description length. Simple models
are easy to infer and to sample from. In addition, model selection within the
MCMs' class is invariant with respect to changes in the representation of the
data. They portray the structure of dependencies among variables in a simple
way. They provide robust predictions on dependencies and symmetries, as
illustrated in several examples. MCMs may contain interactions between
variables of any order. So, for example, our approach reveals whether a dataset
is appropriately described by a pairwise interaction model.
"
"stat.TH","  Optimality results for two outstanding Bayesian estimation problems are given
in this paper: the estimation of the sampling distribution for the squared
total variation function and the estimation of the density for the
$L^1$-squared loss function. The posterior predictive distribution provides the
solution to these problems. Some examples are presented to illustrate it. The
Bayesian estimation problem of a distribution function is also addressed.
"
"stat.TH","  In this paper we present new theoretical results for the Dantzig and Lasso
estimators of the drift in a high dimensional Ornstein-Uhlenbeck model under
sparsity constraints. Our focus is on oracle inequalities for both estimators
and error bounds with respect to several norms. In the context of the Lasso
estimator our paper is strongly related to [11], who investigated the same
problem under row sparsity. We improve their rates and also prove the
restricted eigenvalue property solely under ergodicity assumption on the model.
Finally, we demonstrate a numerical analysis to uncover the finite sample
performance of the Dantzig and Lasso estimators.
"
"stat.TH","  The logrank test is a well-known nonparametric test which is often used to
compare the survival distributions of two samples including right censored
observations, it is also known as the Mantel-Haenszel test. The $G^{\rho}$
family of tests, introduced by Harrington and Fleming (1982), generalizes the
logrank test by using weights assigned to observations. In this paper, we
present a monotonicity property for the $G^{\rho}$ family of tests, which was
motivated by the need to derive bounds for the test statistic in case of
imprecise data observations.
"
"stat.TH","  One of the fundamental problems in Bayesian statistics is the approximation
of the posterior distribution. Gibbs sampler and coordinate ascent variational
inference are renownedly utilized approximation techniques that rely on
stochastic and deterministic approximations. This article aims to clarify the
set-theoretical point of view on the two schemes and provide some insights for
them.
"
"stat.TH","  This paper explores the generalization loss of linear regression in variably
parameterized families of models, both under-parameterized and
over-parameterized. We show that the generalization curve can have an arbitrary
number of peaks, and moreover, locations of those peaks can be explicitly
controlled. Our results highlight the fact that both classical U-shaped
generalization curve and the recently observed double descent curve are not
intrinsic properties of the model family. Instead, their emergence is due to
the interaction between the properties of the data and the inductive biases of
learning algorithms.
"
"stat.TH","  Certain monotonicity properties of the Poisson approximation to the binomial
distribution are established. As a natural application of these results, exact
(rather than approximate) tests of hypotheses on an unknown value of the
parameter $p$ of the binomial distribution are presented.
"
"stat.TH","  We establish that Laplace transforms of the posterior Dirichlet process
converge to those of the limiting Brownian bridge process in a neighbourhood
about zero, uniformly over Glivenko-Cantelli function classes. For real-valued
random variables and functions of bounded variation, we strengthen this result
to hold for all real numbers. This last result is proved via an explicit strong
approximation coupling inequality.
"
"stat.TH","  The purpose of this note is to point out that the theory of expander graphs
leads to an interesting test whether $n$ real numbers $x_1, \dots, x_n$ could
be $n$ independent samples of a random variable. To any distinct, real numbers
$x_1, \dots, x_n$, we associate a 4-regular graph $G$ as follows: using $\pi$
to denote the permutation ordering the elements, $x_{\pi(1)} < x_{\pi(2)} <
\dots < x_{\pi(n)}$, we build a graph on $\left\{1, \dots, n\right\}$ by
connecting $i$ and $i+1$ (cyclically) and $\pi(i)$ and $\pi(i+1)$ (cyclically).
If the numbers are i.i.d. samples, then a result of Friedman implies that $G$
is close to Ramanujan. This suggests a test for whether these numbers are
i.i.d: compute the second largest (in absolute value) eigenvalue of the
adjacency matrix. The larger $\lambda - 2\sqrt{3}$, the less likely it is for
the numbers to be i.i.d. We explain why this is a reasonable test and give many
examples.
"
"stat.TH","  Shape restrictions such as monotonicity on functions often arise naturally in
statistical modeling.
  We consider a Bayesian approach to the problem of estimation of a monotone
regression function and testing for monotonicity. We construct a prior
distribution using piecewise constant functions. For estimation, a prior
imposing monotonicity of the heights of these steps is sensible, but the
resulting posterior is harder to analyze theoretically. We consider a
``projection-posterior'' approach, where a conjugate normal prior is used, but
the monotonicity constraint is imposed on posterior samples by a projection map
on the space of monotone functions. We show that the resulting posterior
contracts at the optimal rate $n^{-1/3}$ under the $L_1$-metric and at a nearly
optimal rate under the empirical $L_p$-metrics for $0<p\le 2$. The
projection-posterior approach is also computationally more convenient. We also
construct a Bayesian test for the hypothesis of monotonicity using the
posterior probability of a shrinking neighborhood of the set of monotone
functions. We show that the resulting test has a universal consistency property
and obtain the separation rate which ensures that the resulting power function
approaches one.
"
"stat.TH","  We consider a set of points sampled from an unknown probability measure on a
Euclidean space, each of which points belongs to one of the finitely many
classes. We study the question of querying the class label at a very small
number of judiciously chosen points so as to be able to attach the appropriate
class label to every point in the set. Our approach is to consider the unknown
probability measure as a convex combination of the conditional probabilities
for each class. Our technique involves the use of a highly localized kernel
constructed from Hermite polynomials, and use them to create a hierarchical
estimate of the supports of the constituent probability measures. We do not
need to make any assumptions on the nature of any of the probability measures
nor know in advance the number of classes involved. We give theoretical
guarantees measured by the $F$-score for our classification scheme. Examples
include classification in hyper-spectral images, separation of distributions,
and MNIST classification.
"
"stat.TH","  Latent Dirichlet allocation (LDA) obtains essential information from data by
using Bayesian inference. It is applied to knowledge discovery via dimension
reducing and clustering in many fields. However, its generalization error had
not been yet clarified since it is a singular statistical model where there is
no one to one map from parameters to probability distributions. In this paper,
we give the exact asymptotic form of its generalization error and marginal
likelihood, by theoretical analysis of its learning coefficient using algebraic
geometry. The theoretical result shows that the Bayesian generalization error
in LDA is expressed in terms of that in matrix factorization and a penalty from
the simplex restriction of LDA's parameter region.
"
"stat.TH","  This paper studies the Schatten-$q$ error of low-rank matrix estimation by
singular value decomposition under perturbation. Specifically, we establish a
tight perturbation bound on the low-rank matrix estimation via a perturbation
projection error bound. This new proof technique has provable advantages over
the classic approaches. Then, we establish lower bounds to justify the
tightness of the upper bound on the low-rank matrix estimation error. Based on
the matrix perturbation projection error bound, we further develop a unilateral
and a user-friendly sin$\Theta$ bound for singular subspace perturbation.
Finally, we demonstrate the advantage of our results over the ones in the
literature by simulation.
"
"stat.TH","  We show that a simple community detection algorithm originated from
stochastic blockmodel literature achieves consistency, and even optimality, for
a broad and flexible class of sparse latent space models. The class of models
includes latent eigenmodels (arXiv:0711.1146). The community detection
algorithm is based on spectral clustering followed by local refinement via
normalized edge counting.
"
"stat.TH","  In this article, a new natural discrete analog of the one parameter
polynomial exponential(OPPE) distribution as a mixture of a number of negative
binomial distributions has been proposed and is called as a natural discrete
one parameter polynomial exponential (NDOPPE) distribution. This distribution
is a generalized version of natural discrete Lindley (NDL) distribution,
proposed and studied by Ahmed and Afify (2019). Two estimators viz., MLE and
UMVUE of the PMF and the CDF of a NDOPPE distribution have been derived. The
estimators have been compared with respect to their MSEs. Simulation study has
been conducted to verify the consistency of the estimators. A real data
illustration has been reported.
"
"stat.TH","  We investigate the effectiveness of convex relaxation and nonconvex
optimization in solving bilinear systems of equations (a.k.a. blind
deconvolution under a subspace model). Despite the wide applicability, the
theoretical understanding about these two paradigms remains largely inadequate
in the presence of noise. The current paper makes two contributions by
demonstrating that: (1) convex relaxation achieves minimax-optimal statistical
accuracy vis-\`a-vis random noise, and (2) a two-stage nonconvex algorithm
attains minimax-optimal accuracy within a logarithmic number of iterations.
Both results improve upon the state-of-the-art results by some factors that
scale polynomially in the problem dimension.
"
"stat.TH","  This article develops a periodic version of a time varying parameter
fractional process in the stationary region. It is a partial extension of
Hosking (1981)'s article which dealt with the case where the coefficients are
invariant in time. We will describe the probabilistic theories of this periodic
model. The results are followed by a graphical representation of the
autocovariances functions.
"
"stat.TH","  We provide the analytic forms of the distributions for the sum of ordered
spacings. We do this both for the case where the boundaries are included in the
calculation of the spacings and the case where they are excluded. Both the
probability densities as well as their cumulatives are provided. These results
will have useful applications in the physical sciences and possibly elsewhere.
"
"stat.TH","  Tensor, also known as multi-dimensional array, arises from many applications
in signal processing, manufacturing processes, healthcare, among others. As one
of the most popular methods in tensor literature, Robust tensor principal
component analysis (RTPCA) is a very effective tool to extract the low rank and
sparse components in tensors. In this paper, a new method to analyze RTPCA is
proposed based on the recently developed tensor-tensor product and tensor
singular value decomposition (t-SVD). Specifically, it aims to solve a convex
optimization problem whose objective function is a weighted combination of the
tensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact
recovery is built on the tensor incoherence conditions and the assumption of a
uniform model on the sparse support. Unlike this conventional way, in this
paper, without any assumption of randomness, the exact recovery can be achieved
in a completely deterministic fashion by characterizing the tensor
rank-sparsity incoherence, which is an uncertainty principle between the
low-rank tensor spaces and the pattern of sparse tensor.
"
"stat.TH","  One fundamental goal of high-dimensional statistics is to detect or recover
structure from noisy data. In many cases, the data can be faithfully modeled by
a planted structure (such as a low-rank matrix) perturbed by random noise. But
even for these simple models, the computational complexity of estimation is
sometimes poorly understood. A growing body of work studies low-degree
polynomials as a proxy for computational complexity: it has been demonstrated
in various settings that low-degree polynomials of the data can match the
statistical performance of the best known polynomial-time algorithms for
detection. While prior work has studied the power of low-degree polynomials for
the task of detecting the presence of hidden structures, it has failed to
address the estimation problem in settings where detection is qualitatively
easier than estimation.
  In this work, we extend the method of low-degree polynomials to address
problems of estimation and recovery. For a large class of ""signal plus noise""
problems, we give a user-friendly lower bound for the best possible mean
squared error achievable by any degree-D polynomial. To our knowledge, this is
the first instance in which the low-degree polynomial method can establish
low-degree hardness of recovery problems where the associated detection problem
is easy. As applications, we give a tight characterization of the low-degree
minimum mean squared error for the planted submatrix and planted dense subgraph
problems, resolving (in the low-degree framework) open problems about the
computational complexity of recovery in both cases.
"
"stat.TH","  In this paper, we develop novel perturbation bounds for the high-order
orthogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we
establish blockwise tensor perturbation bounds for HOOI with guarantees for
both tensor reconstruction in Hilbert-Schmidt norm $\|\widehat{\bcT} - \bcT
\|_{\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\|
\sin \Theta (\widehat{\U}_k, \U_k) \|_q$ for any $q \geq 1$. We show the upper
bounds of mode-$k$ singular subspace estimation are unilateral and converge
linearly to a quantity characterized by blockwise errors of the perturbation
and signal strength. For the tensor reconstruction error bound, we express the
bound through a simple quantity $\xi$, which depends only on perturbation and
the multilinear rank of the underlying signal. Rate matching deterministic
lower bound for tensor reconstruction, which demonstrates the optimality of
HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI
with only a single iteration) is also optimal in terms of tensor reconstruction
and can be used to lower the computational cost. The perturbation results are
also extended to the case that only partial modes of $\bcT$ have low-rank
structure. We support our theoretical results by extensive numerical studies.
Finally, we apply the novel perturbation bounds of HOOI on two applications,
tensor denoising and tensor co-clustering, from machine learning and
statistics, which demonstrates the superiority of the new perturbation results.
"
"stat.TH","  A well-known difficult problem regarding Metropolis-Hastings algorithms is to
get sharp bounds on their convergence rates. Moreover, different
initializations may have different convergence rates so a uniform upper bound
may be too conservative to be used in practice. In this paper, we study the
convergence properties of the Independent Metropolis-Hastings (IMH) algorithms
on both general and discrete state spaces. Under mild conditions, we derive the
exact convergence rate and prove that different initializations of the IMH
algorithm have the same convergence rate. In particular, we get the exact
convergence speed for IMH algorithms on general state spaces under certain
conditions. Connections with the Random Walk Metropolis-Hastings (RWMH)
algorithm are also discussed, which solve the conjecture proposed by Atchade
and Perron using a counterexample.
"
"stat.TH","  In this paper we study asymptotic properties of random forests within the
framework of nonlinear time series modeling. While random forests have been
successfully applied in various fields, the theoretical justification has not
been considered for their use in a time series setting. Under mild conditions,
we prove a uniform concentration inequality for regression trees built on
nonlinear autoregressive processes and, subsequently, we use this result to
prove consistency for a large class of random forests. The results are
supported by various simulations.
"
"stat.TH","  We study the approximation of two-layer compositions $f(x) = g(\phi(x))$ via
deep ReLU networks, where $\phi$ is a nonlinear, geometrically intuitive, and
dimensionality reducing feature map. We focus on two complementary choices for
$\phi$ that are intuitive and frequently appearing in the statistical
literature. The resulting approximation rates are near optimal and show
adaptivity to intrinsic notions of complexity, which significantly extend a
series of recent works on approximating targets over low-dimensional manifolds.
Specifically, we show that ReLU nets can express functions, which are invariant
to the input up to an orthogonal projection onto a low-dimensional manifold,
with the same efficiency as if the target domain would be the manifold itself.
This implies approximation via ReLU nets is faithful to an intrinsic
dimensionality governed by the target $f$ itself, rather than the
dimensionality of the approximation domain. As an application of our
approximation bounds, we study empirical risk minimization over a space of
sparsely constrained ReLU nets under the assumption that the conditional
expectation satisfies one of the proposed models. We show near-optimal
estimation guarantees in regression and classifications problems, for which, to
the best of our knowledge, no efficient estimator has been developed so far.
"
"stat.TH","  The Mat{\'e}rn family of covariance functions has played a central role in
spatial statistics for decades, being a flexible parametric class with one
parameter determining the smoothness of the paths of the underlying spatial
field.
  This paper proposes a new family of spatial covariance functions, which stems
from a reparameterization of the generalized Wendland family. As for the
Mat{\'e}rn case, the new class allows for a continuous parameterization of the
smoothness of the underlying Gaussian random field, being additionally
compactly supported.
  More importantly, we show that the proposed covariance family generalizes the
Mat{\'e}rn model which is attained as a special limit case. The practical
implication of our theoretical results questions the effective flexibility of
the Mat{\'e}rn covariance from modeling and computational viewpoints.
  Our numerical experiments elucidate the speed of convergence of the proposed
model to the Mat{\'e}rn model. We also inspect the level of sparseness of the
associated (inverse) covariance matrix and the asymptotic distribution of the
maximum likelihood estimator under increasing and fixed domain asymptotics. The
effectiveness of our proposal is illustrated by analyzing a georeferenced
dataset on maximum temperatures over the southeastern United States, and
performing a re-analysis of a large spatial point referenced dataset of yearly
total precipitation anomalies
"
"stat.TH","  The present paper, characterizes the invertibility and causality conditions
of a periodic ARFIMA (PARFIMA) models. We first, discuss the conditions in the
multivariate case, by considering the corresponding p-variate stationary ARFIMA
models. Second, we construct the conditions using the univariate case and we
deduce a new infinite autoregressive representation for the PARFIMA model, the
results are investigated through a simulation study.
"
"stat.TH","  We propose a novel stochastic network model, called Fractal Gaussian Network
(FGN), that embodies well-defined and analytically tractable fractal
structures. Such fractal structures have been empirically observed in diverse
applications. FGNs interpolate continuously between the popular purely random
geometric graphs (a.k.a. the Poisson Boolean network), and random graphs with
increasingly fractal behavior. In fact, they form a parametric family of sparse
random geometric graphs that are parametrized by a fractality parameter $\nu$
which governs the strength of the fractal structure. FGNs are driven by the
latent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical
model of fractality in its own right. We asymptotically characterize the
expected number of edges and triangle in FGNs. We then examine the natural
question of detecting the presence of fractality and the problem of parameter
estimation based on observed network data, in addition to fundamental
properties of the FGN as a random graph model. We also explore fractality in
community structures by unveiling a natural stochastic block model in the
setting of FGNs.
"
"stat.TH","  Robustness analysis is an emerging field in the domain of uncertainty
quantification. It consists of analysing the response of a computer model with
uncertain inputs to the perturbation of one or several of its input
distributions. Thus, a practical robustness analysis methodology should rely on
a coherent definition of a distribution perturbation. This paper addresses this
issue by exposing a rigorous way of perturbing densities. The proposed
methodology is based the Fisher distance on manifolds of probability
distributions. A numerical method to calculate perturbed densities in practice
is presented. This method comes from Lagrangian mechanics and consists of
solving an ordinary differential equations system. This perturbation definition
is then used to compute quantile-oriented robustness indices. The resulting
Perturbed-Law based sensitivity Indices (PLI) are illustrated on several
numerical models. This methodology is also applied to an industrial study
(simulation of a loss of coolant accident in a nuclear reactor), where several
tens of the model physical parameters are uncertain with limited knowledge
concerning their distributions.
"
"stat.TH","  We provide an algorithm to generate trajectories of sparse stochastic
processes that are solutions of linear ordinary differential equations driven
by L\'evy white noises. A recent paper showed that these processes are limits
in law of generalized compound-Poisson processes. Based on this result, we
derive an off-the-grid algorithm that generates arbitrarily close
approximations of the target process. Our method relies on a B-spline
representation of generalized compound-Poisson processes. We illustrate
numerically the validity of our approach.
"
"stat.TH","  We consider the following data perturbation model, where the covariates incur
multiplicative errors. For two $n \times m$ random matrices $U, X$, we denote
by $U \circ X$ the Hadamard or Schur product, which is defined as $(U \circ
X)_{ij} = (U_{ij}) \cdot (X_{ij})$. In this paper, we study the subgaussian
matrix variate model, where we observe the matrix variate data $X$ through a
random mask $U$:
  \begin{equation*} {\mathcal X} = U \circ X \; \; \; \text{ where} \; \; \;X =
B^{1/2} {\mathbb Z} A^{1/2}, \end{equation*} where ${\mathbb Z}$ is a random
matrix with independent subgaussian entries, and $U$ is a mask matrix with
either zero or positive entries, where ${\mathbb E} U_{ij} \in [0, 1]$ and all
entries are mutually independent. Subsampling in rows, or columns, or random
sampling of entries of $X$ are special cases of this model. Under the
assumption of independence between $U$ and $X$, we introduce componentwise
unbiased estimators for estimating covariance $A$ and $B$, and prove the
concentration of measure bounds in the sense of guaranteeing the restricted
eigenvalue conditions to hold on the estimator for $B$, when columns of data
matrix $X$ are sampled with different rates. Our results provide insight for
sparse recovery for relationships among people (samples, locations, items) when
features (variables, time points, user ratings) are present in the observed
data matrix ${\mathcal X}$ with heterogenous rates. Our proof techniques can
certainly be extended to other scenarios.
"
"stat.TH","  We study the problem of recovering an unknown signal $\boldsymbol x$ given
measurements obtained from a generalized linear model with a Gaussian sensing
matrix. Two popular solutions are based on a linear estimator $\hat{\boldsymbol
x}^{\rm L}$ and a spectral estimator $\hat{\boldsymbol x}^{\rm s}$. The former
is a data-dependent linear combination of the columns of the measurement
matrix, and its analysis is quite simple. The latter is the principal
eigenvector of a data-dependent matrix, and a recent line of work has studied
its performance. In this paper, we show how to optimally combine
$\hat{\boldsymbol x}^{\rm L}$ and $\hat{\boldsymbol x}^{\rm s}$. At the heart
of our analysis is the exact characterization of the joint empirical
distribution of $(\boldsymbol x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol
x}^{\rm s})$ in the high-dimensional limit. This allows us to compute the
Bayes-optimal combination of $\hat{\boldsymbol x}^{\rm L}$ and
$\hat{\boldsymbol x}^{\rm s}$, given the limiting distribution of the signal
$\boldsymbol x$. When the distribution of the signal is Gaussian, then the
Bayes-optimal combination has the form $\theta\hat{\boldsymbol x}^{\rm
L}+\hat{\boldsymbol x}^{\rm s}$ and we derive the optimal combination
coefficient. In order to establish the limiting distribution of $(\boldsymbol
x, \hat{\boldsymbol x}^{\rm L}, \hat{\boldsymbol x}^{\rm s})$, we design and
analyze an Approximate Message Passing (AMP) algorithm whose iterates give
$\hat{\boldsymbol x}^{\rm L}$ and approach $\hat{\boldsymbol x}^{\rm s}$.
Numerical simulations demonstrate the improvement of the proposed combination
with respect to the two methods considered separately.
"
"stat.TH","  Multivariate extreme value theory is concerned with modeling the joint tail
behavior of several random variables. Existing work mostly focuses on
asymptotic dependence, where the probability of observing a large value in one
of the variables is of the same order as observing a large value in all
variables simultaneously. However, there is growing evidence that asymptotic
independence is equally important in real world applications. Available
statistical methodology in the latter setting is scarce and not well understood
theoretically. We revisit non-parametric estimation and introduce rank-based
M-estimators for parametric models that simultaneously work under asymptotic
dependence and asymptotic independence, without requiring prior knowledge on
which of the two regimes applies. Asymptotic normality of the proposed
estimators is established under weak regularity conditions. We further show how
bivariate estimators can be leveraged to obtain parametric estimators in
spatial tail models, and again provide a thorough theoretical justification for
our approach.
"
"stat.TH","  Recursive linear structural equation models are widely used to postulate
causal mechanisms underlying observational data. In these models, each variable
equals a linear combination of a subset of the remaining variables plus an
error term. When there is no unobserved confounding or selection bias, the
error terms are assumed to be independent. We consider estimating a total
causal effect in this setting. The causal structure is assumed to be known only
up to a maximally oriented partially directed acyclic graph (MPDAG), a general
class of graphs that can represent a Markov equivalence class of directed
acyclic graphs (DAGs) with added background knowledge. We propose a simple
estimator based on recursive least squares, which can consistently estimate any
identified total causal effect, under point or joint intervention. We show that
this estimator is the most efficient among all regular estimators that are
based on the sample covariance, which includes covariate adjustment and the
estimators employed by the joint-IDA algorithm. Notably, our result holds
without assuming Gaussian errors.
"
"stat.TH","  This paper introduces structured machine learning regressions for prediction
and nowcasting with panel data consisting of series sampled at different
frequencies. Motivated by the empirical problem of predicting corporate
earnings for a large cross-section of firms with macroeconomic, financial, and
news time series sampled at different frequencies, we focus on the sparse-group
LASSO regularization. This type of regularization can take advantage of the
mixed frequency time series panel data structures and we find that it
empirically outperforms the unstructured machine learning methods. We obtain
oracle inequalities for the pooled and fixed effects sparse-group LASSO panel
data estimators recognizing that financial and economic data exhibit heavier
than Gaussian tails. To that end, we leverage on a novel Fuk-Nagaev
concentration inequality for panel data consisting of heavy-tailed
$\tau$-mixing processes which may be of independent interest in other
high-dimensional panel data settings.
"
"stat.TH","  Determinantal point processes (DPPs) are popular probabilistic models of
diversity. In this paper, we investigate DPPs from a new perspective: property
testing of distributions. Given sample access to an unknown distribution $q$
over the subsets of a ground set, we aim to distinguish whether $q$ is a DPP
distribution, or $\epsilon$-far from all DPP distributions in
$\ell_1$-distance. In this work, we propose the first algorithm for testing
DPPs. Furthermore, we establish a matching lower bound on the sample complexity
of DPP testing. This lower bound also extends to showing a new hardness result
for the problem of testing the more general class of log-submodular
distributions.
"
"stat.TH","  In observational studies, it is important to balance covariates in different
treatment groups in order to estimate treatment effects. One of the most
commonly used methods for such purpose is the weighting method. The performance
quality of this method usually depends on either the correct model
specification for the propensity score or strong regularity conditions for the
underlying model, which might not hold in practice. In this paper, we introduce
a new robust and computationally efficient framework of weighting methods for
covariate balancing, which allows us to conduct model-free inferences for the
sake of robustness and integrate an extra `unlabeled' data set if available.
Unlike existing methods, the new framework reduces the weights construction
problem to a classical density estimation problem by applying a data-driven
transformation to the observed covariates. We characterize the theoretical
properties of the new estimators of average treatment effect under a
nonparametric setting and show that they are able to work robustly under low
regularity conditions. The new framework is also applied to several numerical
examples using both simulated and real datasets to demonstrate its practical
merits.
"
"stat.TH","  Community detection in large social networks is affected by degree
heterogeneity of nodes. The D-SCORE algorithm for directed networks was
introduced to reduce this effect by taking the element-wise ratios of the
singular vectors of the adjacency matrix before clustering. Meaningful results
were obtained for the statistician citation network, but rigorous analysis on
its performance was missing. First, this paper establishes theoretical
guarantee for this algorithm and its variants for the directed degree-corrected
block model (Directed-DCBM). Second, this paper provides significant
improvements for the original D-SCORE algorithms by attaching the nodes outside
of the community cores using the information of the original network instead of
the singular vectors.
"
"stat.TH","  The likelihood ratio test (LRT) is widely used for comparing the relative fit
of nested latent variable models. Following Wilks' theorem, the LRT is
conducted by comparing the LRT statistic with its asymptotic distribution under
the restricted model, a $\chi^2$-distribution with degrees of freedom equal to
the difference in the number of free parameters between the two nested models
under comparison. For models with latent variables such as factor analysis,
structural equation models and random effects models, however, it is often
found that the $\chi^2$ approximation does not hold. In this note, we show how
the regularity conditions of Wilks' theorem may be violated using three
examples of models with latent variables. In addition, a more general theory
for LRT is given that provides the correct asymptotic theory for these LRTs.
This general theory was first established in Chernoff (1954) and discussed in
both van der Vaart (2000) and Drton (2009), but it does not seem to have
received enough attention. We illustrate this general theory with the three
examples.
"
"stat.TH","  The log-likelihood for clustering multivariate normal distributions is
calculated for a partition with equal means in each cluster. The result has
terms to penalise poor fits and model complexity, and determines both the
number and composition of clusters. The procedure is equivalent to calculating
the Bayesian Information Criterion (BIC) without approximation, and can produce
similar, but less subjective results as the ad-hoc ""elbow criterion"". An
intended application is clustering of parametric models, whose maximum
likelihood estimates (MLEs) are normally distributed. Many parametric models
are more familiar and interpretable than directly clustered data. For example,
survival models can build-in prior knowledge, adjust for known confounders, and
use marginalisation to emphasise parameters of interest. The combined approach
is equivalent to a multi-layer clustering algorithm that characterises features
through the normally distributed MLE parameters of a fitted model, and then
clusters the normal distributions. The results can alternately be applied
directly to measured data and their estimated covariances.
"
"stat.TH","  In the Tensor PCA problem introduced by Richard and Montanari (2014), one is
given a dataset consisting of $n$ samples $\mathbf{T}_{1:n}$ of i.i.d. Gaussian
tensors of order $k$ with the promise that $\mathbb{E}\mathbf{T}_1$ is a rank-1
tensor and $\|\mathbb{E} \mathbf{T}_1\| = 1$. The goal is to estimate
$\mathbb{E} \mathbf{T}_1$. This problem exhibits a large conjectured hard phase
when $k>2$: When $d \lesssim n \ll d^{\frac{k}{2}}$ it is information
theoretically possible to estimate $\mathbb{E} \mathbf{T}_1$, but no polynomial
time estimator is known. We provide a sharp analysis of the optimal sample
complexity in the Statistical Query (SQ) model and show that SQ algorithms with
polynomial query complexity not only fail to solve Tensor PCA in the
conjectured hard phase, but also have a strictly sub-optimal sample complexity
compared to some polynomial time estimators such as the Richard-Montanari
spectral estimator. Our analysis reveals that the optimal sample complexity in
the SQ model depends on whether $\mathbb{E} \mathbf{T}_1$ is symmetric or not.
For symmetric, even order tensors, we also isolate a sample size regime in
which it is possible to test if $\mathbb{E} \mathbf{T}_1 = \mathbf{0}$ or
$\mathbb{E}\mathbf{T}_1 \neq \mathbf{0}$ with polynomially many queries but not
estimate $\mathbb{E}\mathbf{T}_1$. Our proofs rely on the Fourier analytic
approach of Feldman, Perkins and Vempala (2018) to prove sharp SQ lower bounds.
"
"stat.TH","  Vinberg cones and the ambient vector spaces are important in modern
statistics of sparse models and of graphical models. The aim of this paper is
to study eigenvalue distributions of Gaussian, Wigner and covariance matrices
related to growing Vinberg matrices, corresponding to growing daisy graphs. For
Gaussian or Wigner ensembles, we give an explicit formula for the limiting
distribution. For Wishart ensembles defined naturally on Vinberg cones, their
limiting Stieltjes transforms, support and atom at 0 are described explicitly
in terms of the Lambert-Tsallis functions, which are defined by using the
Tsallis $q$-exponential functions.
"
"stat.TH","  Large dimensional Gram type matrices are common objects in high-dimensional
statistics and machine learning. In this paper, we study the limiting
distribution of the edge eigenvalues for a general class of high-dimensional
Gram type random matrices, including separable sample covariance matrices,
sparse sample covariance matrices, bipartite stochastic block model and random
Gram matrices with general variance profiles. Specifically, we prove that under
(almost) sharp moment conditions and certain tractable regularity assumptions,
the edge eigenvalues, i.e., the largest few eigenvalues of non-spiked Gram type
random matrices or the extremal bulk eigenvalues of spiked Gram type random
matrices, satisfy the Tracy-Widom distribution asymptotically. Our results can
be used to construct adaptive, accurate and powerful statistics for
high-dimensional statistical inference. In particular, we propose
data-dependent statistics to infer the number of signals under general noise
structure, test the one-sided sphericity of separable matrix, and test the
structure of bipartite stochastic block model. Numerical simulations show
strong support of our proposed statistics. The core of our proof is to
establish the edge universality and Tracy-Widom distribution for a rectangular
Dyson Brownian motion with regular initial data. This is a general strategy to
study the edge statistics for high-dimensional Gram type random matrices
without exploring the specific independence structure of the target matrices.
It has potential to be applied to more general random matrices that are beyond
the ones considered in this paper.
"
"stat.TH","  Many clustering problems enjoy solutions by semidefinite programming.
Theoretical results in this vein frequently consider data with a planted
clustering and a notion of signal strength such that the semidefinite program
exactly recovers the planted clustering when the signal strength is
sufficiently large. In practice, semidefinite programs are notoriously slow,
and so speedups are welcome. In this paper, we show how to sketch a popular
semidefinite relaxation of a graph clustering problem known as minimum
bisection, and our analysis supports a meta-claim that the clustering task is
less computationally burdensome when there is more signal.
"
"stat.TH","  A standard model for epidemics is the SIR model on a graph. We introduce a
simple algorithm that uses the early infection times from a sample path of the
SIR model to estimate the parameters this model, and we provide a performance
guarantee in the setting of locally tree-like graphs.
"
"stat.TH","  U-statistics are widely used in fields such as economics, machine learning,
and statistics. However, while they enjoy desirable statistical properties,
they have an obvious drawback in that the computation becomes impractical as
the data size $n$ increases. Specifically, the number of combinations, say $m$,
that a U-statistic of order $d$ has to evaluate is $O(n^d)$. Many efforts have
been made to approximate the original U-statistic using a small subset of
combinations since Blom (1976), who referred to such an approximation as an
incomplete U-statistic. To the best of our knowledge, all existing methods
require $m$ to grow at least faster than $n$, albeit more slowly than $n^d$, in
order for the corresponding incomplete U-statistic to be asymptotically
efficient in terms of the mean squared error. In this paper, we introduce a new
type of incomplete U-statistic that can be asymptotically efficient, even when
$m$ grows more slowly than $n$. In some cases, $m$ is only required to grow
faster than $\sqrt{n}$. Our theoretical and empirical results both show
significant improvements in the statistical efficiency of the new incomplete
U-statistic.
"
"stat.TH","  This paper introduces link functions for transforming one probability
distribution to another such that the Kullback-Leibler and R\'enyi divergences
between the two distributions are symmetric. Two general classes of link models
are proposed. The first model links two survival functions and is applicable to
models such as the proportional odds and change point, which are used in
survival analysis and reliability modeling. A prototype application involving
the proportional odds model demonstrates advantages of symmetric divergence
measures over asymmetric measures for assessing the efficacy of features and
for model averaging purposes. The advantages include providing unique ranks for
models and unique information weights for model averaging with one-half as much
computation requirement of asymmetric divergences. The second model links two
cumulative probability distribution functions. This model produces a
generalized location model which are continuous counterparts of the binary
probability models such as probit and logit models. Examples include the
generalized probit and logit models which have appeared in the survival
analysis literature, and a generalized Laplace model and a generalized
Student-$t$ model, which are survival time models corresponding to the
respective binary probability models. Lastly, extensions to symmetric
divergence between survival functions and conditions for copula dependence
information are presented.
"
"stat.TH","  Let $X_1,\ldots,X_n$ be an i.i.d. sample from symmetric stable distribution
with stability parameter $\alpha$ and scale parameter $\gamma$. Let $\varphi_n$
be the empirical characteristic function. We prove an uniform large deviation
inequality: given preciseness $\epsilon>0$ and probability $p\in (0,1)$, there
exists universal (depending on $\epsilon$ and $p$ but not depending on $\alpha$
and $\gamma$) constant $\bar{r}>0$ so that $$P\big(\sup_{u>0:r(u)\leq
\bar{r}}|r(u)-\hat{r}(u)|\geq \epsilon\big)\leq p,$$ where
$r(u)=(u\gamma)^{\alpha}$ and $\hat{r}(u)=-\ln|\varphi_n(u)|$. As an
applications of the result, we show how it can be used in estimation unknown
stability parameter $\alpha$.
"
"stat.TH","  We investigate the general class of stick-breaking processes with
exchangeable length variables. These generalize well-known Bayesian
non-parametric priors in an unexplored direction. We give conditions to assure
the respective species sampling process is discrete almost surely and the
corresponding prior has full support. For a rich sub-class we find the
probability that the stick-breaking weights are decreasingly ordered. A general
formulae for the distribution of the latent allocation variables is derived and
an MCMC algorithm is proposed for density estimation purposes.
"
"stat.TH","  The marginal likelihood or evidence in Bayesian statistics contains an
intrinsic penalty for larger model sizes and is a fundamental quantity in
Bayesian model comparison. Over the past two decades, there has been steadily
increasing activity to understand the nature of this penalty in singular
statistical models, building on pioneering work by Sumio Watanabe. Unlike
regular models where the Bayesian information criterion (BIC) encapsulates a
first-order expansion of the logarithm of the marginal likelihood, parameter
counting gets trickier in singular models where a quantity called the real log
canonical threshold (RLCT) summarizes the effective model dimensionality. In
this article, we offer a probabilistic treatment to recover non-asymptotic
versions of established evidence bounds as well as prove a new result based on
the Gibbs variational inequality. In particular, we show that mean-field
variational inference correctly recovers the RLCT for any singular model in its
canonical or normal form. We additionally exhibit sharpness of our bound by
analyzing the dynamics of a general purpose coordinate ascent algorithm (CAVI)
popularly employed in variational inference.
"
"stat.TH","  We discuss Bayesian inference for parameters selected using the data. We
argue that, in general, an adjustment for selection is necessary in order to
achieve approximate repeated-sampling validity, and discuss two issues that
emerge from such adjustment. The first one concerns a potential ambiguity in
the choice of posterior distribution. The second one concerns the choice of
non-informative prior densities that lead to well-calibrated posterior
inferences. We show that non-informative priors that are independent of the
sample size tend to overstate regions of the parameter space with low selection
probability.
"
"stat.TH","  The notion of multivariate total positivity has proved to be useful in
finance and psychology but may be too restrictive in other applications. In
this paper we propose a concept of local association, where highly connected
components in a graphical model are positively associated and study its
properties. Our main motivation comes from gene expression data, where
graphical models have become a popular exploratory tool. We focus the
exposition on Gaussian distributions but our methods readily extend to
non-paranormal distributions. Motivated by a convex optimization problem that
arises in this context, we develop a GOLAZO approach that generalizes a number
of optimization procedures that arise in the context of graphical models (e.g.
the GLASSO) and propose a simple block-coordinate descent optimization
procedure for solving the dual problem. Our results on existence of the optimum
for such problems are of separate interest.
"
"stat.TH","  We study kmeans clustering estimation of panel data models with a latent
group structure and $N$ units and $T$ time periods under long panel
asymptotics. We show that the group-specific coefficients can be estimated at
the parametric root $NT$ rate even if error variances diverge as $T \to \infty$
and some units are asymptotically misclassified. This limit case approximates
empirically relevant settings and is not covered by existing asymptotic
results.
"
"stat.TH","  This article studies the estimation of static community memberships from
temporally correlated pair interactions represented by an $N$-by-$N$-by-$T$
tensor where $N$ is the number of nodes and $T$ is the length of the time
horizon. We present several estimation algorithms, both offline and online,
which fully utilise the temporal nature of the observed data. As an
information-theoretic benchmark, we study data sets generated by a dynamic
stochastic block model, and derive fundamental information criteria for the
recoverability of the community memberships as $N \to \infty$ both for bounded
and diverging $T$. These results show that (i) even a small increase in $T$ may
have a big impact on the recoverability of community memberships, (ii)
consistent recovery is possible even for very sparse data (e.g. bounded average
degree) when $T$ is large enough. We analyse the accuracy of the proposed
estimation algorithms under various assumptions on data sparsity and
identifiability, and prove that an efficient online algorithm is strongly
consistent up to the information-theoretic threshold under suitable
initialisation. Numerical experiments show that even a poor initial estimate
(e.g., blind random guess) of the community assignment leads to high accuracy
after a small number of iterations, and remarkably so also in very sparse
regimes.
"
"stat.TH","  In this paper, we first introduce the notion of channel leakage as the
minimum mutual information between the channel input and channel output. As its
name indicates, channel leakage quantifies the minimum information leakage to
the malicious receiver. In a broad sense, it can be viewed as a dual concept of
channel capacity, which characterizes the maximum information transmission to
the targeted receiver. We obtain explicit formulas of channel leakage for the
white Gaussian case, the colored Gaussian case, and the fading case. We then
utilize this notion to investigate the fundamental limitations of obfuscation
in terms of privacy-distortion tradeoffs (as well as privacy-power tradeoffs)
for streaming data; particularly, we derive analytical tradeoff equations for
the stationary case, the non-stationary case, and the finite-time case. Our
results also indicate explicitly how to design the privacy masks in an optimal
way.
"
"stat.TH","  Shapley value is a concept from game theory. Recently, it has been used for
explaining complex models produced by machine learning techniques. Although the
mathematical definition of Shapley value is straight-forward, the implication
of using it as a model interpretation tool is yet to be described. In the
current paper, we analyzed Shapley value in the Bayesian network framework. We
established the relationship between Shapley value and conditional
independence, a key concept in both predictive and causal modeling. Our results
indicate that, eliminating a variable with high Shapley value from a model do
not necessarily impair predictive performance, whereas eliminating a variable
with low Shapley value from a model could impair performance. Therefore, using
Shapley value for feature selection do not result in the most parsimonious and
predictively optimal model in the general case. More importantly, Shapley value
of a variable do not reflect their causal relationship with the target of
interest.
"
"stat.TH","  The notable claim of quantum supremacy presented by Google's team in 2019
consists of demonstrating the ability of a quantum circuit to generate, albeit
with considerable noise, bitstrings from a distribution that is considered hard
to simulate on classical computers. Verifying that the generated data is indeed
from the claimed distribution and assessing the circuit's noise level and its
fidelity is a purely statistical undertaking. The objective of this paper is to
explain the relations between quantum computing and some of the statistical
aspects involved in demonstrating quantum supremacy in terms that are
accessible to statisticians, computer scientists, and mathematicians. Starting
with the statistical analysis in Google's demonstration, which we explain, we
study various estimators of the fidelity, and different approaches to testing
the distributions generated by the quantum computer. We propose different noise
models, and discuss their implications. A preliminary study of the Google data,
focusing mostly on circuits of 12 and 14 qubits is discussed throughout the
paper.
"
"stat.TH","  We deal with parametric estimation for a parabolic linear second order
stochastic partial differential equation (SPDE) with a small dispersion
parameter based on high frequency data which are observed in time and space. By
using the thinned data with respect to space obtained from the high frequency
data, the minimum contrast estimators of two coefficient parameters of the SPDE
are proposed. With these estimators and the thinned data with respect to time
obtained from the high frequency data, we construct an approximation of the
coordinate process of the SPDE. Using the approximate coordinate process, we
obtain the adaptive estimator of a coefficient parameter of the SPDE. Moreover,
we give simulation results of the proposed estimators of the SPDE.
"
"stat.TH","  There are two main classes of dispersion models studied in the literature:
proper (PDM), and exponential dispersion models (EDM). Dispersion models that
are neither proper nor exponential dispersion models are termed here
non-standard dispersion models (NSDM). This paper exposes a technique for
constructing new PDMs and NSDMs. This construction provides a solution to an
open question in the theory of dispersion models about the extension of
non-standard dispersion models. Given a unit deviance function, a dispersion
model is usually constructed by calculating a normalising function that makes
the density function integrates one. This calculation involves the solution of
non-trivial integral equations. The main idea explored here is to use
characteristic functions of real non-lattice symmetric probability measures to
construct a family of unit deviances that are sufficiently regular to make the
associated integral equations tractable. The integral equations associated to
those unit deviances admit a trivial solution, in the sense that the
normalising function is a constant function independent of the observed values.
However, we show, using the machinery of distributions (i.e., generalised
functions) and expansions of the normalising function with respect to specially
constructed Riez systems, that those integral equations also admit infinitely
many non-trivial solutions, generating many NSDMs. We conclude that, the
cardinality of the class of non-standard dispersion models is larger than the
cardinality of the class of real non-lattice symmetric probability measures.
"
"stat.TH","  This paper presents a detailed theoretical analysis of the three stochastic
approximation proximal gradient algorithms proposed in our companion paper [49]
to set regularization parameters by marginal maximum likelihood estimation. We
prove the convergence of a more general stochastic approximation scheme that
includes the three algorithms of [49] as special cases. This includes
asymptotic and non-asymptotic convergence results with natural and easily
verifiable conditions, as well as explicit bounds on the convergence rates.
Importantly, the theory is also general in that it can be applied to other
intractable optimisation problems. A main novelty of the work is that the
stochastic gradient estimates of our scheme are constructed from inexact
proximal Markov chain Monte Carlo samplers. This allows the use of samplers
that scale efficiently to large problems and for which we have precise
theoretical guarantees.
"
"stat.TH","  Wilk's theorem, which offers universal chi-squared approximations for
likelihood ratio tests, is widely used in many scientific hypothesis testing
problems. For modern datasets with increasing dimension, researchers have found
that the conventional Wilk's phenomenon of the likelihood ratio test statistic
often fails. Although new approximations have been proposed in high dimensional
settings, there still lacks a clear statistical guideline regarding how to
choose between the conventional and newly proposed approximations, especially
for moderate-dimensional data. To address this issue, we develop the necessary
and sufficient phase transition conditions for Wilk's phenomenon under popular
tests on multivariate mean and covariance structures. Moreover, we provide an
in-depth analysis of the accuracy of chi-squared approximations by deriving
their asymptotic biases. These results may provide helpful insights into the
use of chi-squared approximations in scientific practices.
"
"stat.TH","  In this article, we introduce and study time- and space-fractional Poisson
processes of order k. These processes are defined in terms of fractional
compound Poisson processes. Time-fractional Poisson process of order k
naturally generalizes the Poisson process and Poisson process of order k to a
heavy tailed waiting times counting process. The space-fractional Poisson
process of order k, allows on average infinite number of arrivals in any
interval. We derive the marginal probabilities, governing
difference-differential equations of the introduced processes.
"
"stat.TH","  We consider the problem of detecting an elevated mean on an interval with
unknown location and length in the univariate Gaussian sequence model. Recent
results have shown that using scale-dependent critical values for the scan
statistic allows to attain asymptotically optimal detection simultaneously for
all signal lengths, thereby improving on the traditional scan, but this
procedure has been criticized for losing too much power for short signals. We
explain this discrepancy by showing that these asymptotic optimality results
will necessarily be too imprecise to discern the performance of scan statistics
in a practically relevant way, even in a large sample context. Instead, we
propose to assess the performance with a new finite sample criterion. We then
present three calibrations for scan statistics that perform well across a range
of relevant signal lengths: The first calibration uses a particular adjustment
to the critical values and is therefore tailored to the Gaussian case. The
second calibration uses a scale-dependent adjustment to the significance levels
and is therefore applicable to arbitrary known null distributions. The third
calibration restricts the scan to a particular sparse subset of the scan
windows and then applies a weighted Bonferroni adjustment to the corresponding
test statistics. This calibration is also applicable to arbitrary null
distributions and in addition is very simple to implement.
"
"stat.TH","  We investigate optimal posteriors for recently introduced \cite{begin2016pac}
chi-squared divergence based PAC-Bayesian bounds in terms of nature of their
distribution, scalability of computations, and test set performance. For a
finite classifier set, we deduce bounds for three distance functions:
KL-divergence, linear and squared distances. Optimal posterior weights are
proportional to deviations of empirical risks, usually with subset support. For
uniform prior, it is sufficient to search among posteriors on classifier
subsets ordered by these risks. We show the bound minimization for linear
distance as a convex program and obtain a closed-form expression for its
optimal posterior. Whereas that for squared distance is a quasi-convex program
under a specific condition, and the one for KL-divergence is non-convex
optimization (a difference of convex functions). To compute such optimal
posteriors, we derive fast converging fixed point (FP) equations. We apply
these approaches to a finite set of SVM regularization parameter values to
yield stochastic SVMs with tight bounds. We perform a comprehensive performance
comparison between our optimal posteriors and known KL-divergence based
posteriors on a variety of UCI datasets with varying ranges and variances in
risk values, etc. Chi-squared divergence based posteriors have weaker bounds
and worse test errors, hinting at an underlying regularization by KL-divergence
based posteriors. Our study highlights the impact of divergence function on the
performance of PAC-Bayesian classifiers. We compare our stochastic classifiers
with cross-validation based deterministic classifier. The latter has better
test errors, but ours is more sample robust, has quantifiable generalization
guarantees, and is computationally much faster.
"
"stat.TH","  The three-parameter generalized extreme value distribution arises from
classical univariate extreme value theory and is in common use for analyzing
the far tail of observed phenomena. Curiously, important asymptotic properties
of likelihood-based estimation under this standard model have yet to be
established. In this paper, we formally prove that the maximum likelihood
estimator is global and unique. An interesting secondary result entails the
uniform consistency of a class of limit relations in a tight neighborhood of
the shape parameter.
"
"stat.TH","  The likelihood ratio test is widely used in exploratory factor analysis to
assess the model fit and determine the number of latent factors. Despite its
popularity and clear statistical rationale, researchers have found that when
the dimension of the response data is large compared to the sample size, the
classical chi-square approximation of the likelihood ratio test statistic often
fails. Theoretically, it has been an open problem when such a phenomenon
happens as the dimension of data increases; practically, the effect of high
dimensionality is less examined in exploratory factor analysis, and there lacks
a clear statistical guideline on the validity of the conventional chi-square
approximation. To address this problem, we investigate the failure of the
chi-square approximation of the likelihood ratio test in high-dimensional
exploratory factor analysis, and derive the necessary and sufficient condition
to ensure the validity of the chi-square approximation. The results yield
simple quantitative guidelines to check in practice and would also provide
useful statistical insights into the practice of exploratory factor analysis.
"
"stat.TH","  Considerable effort has been directed to developing asymptotically minimax
procedures in problems of recovering functions and densities. These methods
often rely on somewhat arbitrary and restrictive assumptions such as isotropy
or spatial homogeneity. This work enhances theoretical understanding of
Bayesian forests (including BART) under substantially relaxed smoothness
assumptions. In particular, we provide a comprehensive study of asymptotic
optimality and posterior contraction of Bayesian forests when the regression
function has anisotropic smoothness that possibly varies over the function
domain. We introduce a new class of sparse piecewise heterogeneous anisotropic
H\""{o}lder functions and derive their minimax rate of estimation in
high-dimensional scenarios under the $L_2$ loss. Next, we find that the default
Bayesian CART prior, coupled with a subset selection prior for sparse
estimation in high-dimensional scenarios, adapts to unknown heterogeneous
smoothness and sparsity. These results show that Bayesian forests are uniquely
suited for more general estimation problems which would render other default
machine learning tools, such as Gaussian processes, suboptimal. Beyond
nonparametric regression, we also show that Bayesian forests can be
successfully applied to many other problems including density estimation and
binary classification.
"
"stat.TH","  Random divisions of an interval arise in various context, including
statistics, physics, and geometric analysis. For testing the uniformity of a
random partition of the unit interval $[0,1]$ into $k$ disjoint subintervals of
size $(S_k[1],\ldots,S_k[k])$, Greenwood (1946) suggested using the squared
$\ell_2$-norm of this size vector as a test statistic, prompting a number of
subsequent studies. Despite much progress on understanding its power and
asymptotic properties, attempts to find its exact distribution have succeeded
so far for only small values of $k$. Here, we develop an efficient method to
compute the distribution of the Greenwood statistic and more general
spacing-statistics for an arbitrary value of $k$. Specifically, we consider
random divisions of $\{1,2,\dots,n\}$ into $k$ subsets of consecutive integers
and study $\|S_{n,k}\|^p_{p,w}$, the $p$th power of the weighted $\ell_p$-norm
of the subset size vector $S_{n,k}=(S_{n,k}[1],\ldots,S_{n,k}[k])$ for
arbitrary weights $w=(w_1,\ldots,w_k)$. We present an exact and quickly
computable formula for its moments, as well as a simple algorithm to accurately
reconstruct a probability distribution using the moment sequence. We also study
various scaling limits, one of which corresponds to the Greenwood statistic in
the case of $p=2$ and $w=(1,\ldots,1)$, and this connection allows us to obtain
information about regularity, monotonicity and local behavior of its
distribution. Lastly, we devise a new family of non-parametric tests using
$\|S_{n,k}\|^p_{p,w}$ and demonstrate that they exhibit substantially improved
power for a large class of alternatives, compared to existing popular methods
such as the Kolmogorov-Smirnov, Cramer-von Mises, and Mann-Whitney/Wilcoxon
rank-sum tests.
"
"stat.TH","  In this paper we propose a novel and practical variance reduction approach
for additive functionals of dependent sequences. Our approach combines the use
of control variates with the minimisation of an empirical variance estimate. We
analyse finite sample properties of the proposed method and derive finite-time
bounds of the excess asymptotic variance to zero. We apply our methodology to
Stochastic Gradient MCMC (SGMCMC) methods for Bayesian inference on large data
sets and combine it with existing variance reduction methods for SGMCMC. We
present empirical results carried out on a number of benchmark examples showing
that our variance reduction method achieves significant improvement as compared
to state-of-the-art methods at the expense of a moderate increase of
computational overhead.
"
"stat.TH","  The inferential model (IM) framework produces data-dependent, non-additive
degrees of belief about the unknown parameter that are provably valid. The
validity property guarantees, among other things, that inference procedures
derived from the IM control frequentist error rates at the nominal level. A
technical complication is that IMs are built on a relatively unfamiliar theory
of random sets. Here we develop an alternative---and practically
equivalent---formulation, based on a theory of possibility measures, which is
simpler in many respects. This new perspective also sheds light on the
relationship between IMs and Fisher's fiducial inference, as well as on the
construction of optimal IMs.
"
"stat.TH","  In this paper a new family of minimum divergence estimators based on the
Bregman divergence is proposed. The popular density power divergence (DPD)
class of estimators is a sub-class of Bregman divergences. We propose and study
a new sub-class of Bregman divergences called the exponentially weighted
divergence (EWD). Like the minimum DPD estimator, the minimum EWD estimator is
recognised as an M-estimator. This characterisation is useful while discussing
the asymptotic behaviour as well as the robustness properties of this class of
estimators. Performances of the two classes are compared -- both through
simulations as well as through real life examples. We develop an estimation
process not only for independent and homogeneous data, but also for
non-homogeneous data. General tests of parametric hypotheses based on the
Bregman divergences are also considered. We establish the asymptotic null
distribution of our proposed test statistic and explore its behaviour when
applied to real data. The inference procedures generated by the new EWD
divergence appear to be competitive or better that than the DPD based
procedures.
"
"stat.TH","  In this paper, we propose a new framework to construct confidence sets for a
$d$-dimensional unknown sparse parameter $\theta$ under the normal mean model
$X\sim N(\theta,\sigma^2I)$. A key feature of the proposed confidence set is
its capability to account for the sparsity of $\theta$, thus named as {\em
sparse} confidence set. This is in sharp contrast with the classical methods,
such as Bonferroni confidence intervals and other resampling based procedures,
where the sparsity of $\theta$ is often ignored. Specifically, we require the
desired sparse confidence set to satisfy the following two conditions: (i)
uniformly over the parameter space, the coverage probability for $\theta$ is
above a pre-specified level; (ii) there exists a random subset $S$ of
$\{1,...,d\}$ such that $S$ guarantees the pre-specified true negative rate
(TNR) for detecting nonzero $\theta_j$'s. To exploit the sparsity of $\theta$,
we define that the confidence interval for $\theta_j$ degenerates to a single
point 0 for any $j\notin S$. Under this new framework, we first consider
whether there exist sparse confidence sets that satisfy the above two
conditions. To address this question, we establish a non-asymptotic minimax
lower bound for the non-coverage probability over a suitable class of sparse
confidence sets. The lower bound deciphers the role of sparsity and minimum
signal-to-noise ratio (SNR) in the construction of sparse confidence sets.
Furthermore, under suitable conditions on the SNR, a two-stage procedure is
proposed to construct a sparse confidence set. To evaluate the optimality, the
proposed sparse confidence set is shown to attain a minimax lower bound of some
properly defined risk function up to a constant factor. Finally, we develop an
adaptive procedure to the unknown sparsity and SNR. Numerical studies are
conducted to verify the theoretical results.
"
"stat.TH","  For a continuous random variable $Z$, testing conditional independence $X
\perp \!\!\! \perp Y |Z$ is known to be a particularly hard problem. It
constitutes a key ingredient of many constraint-based causal discovery
algorithms. These algorithms are often applied to datasets containing binary
(or discrete) variables, which indicate the 'context' of the observations, e.g.
a control or treatment group within an experiment. In these settings,
conditional independence testing with $X$ or $Y$ discrete (and the other
continuous) is paramount to the performance of the causal discovery algorithm.
To our knowledge no such conditional independence test currently exists, and in
practice tests which assume all variables to be continuous are used instead. In
this paper we aim to fill this gap, as we combine elements of Holmes et al.
(2015) and Teymur and Filippi (2020) to propose a novel Bayesian nonparametric
conditional two-sample test. Applied to the Local Causal Discovery algorithm,
we investigate its performance on both synthetic and real-world data, and
compare with state-of-the-art continuous conditional independence tests.
"
"stat.TH","  The extropy is a measure of information introduced by Lad et al. (2015) as
dual to entropy. As the entropy, it is a shift-independent information measure.
We introduce here the notion of weighted extropy, a shift-dependent information
measure which gives higher weights to large values of observed random
variables. We also study the weighted residual and past extropies as weighted
versions of extropy for residual and past lifetimes. Bivariate versions extropy
and weighted extropy are also provided. Several examples are presented through
out to illustrate the various concepts introduced here.
"
"stat.TH","  It is proved that the sum of n independent but non-identically distributed
doubly truncated Normal distributions converges in distribution to a Normal
distribution. It is also shown how the result can be applied in estimating a
constrained mixed effects model.
"
"stat.TH","  We study the problem of robustly estimating the mean of a $d$-dimensional
distribution given $N$ examples, where $\varepsilon N$ examples may be
arbitrarily corrupted and most coordinates of every example may be missing.
Assuming each coordinate appears in a constant factor more than $\varepsilon N$
examples, we show algorithms that estimate the mean of the distribution with
information-theoretically optimal dimension-independent error guarantees in
nearly-linear time $\widetilde O(Nd)$. Our results extend recent work on
computationally-efficient robust estimation to a more widely applicable
incomplete-data setting.
"
"stat.TH","  Introduced by Kiefer and Wolfowitz \cite{KW56}, the nonparametric maximum
likelihood estimator (NPMLE) is a widely used methodology for learning mixture
odels and empirical Bayes estimation. Sidestepping the non-convexity in mixture
likelihood, the NPMLE estimates the mixing distribution by maximizing the total
likelihood over the space of probability measures, which can be viewed as an
extreme form of overparameterization.
  In this paper we discover a surprising property of the NPMLE solution.
Consider, for example, a Gaussian mixture model on the real line with a
subgaussian mixing distribution. Leveraging complex-analytic techniques, we
show that with high probability the NPMLE based on a sample of size $n$ has
$O(\log n)$ atoms (mass points), significantly improving the deterministic
upper bound of $n$ due to Lindsay \cite{lindsay1983geometry1}. Notably, any
such Gaussian mixture is statistically indistinguishable from a finite one with
$O(\log n)$ components (and this is tight for certain mixtures). Thus, absent
any explicit form of model selection, NPMLE automatically chooses the right
model complexity, a property we term \emph{self-regularization}. Extensions to
other exponential families are given. As a statistical application, we show
that this structural property can be harnessed to bootstrap existing Hellinger
risk bound of the (parametric) MLE for finite Gaussian mixtures to the NPMLE
for general Gaussian mixtures, recovering a result of Zhang
\cite{zhang2009generalized}.
"
"stat.TH","  We study periodic review stochastic inventory control in the data-driven
setting, in which the retailer makes ordering decisions based only on
historical demand observations without any knowledge of the probability
distribution of the demand. Since an $(s, S)$-policy is optimal when the demand
distribution is known, we investigate the statistical properties of the
data-driven $(s, S)$-policy obtained by recursively computing the empirical
cost-to-go functions. This policy is inherently challenging to analyze because
the recursion induces propagation of the estimation error backwards in time. In
this work, we establish the asymptotic properties of this data-driven policy by
fully accounting for the error propagation. First, we rigorously show the
consistency of the estimated parameters by filling in some gaps (due to
unaccounted error propagation) in the existing studies. On the other hand,
empirical process theory cannot be directly applied to show asymptotic
normality. To explain, the empirical cost-to-go functions for the estimated
parameters are not i.i.d. sums, again due to the error propagation. Our main
methodological innovation comes from an asymptotic representation for
multi-sample $U$-processes in terms of i.i.d. sums. This representation enables
us to apply empirical process theory to derive the influence functions of the
estimated parameters and establish joint asymptotic normality. Based on these
results, we also propose an entirely data-driven estimator of the optimal
expected cost and we derive its asymptotic distribution. We demonstrate some
useful applications of our asymptotic results, including sample size
determination, as well as interval estimation and hypothesis testing on vital
parameters of the inventory problem. The results from our numerical simulations
conform to our theoretical analysis.
"
"stat.TH","  For integers $n>2$ and $k>0$, an $(n\times n)/k$ semi-Latin square is an
$n\times n$ array of $k$-subsets (called blocks) of an $nk$-set (of
treatments), such that each treatment occurs once in each row and once in each
column of the array. A semi-Latin square is uniform if every pair of blocks,
not in the same row or column, intersect in the same positive number of
treatments. It is known that a uniform $(n\times n)/k$ semi-Latin square is
Schur optimal in the class of all $(n\times n)/k$ semi-Latin squares, and here
we show that when a uniform $(n\times n)/k$ semi-Latin square exists, the Schur
optimal $(n\times n)/k$ semi-Latin squares are precisely the uniform ones. We
then compare uniform semi-Latin squares using the criterion of
pairwise-variance (PV) aberration, introduced by J. P. Morgan for affine
resolvable designs, and determine the uniform $(n\times n)/k$ semi-Latin
squares with minimum PV aberration when there exist $n-1$ mutually orthogonal
Latin squares of order $n$. These do not exist when $n=6$, and the smallest
uniform semi-Latin squares in this case have size $(6\times 6)/10$. We present
a complete classification of the uniform $(6\times 6)/10$ semi-Latin squares,
and display (the dual of) the one with least PV aberration. We give a
construction producing a uniform $((n+1)\times (n+1))/((n-2)n)$ semi-Latin
square when there exist $n-1$ mutually orthogonal Latin squares of order $n$,
and determine the PV aberration of such a uniform semi-Latin square. Finally,
we describe how certain affine resolvable designs and balanced incomplete-block
designs can be constructed from uniform semi-Latin squares, and from the
uniform $(6\times 6)/10$ semi-Latin squares classified, we obtain many affine
resolvable designs for 72 treatments in 36 blocks of size 12, as well as new
balanced incomplete-block designs for 36 treatments in 84 blocks of size 6.
"
"stat.TH","  This paper presents a novel data-driven strategy to choose the hyperparameter
$k$ in the $k$-NN regression estimator. We treat the problem of choosing the
hyperparameter as an iterative procedure (over $k$) and propose using an easily
implemented in practice strategy based on the idea of early stopping and the
minimum discrepancy principle. This estimation strategy is proven to be minimax
optimal, under the fixed-design assumption on covariates, over different
smoothness function classes, for instance, the Lipschitz functions class on a
bounded domain. After that, the novel strategy shows consistent simulations
results on artificial and real-world data sets in comparison to other model
selection strategies such as the Hold-out method.
"
"stat.TH","  The original formulation of de Finetti's theorem says that an exchangeable
sequence of Bernoulli random variables is a mixture of iid sequences of random
variables. Following the work of Hewitt and Savage, this theorem is known for
several classes of exchangeable random variables (for instance, for Baire
measurable random variables taking values in a compact Hausdorff space, and for
Borel measurable random variables taking values in a Polish space). Under an
assumption of the underlying common distribution being Radon, we show that de
Finetti's theorem holds for a sequence of Borel measurable exchangeable random
variables taking values in any Hausdorff space. This includes and generalizes
the currently known versions of de Finetti's theorem. We use nonstandard
analysis to first study the empirical measures induced by hyperfinitely many
identically distributed random variables, which leads to a proof of de
Finetti's theorem in great generality while retaining the combinatorial
intuition of proofs of simpler versions of de Finetti's theorem. The required
tools from topological measure theory are developed with the aid of
perspectives provided by nonstandard measure theory. One highlight of this
development is a new generalization of Prokhorov's theorem.
"
"stat.TH","  We consider the strongly consistent question for model selection in a large
class of causal time series models, including AR($\infty$), ARCH($\infty$),
TARCH($\infty$), ARMA-GARCH and many classical others processes.
  We propose a penalized criterion based on the quasi likelihood of the model.
  We provide sufficient conditions that ensure the strong consistency of the
proposed procedure. Also, the estimator of the parameter of the selected model
obeys the law of iterated logarithm.
  It appears that, unlike the result of the weak consistency obtained by Bardet
{\it et al.} \cite{Bardet2020}, a dependence between the regularization
parameter and the model structure is not needed.
"
"stat.TH","  The paper re-analyzes a version of the celebrated Johnson-Lindenstrauss
Lemma, in which matrices are subjected to constraints that naturally emerge
from neuroscience applications: a) sparsity and b) sign-consistency. This
particular variant was studied first by Allen-Zhu, Gelashvili, Micali, Shavit
and more recently by Jagadeesan (RANDOM'19).
  The contribution of this work is a novel proof, which in contrast to previous
works a) uses the modern probability toolkit, particularly basics of
sub-gaussian and sub-gamma estimates b) is self-contained, with no dependencies
on subtle third-party results c) offers explicit constants.
  At the heart of our proof is a novel variant of Hanson-Wright Lemma (on
concentration of quadratic forms). Of independent interest are also auxiliary
facts on sub-gaussian random variables.
"
"stat.TH","  Recently, an alternative measure of uncertainty called cumulative residual
extropy (CREX) was proposed by Jahanshahi et al. (2019). In this paper, we
consider uncertainty measures of minimum ranked set sampling procedure with
unequal samples (MinRSSU) in terms of CREX and its dynamic version and we
compare the uncertainty and information content of CREX based on MinRSSU and
simple random sampling (SRS) designs. Also, using simulation, we study on new
estimators of CREX for MinRSSU and SRS designs in terms of bias and mean square
error. Finally, we provide a new discrimination measure of disparity between
the distribution of MinRSSU and parental data SRS.
"
"stat.TH","  We study the robust mean estimation problem in high dimensions, where $\alpha
<0.5$ fraction of the data points can be arbitrarily corrupted. Motivated by
compressive sensing, we formulate the robust mean estimation problem as the
minimization of the $\ell_0$-`norm' of the outlier indicator vector, under
second moment constraints on the inlier data points. We prove that the global
minimum of this objective is order optimal for the robust mean estimation
problem, and we propose a general framework for minimizing the objective. We
further leverage the $\ell_1$ and $\ell_p$ $(0<p<1)$, minimization techniques
in compressive sensing to provide computationally tractable solutions to the
$\ell_0$ minimization problem. Both synthetic and real data experiments
demonstrate that the proposed algorithms significantly outperform
state-of-the-art robust mean estimation methods.
"
"stat.TH","  This article presents a homogeneity test for testing the equality of several
high-dimensional covariance matrices for stationary processes with ignoring the
assumption of normality. We give the asymptotic distribution of the proposed
test. The simulation illustrates that the proposed test has perfect
performance. Moreover, the power of the test can approach any high probability
uniformly on a set of covariance matrices.
"
"stat.TH","  We study the weak convergence of conditional empirical copula processes, when
the conditioning event has a nonzero probability. The validity of several
bootstrap schemes is stated, including the exchangeable bootstrap. We define
general - possibly conditional - multivariate dependence measures and their
estimators. By applying our theoretical results, we prove the asymptotic
normality of some estimators of such dependence measures.
"
"stat.TH","  Very rare events in which the largest eigenvalue of a random matrix is
atypically large have important consequences in statistics, e.g. in principal
components analysis, and for studying the rough high-dimensional landscapes
encountered in disordered systems in statistical mechanics. These problems lead
to consider matrices $(1/m) \sum_{\mu=1}^m d_\mu \mathbf{z}_\mu
\mathbf{z}_\mu^\dagger$, with $\{\mathbf{z}_\mu\}_{\mu=1}^m$ standard Gaussian
vectors of size $n$, and (fixed) real $d_\mu$. In a high-dimensional limit we
leverage recent techniques to derive the probability of large deviations of the
extreme eigenvalues away from the bulk. We probe our results with Monte-Carlo
methods that effectively simulate events with probability as small as
$10^{-100}$.
"
"stat.TH","  Several procedures have been recently proposed to test the simplifying
assumption for conditional copulas. Instead of considering pointwise
conditioning events, we study the constancy of the conditional dependence
structure when some covariates belong to general borelian conditioning subsets.
Several test statistics based on the equality of conditional Kendall's tau are
introduced, and we derive their asymptotic distributions under the null. When
such conditioning events are not fixed ex ante, we propose a data-driven
procedure to recursively build such relevant subsets. It is based on decision
trees that maximize the differences between the conditional Kendall's taus
corresponding to the leaves of the trees. The performances of such tests are
illustrated in a simulation experiment. Moreover, a study of the conditional
dependence between financial stock returns is managed, given some clustering of
their past values. The last application deals with the conditional dependence
between coverage amounts in an insurance dataset.
"
"stat.TH","  The class of location-scale finite mixtures is of enduring interest both from
applied and theoretical perspectives of probability and statistics. We prove
the following results: to an arbitrary degree of accuracy, (a) location-scale
mixtures of a continuous probability density function (PDF) can approximate any
continuous PDF, uniformly, on a compact set; and (b) for any finite $p\ge1$,
location-scale mixtures of an essentially bounded PDF can approximate any PDF
in $\mathcal{L}_{p}$, in the $\mathcal{L}_{p}$ norm.
"
"stat.TH","  Latent class models have recently become popular for multiple-systems
estimation in human rights applications. However, it is currently unknown when
a given family of latent class models is identifiable in this context. We
provide necessary and sufficient conditions on the number of latent classes
needed for a family of latent class models to be identifiable. Along the way we
provide a mechanism for verifying identifiability in a class of
multiple-systems estimation models that allow for individual heterogeneity.
"
"stat.TH","  We propose a projection-based class of uniformity tests on the hypersphere
using statistics that integrate, along all possible directions, the weighted
quadratic discrepancy between the empirical cumulative distribution function of
the projected data and the projected uniform distribution. Simple expressions
for several test statistics are obtained for the circle and sphere, and
relatively tractable forms for higher dimensions. Despite its different origin,
the proposed class is shown to be related with the well-studied Sobolev class
of uniformity tests. Our new class proves itself advantageous by allowing to
derive new tests for hyperspherical data that neatly extend the circular tests
by Watson, Ajne, and Rothman, and by introducing the first instance of an
Anderson-Darling-like test for such data. The asymptotic distributions and the
local optimality against certain alternatives of the new tests are obtained. A
simulation study evaluates the theoretical findings and evidences that, for
certain scenarios, the new tests are competitive against previous proposals.
The new tests are employed in three astronomical applications.
"
"stat.TH","  The hardcore model on a graph $G$ with parameter $\lambda>0$ is a probability
measure on the collection of all independent sets of $G$, that assigns to each
independent set $I$ a probability proportional to $\lambda^{|I|}$. In this
paper we consider the problem of estimating the parameter $\lambda$ given a
single sample from the hardcore model on a graph $G$. To bypass the
computational intractability of the maximum likelihood method, we use the
maximum pseudo-likelihood (MPL) estimator, which for the hardcore model has a
surprisingly simple closed form expression. We show that for any sequence of
graphs $\{G_N\}_{N\geq 1}$, where $G_N$ is a graph on $N$ vertices, the MPL
estimate of $\lambda$ is $\sqrt N$-consistent, whenever the graph sequence has
uniformly bounded average degree. We then derive sufficient conditions under
which the MPL estimate of the activity parameters is $\sqrt N$-consistent given
a single sample from a general $H$-coloring model, in which restrictions
between adjacent colors are encoded by a constraint graph $H$. We verify the
sufficient conditions for models where there is at least one unconstrained
color as long as the graph sequence has uniformly bounded average degree. This
applies to many $H$-coloring examples such as the Widom-Rowlinson and
multi-state hard-core models. On the other hand, for the $q$-coloring model,
which falls outside this class, we show that consistent estimation may be
impossible even for graphs with bounded average degree. Nevertheless, we show
that the MPL estimate is $\sqrt N$-consistent in the $q$-coloring model when
$\{G_N\}_{N\geq 1}$ has bounded average double neighborhood. The presence of
hard constraints, as opposed to soft constraints, leads to new challenges, and
our proofs entail applications of the method of exchangeable pairs as well as
combinatorial arguments that employ the probabilistic method.
"
"stat.TH","  We study the problem of detecting the edge correlation between two random
graphs with $n$ unlabeled nodes. This is formalized as a hypothesis testing
problem, where under the null hypothesis, the two graphs are independently
generated; under the alternative, the two graphs are edge-correlated under some
latent node correspondence, but have the same marginal distributions as the
null. For both Gaussian-weighted complete graphs and dense Erd\H{o}s-R\'enyi
graphs (with edge probability $n^{-o(1)}$), we determine the sharp threshold at
which the optimal testing error probability exhibits a phase transition from
zero to one as $n\to \infty$. For sparse Erd\H{o}s-R\'enyi graphs with edge
probability $n^{-\Omega(1)}$, we determine the threshold within a constant
factor.
  The proof of the impossibility results is an application of the conditional
second-moment method, where we bound the truncated second moment of the
likelihood ratio by carefully conditioning on the typical behavior of the
intersection graph (consisting of edges in both observed graphs) and taking
into account the cycle structure of the induced random permutation on the
edges. Notably, in the sparse regime, this is accomplished by leveraging the
pseudoforest structure of subcritical Erd\H{o}s-R\'enyi graphs and a careful
enumeration of subpseudoforests that can be assembled from short orbits of the
edge permutation.
"
"stat.TH","  Recently, Chatterjee has introduced a new coefficient of correlation which
has several natural properties. In particular, the coefficient attains its
maximal value if and only if one variable is a measurable function of the other
variable. In this paper, we seek to define correlations which have a similar
property, except now the measurable function must belong to a pre-specified
class, which amounts to a shape restriction on the function. We will then look
specifically at the correlation corresponding to the class of monotone
nondecreasing functions, in which case we can prove various asymptotic results,
as well as perform local power calculations.
"
"stat.TH","  We study frequentist asymptotic properties of Bayesian procedures for
high-dimensional Gaussian sparse regression when unknown nuisance parameters
are involved. Nuisance parameters can be finite-, high-, or
infinite-dimensional. A mixture of point masses at zero and continuous
distributions is used for the prior distribution on sparse regression
coefficients, and appropriate prior distributions are used for nuisance
parameters. The optimal posterior contraction of sparse regression
coefficients, hampered by the presence of nuisance parameters, is also examined
and discussed. It is shown that the procedure yields strong model selection
consistency. A Bernstein-von Mises-type theorem for sparse regression
coefficients is also obtained for uncertainty quantification through credible
sets with guaranteed frequentist coverage. Asymptotic properties of numerous
examples are investigated using the theories developed in this study.
"
"stat.TH","  We develop random graph models where graphs are generated by connecting not
only pairs of vertices by edges but also larger subsets of vertices by copies
of small atomic subgraphs of arbitrary topology. This allows the for the
generation of graphs with extensive numbers of triangles and other network
motifs commonly observed in many real world networks. More specifically we
focus on maximum entropy ensembles under constraints placed on the counts and
distributions of atomic subgraphs and derive general expressions for the
entropy of such models. We also present a procedure for combining distributions
of multiple atomic subgraphs that enables the construction of models with fewer
parameters. Expanding the model to include atoms with edge and vertex labels we
obtain a general class of models that can be parametrized in terms of basic
building blocks and their distributions that includes many widely used models
as special cases. These models include random graphs with arbitrary
distributions of subgraphs, random hypergraphs, bipartite models, stochastic
block models, models of multilayer networks and their degree corrected and
directed versions. We show that the entropy for all these models can be derived
from a single expression that is characterized by the symmetry groups of atomic
subgraphs.
"
"stat.TH","  In the phase retrieval problem one seeks to recover an unknown $n$
dimensional signal vector $\mathbf{x}$ from $m$ measurements of the form $y_i =
|(\mathbf{A} \mathbf{x})_i|$ where $\mathbf{A}$ denotes the sensing matrix. A
popular class of algorithms for this problem are based on approximate message
passing. For these algorithms, it is known that if the sensing matrix
$\mathbf{A}$ is generated by sub-sampling $n$ columns of a uniformly random
(i.e. Haar distributed) orthogonal matrix, in the high dimensional asymptotic
regime ($m,n \rightarrow \infty, n/m \rightarrow \kappa$), the dynamics of the
algorithm are given by a deterministic recursion known as the state evolution.
For the special class of linearized message passing algorithms, we show that
the state evolution is universal: it continues to hold even when $\mathbf{A}$
is generated by randomly sub-sampling columns of certain deterministic
orthogonal matrices such as the Hadamard-Walsh matrix, provided the signal is
drawn from a Gaussian prior.
"
"stat.TH","  In this paper, we study smooth stochastic multi-level composition
optimization problems, where the objective function is a nested composition of
$T$ functions. We assume access to noisy evaluations of the functions and their
gradients, through a stochastic first-order oracle. For solving this class of
problems, we propose two algorithms using moving-average stochastic estimates,
and analyze their convergence to an $\epsilon$-stationary point of the problem.
We show that the first algorithm, which is a generalization of [22] to the $T$
level case, can achieve a sample complexity of $\mathcal{O}(1/\epsilon^6)$ by
using mini-batches of samples in each iteration. By modifying this algorithm
using linearized stochastic estimates of the function values, we improve the
sample complexity to $\mathcal{O}(1/\epsilon^4)$. This modification also
removes the requirement of having a mini-batch of samples in each iteration. To
the best of our knowledge, this is the first time that such an online algorithm
designed for the (un)constrained multi-level setting, obtains the same sample
complexity of the smooth single-level setting, under mild assumptions on the
stochastic first-order oracle.
"
"stat.TH","  This review paper provides an introduction of Markov chains and their
convergence rates which is an important and interesting mathematical topic
which also has important applications for very widely used Markov chain Monte
Carlo (MCMC) algorithm. We first discuss eigenvalue analysis for Markov chains
on finite state spaces. Then, using the coupling construction, we prove two
quantitative bounds based on minorization condition and drift conditions, and
provide descriptive and intuitive examples to showcase how these theorems can
be implemented in practice. This paper is meant to provide a general overview
of the subject and spark interest in new Markov chain research areas.
"
"stat.TH","  We consider the Graph Ornstein-Uhlenbeck (GrOU) process observed on a
non-uniform discrete time grid and introduce discretised maximum likelihood
estimators with parameters specific to the whole graph or specific to each
component, or node. Under a high-frequency sampling scheme, we study the
asymptotic behaviour of those estimators as the mesh size of the observation
grid goes to zero. We prove two stable central limit theorems to the same
distribution as in the continuously-observed case under both finite and
infinite jump activity for the L\'evy driving noise. When a graph structure is
not explicitly available, the stable convergence allows to consider
purpose-specific sparse inference procedures, i.e. pruning, on the edges
themselves in parallel to the GrOU inference and preserve its asymptotic
properties. We apply the new estimators to wind capacity factor measurements,
i.e. the ratio between the wind power produced locally compared to its rated
peak power, across fifty locations in Northern Spain and Portugal. We show the
superiority of those estimators compared to the standard least squares
estimator through a simulation study extending known univariate results across
graph configurations, noise types and amplitudes.
"
"stat.TH","  The paper deals with the problem of nonparametric estimating the $L_p$--norm,
$p\in (1,\infty)$, of a probability density on $R^d$, $d\geq 1$ from
independent observations. The unknown density %to be estimated is assumed to
belong to a ball in the anisotropic Nikolskii's space. We adopt the minimax
approach, and derive lower bounds on the minimax risk. In particular, we
demonstrate that accuracy of estimation procedures essentially depends on
whether $p$ is integer or not. Moreover, we develop a general technique for
derivation of lower bounds on the minimax risk in the problems of estimating
nonlinear functionals. The proposed technique is applicable for a broad class
of nonlinear functionals, and it is used for derivation of the lower bounds in
the~$L_p$--norm estimation.
"
"stat.TH","  In this paper we develop rate--optimal estimation procedures in the problem
of estimating the $L_p$--norm, $p\in (0, \infty)$ of a probability density from
independent observations. The density is assumed to be defined on $R^d$, $d\geq
1$ and to belong to a ball in the anisotropic Nikolskii space. We adopt the
minimax approach and construct rate--optimal estimators in the case of integer
$p\geq 2$. We demonstrate that, depending on parameters of Nikolskii's class
and the norm index $p$, the risk asymptotics ranges from inconsistency to
$\sqrt{n}$--estimation. The results in this paper complement the minimax lower
bounds derived in the companion paper \cite{gl20}.
"
"stat.TH","  We propose a nonparametric two-sample test procedure based on Maximum Mean
Discrepancy (MMD) for testing the hypothesis that two samples of functions have
the same underlying distribution, using kernels defined on function spaces.
This construction is motivated by a scaling analysis of the efficiency of
MMD-based tests for datasets of increasing dimension. Theoretical properties of
kernels on function spaces and their associated MMD are established and
employed to ascertain the efficacy of the newly proposed test, as well as to
assess the effects of using functional reconstructions based on discretised
function samples. The theoretical results are demonstrated over a range of
synthetic and real world datasets.
"
"stat.TH","  We develop an inference method for a (sub)vector of parameters identified by
conditional moment restrictions, which are implied by economic models such as
rational behavior and Euler equations. Building on Bierens (1990), we propose
penalized maximum statistics and combine bootstrap inference with model
selection. Our method is optimized to be powerful against a set of local
alternatives of interest by solving a data-dependent max-min problem for tuning
parameter selection. We demonstrate the efficacy of our method by a proof of
concept using two empirical examples: rational unbiased reporting of ability
status and the elasticity of intertemporal substitution.
"
"stat.TH","  One of the fundamental tasks in quantum metrology is to estimate multiple
parameters embedded in a noisy process, i.e., a quantum channel. In this paper,
we study fundamental limits to quantum channel estimation via the concept of
amortization and the right logarithmic derivative (RLD) Fisher information
value. Our key technical result is the proof of a chain-rule inequality for the
RLD Fisher information value, which implies that amortization, i.e., access to
a catalyst state family, does not increase the RLD Fisher information value of
quantum channels. This technical result leads to a fundamental and efficiently
computable limitation for multiparameter channel estimation in the sequential
setting, in terms of the RLD Fisher information value. As a consequence, we
conclude that if the RLD Fisher information value is finite, then Heisenberg
scaling is unattainable in the multiparameter setting.
"
"stat.TH","  Chatterjee (2020) introduced a simple new rank correlation coefficient that
has attracted much recent attention. The coefficient has the unusual appeal
that it not only estimates a population quantity that is zero if and only if
the underlying pair of random variables is independent, but also is
asymptotically normal under independence. This paper compares Chatterjee's new
coefficient to three established rank correlations that also facilitate
consistent tests of independence, namely, Hoeffding's $D$,
Blum-Kiefer-Rosenblatt's $R$, and Bergsma-Dassios-Yanagimoto's $\tau^*$. We
contrast their computational efficiency in light of recent advances, and
investigate their power against local alternatives. Our main results show that
Chatterjee's coefficient is unfortunately rate sub-optimal compared to $D$,
$R$, and $\tau^*$. The situation is similar but more subtle for a related
earlier estimator of Dette et al. (2013). These results favor $D$, $R$, and
$\tau^*$ over Chatterjee's new coefficient for the purpose of testing
independence.
"
"stat.TH","  This paper introduces an extension to the normal distribution through the
polar method to capture bimodality and asymmetry, which are often observed
characteristics of empirical data. The later two features are entirely
controlled by a separate scalar parameter. Explicit expressions for the
cumulative distribution function, the density function and the moments were
derived. The stochastic representation of the distribution facilitates
implementing Bayesian estimation via the Markov chain Monte Carlo methods. Some
real-life data as well as simulated data are analyzed to illustrate the
flexibility of the distribution for modeling asymmetric bimodality.
"
"stat.TH","  This paper studies Bayesian nonparametric estimation of a binary regression
function in a semi-supervised setting. We assume that the features are
supported on a hidden manifold, and use unlabeled data to construct a sequence
of graph-based priors over the regression function restricted to the given
features. We establish contraction rates for the corresponding graph-based
posteriors, interpolated to be supported over regression functions on the
underlying manifold. Minimax optimal contraction rates are achieved under
certain conditions. Our results provide novel understanding on why and how
unlabeled data are helpful in Bayesian semi-supervised classification.
"
"stat.TH","  A generic out-of-sample error estimate is proposed for robust $M$-estimators
regularized with a convex penalty in high-dimensional linear regression where
$(X,y)$ is observed and $p,n$ are of the same order. If $\psi$ is the
derivative of the robust data-fitting loss $\rho$, the estimate depends on the
observed data only through the quantities $\hat\psi = \psi(y-X\hat\beta)$,
$X^\top \hat\psi$ and the derivatives $(\partial/\partial y) \hat\psi$ and
$(\partial/\partial y) X\hat\beta$ for fixed $X$.
  The out-of-sample error estimate enjoys a relative error of order $n^{-1/2}$
in a linear model with Gaussian covariates and independent noise, either
non-asymptotically when $p/n\le \gamma$ or asymptotically in the
high-dimensional asymptotic regime $p/n\to\gamma'\in(0,\infty)$. General
differentiable loss functions $\rho$ are allowed provided that $\psi=\rho'$ is
1-Lipschitz. The validity of the out-of-sample error estimate holds either
under a strong convexity assumption, or for the $\ell_1$-penalized Huber
M-estimator if the number of corrupted observations and sparsity of the true
$\beta$ are bounded from above by $s_*n$ for some small enough constant
$s_*\in(0,1)$ independent of $n,p$.
  For the square loss and in the absence of corruption in the response, the
results additionally yield $n^{-1/2}$-consistent estimates of the noise
variance and of the generalization error. This generalizes, to arbitrary convex
penalty, estimates that were previously known for the Lasso.
"
"stat.TH","  Approximate Message Passing (AMP) algorithms have seen widespread use across
a variety of applications. However, the precise forms for their Onsager
corrections and state evolutions depend on properties of the underlying random
matrix ensemble, limiting the extent to which AMP algorithms derived for white
noise may be applicable to data matrices that arise in practice.
  In this work, we study more general AMP algorithms for random matrices $W$
that satisfy orthogonal rotational invariance in law, where $W$ may have a
spectral distribution that is different from the semicircle and Marcenko-Pastur
laws characteristic of white noise. The Onsager corrections and state
evolutions in these algorithms are defined by the free cumulants or rectangular
free cumulants of the spectral distribution of $W$. Their forms were derived
previously by Opper, \c{C}akmak, and Winther using non-rigorous dynamic
functional theory techniques, and we provide rigorous proofs.
  Our motivating application is a Bayes-AMP algorithm for Principal Components
Analysis, when there is prior structure for the principal components (PCs) and
possibly non-white noise. For sufficiently large signal strengths and any
non-Gaussian prior distributions for the PCs, we show that this algorithm
provably achieves higher estimation accuracy than the sample PCs.
"
"stat.TH","  In this paper, we study the asymptotic behavior of the extreme eigenvalues
and eigenvectors of the high dimensional spiked sample covariance matrices, in
the supercritical case when a reliable detection of spikes is possible.
Especially, we derive the joint distribution of the extreme eigenvalues and the
generalized components of the associated eigenvectors, i.e., the projections of
the eigenvectors onto arbitrary given direction, assuming that the dimension
and sample size are comparably large. In general, the joint distribution is
given in terms of linear combinations of finitely many Gaussian and Chi-square
variables, with parameters depending on the projection direction and the
spikes. Our assumption on the spikes is fully general. First, the strengths of
spikes are only required to be slightly above the critical threshold and no
upper bound on the strengths is needed. Second, multiple spikes, i.e., spikes
with the same strength, are allowed. Third, no structural assumption is imposed
on the spikes. Thanks to the general setting, we can then apply the results to
various high dimensional statistical hypothesis testing problems involving both
the eigenvalues and eigenvectors. Specifically, we propose accurate and
powerful statistics to conduct hypothesis testing on the principal components.
These statistics are data-dependent and adaptive to the underlying true spikes.
Numerical simulations also confirm the accuracy and powerfulness of our
proposed statistics and illustrate significantly better performance compared to
the existing methods in the literature. Especially, our methods are accurate
and powerful even when either the spikes are small or the dimension is large.
"
"stat.TH","  Local depth functions (LDFs) are used for describing the local geometric
features of multivariate distributions, especially in multimodal models. In
this paper, we undertake a rigorous systematic study of the LDFs and use it to
develop a theoretically validated algorithm for clustering. For this reason, we
establish several analytical and statistical properties of LDFs. First, we show
that, when the underlying probability distribution is absolutely continuous,
under an appropriate scaling that converge to zero (referred to as extreme
localization), LDFs converge uniformly to a power of the density and obtain a
related rate of convergence result. Second, we establish that the centered and
scaled sample LDFs converge in distribution to a centered Gaussian process,
uniformly in the space of bounded functions on R p x [0,infinity], as the
sample size diverges to infinity. Third, under an extreme localization that
depends on the sample size, we determine the correct centering and scaling for
the sample LDFs to possess a limiting normal distribution. Fourth, invoking the
above results, we develop a new clustering algorithm that uses the LDFs and
their differentiability properties. Fifth, for the last purpose, we establish
several results concerning the gradient systems related to LDFs. Finally, we
illustrate the finite sample performance of our results using simulations and
apply them to two datasets.
"
"stat.TH","  Recent years have witnessed the success of adaptive (or unified) approaches
in estimating symmetric properties of discrete distributions, where one first
obtains a distribution estimator independent of the target property, and then
plugs the estimator into the target property as the final estimator. Several
such approaches have been proposed and proved to be adaptively optimal, i.e.
they achieve the optimal sample complexity for a large class of properties
within a low accuracy, especially for a large estimation error $\varepsilon\gg
n^{-1/3}$ where $n$ is the sample size.
  In this paper, we characterize the high accuracy limitation, or the penalty
for adaptation, for all such approaches. Specifically, we show that under a
mild assumption that the distribution estimator is close to the true sorted
distribution in expectation, any adaptive approach cannot achieve the optimal
sample complexity for every $1$-Lipschitz property within accuracy $\varepsilon
\ll n^{-1/3}$. In particular, this result disproves a conjecture in [Acharya et
al. 2017] that the profile maximum likelihood (PML) plug-in approach is optimal
in property estimation for all ranges of $\varepsilon$, and confirms a
conjecture in [Han and Shiragur, 2020] that their competitive analysis of the
PML is tight.
"
"stat.TH","  We extend the notion of jittered sampling to arbitrary partitions and study
the discrepancy of the related point sets. Let
$\mathbf{Omega}=(\Omega_1,\ldots,\Omega_N)$ be a partition of $[0,1]^d$ and let
the $i$th point in $\mathcal{P}$ be chosen uniformly in the $i$th set of the
partition (and stochastically independent of the other points), $i=1,\ldots,N$.
For the study of such sets we introduce the concept of a uniformly distributed
triangular array and compare this notion to related notions in the literature.
We prove that the expected ${\mathcal{L}_p}$-discrepancy, $\mathbb{E}
{\mathcal{L}_p}(\mathcal{P}_{\mathbf{Omega}})^p$, of a point set
$\mathcal{P}_\mathbf{Omega}$ generated from any equivolume partition
$\mathbf{Omega}$ is always strictly smaller than the expected
${\mathcal{L}_p}$-discrepancy of a set of $N$ uniform random samples for $p>1$.
For fixed $N$ we consider classes of stratified samples based on equivolume
partitions of the unit cube into convex sets or into sets with a uniform
positive lower bound on their reach. It is shown that these classes contain at
least one minimizer of the expected ${\mathcal{L}_p}$-discrepancy. We
illustrate our results with explicit constructions for small $N$. In addition,
we present a family of partitions that seems to improve the expected
discrepancy of Monte Carlo sampling by a factor of 2 for every $N$.
"
"stat.TH","  The linear conditional expectation (LCE) provides a best linear (or rather,
affine) estimate of the conditional expectation and hence plays an important
r\^ole in approximate Bayesian inference, especially the Bayes linear approach.
This article establishes the analytical properties of the LCE in an
infinite-dimensional Hilbert space context. In addition, working in the space
of affine Hilbert--Schmidt operators, we establish a regularisation procedure
for this LCE. As an important application, we obtain a simple alternative
derivation and intuitive justification of the conditional mean embedding
formula, a concept widely used in machine learning to perform the conditioning
of random variables by embedding them into reproducing kernel Hilbert spaces.
"
"stat.TH","  We propose generalizations of the Hotelling's $T^2$ statistic and the
Bhattacharayya distance for data taking values in Lie groups. A key feature of
the derived measures is that they are compatible with the group structure even
for manifolds that do not admit any bi-invariant metric. This property, e.g.,
assures analysis that does not depend on the reference shape, thus, preventing
bias due to arbitrary choices thereof. Furthermore, the generalizations agree
with the common definitions for the special case of flat vector spaces
guaranteeing consistency. Employing a permutation test setup, we further obtain
nonparametric, two-sample testing procedures that themselves are bi-invariant
and consistent. We validate our method in group tests revealing significant
differences in hippocampal shape between individuals with mild cognitive
impairment and normal controls.
"
"stat.TH","  The question of fast convergence in the classical problem of high dimensional
linear regression has been extensively studied. Arguably, one of the fastest
procedures in practice is Iterative Hard Thresholding (IHT). Still, IHT relies
strongly on the knowledge of the true sparsity parameter $s$. In this paper, we
present a novel fast procedure for estimation in the high dimensional linear
regression. Taking advantage of the interplay between estimation, support
recovery and optimization we achieve both optimal statistical accuracy and fast
convergence. The main advantage of our procedure is that it is fully adaptive,
making it more practical than state of the art IHT methods. Our procedure
achieves optimal statistical accuracy faster than, for instance, classical
algorithms for the Lasso. Moreover, we establish sharp optimal results for both
estimation and support recovery. As a consequence, we present a new iterative
hard thresholding algorithm for high dimensional linear regression that is
scaled minimax optimal (achieves the estimation error of the oracle that knows
the sparsity pattern if possible), fast and adaptive.
"
"stat.TH","  This paper focuses on the non-asymptotic concentration of the heteroskedastic
Wishart-type matrices. Suppose $Z$ is a $p_1$-by-$p_2$ random matrix and
$Z_{ij} \sim N(0,\sigma_{ij}^2)$ independently, we prove that
  \begin{equation*}
  \bbE \left\|ZZ^\top - \bbE ZZ^\top\right\| \leq
(1+\epsilon)\left\{2\sigma_C\sigma_R + \sigma_C^2 +
C\sigma_R\sigma_*\sqrt{\log(p_1 \wedge p_2)} + C\sigma_*^2\log(p_1 \wedge
p_2)\right\},
  \end{equation*}
  where $\sigma_C^2 := \max_j \sum_{i=1}^{p_1}\sigma_{ij}^2$, $\sigma_R^2 :=
\max_i \sum_{j=1}^{p_2}\sigma_{ij}^2$ and $\sigma_*^2 :=
\max_{i,j}\sigma_{ij}^2$. A minimax lower bound is developed that matches this
upper bound. Then, we derive the concentration inequalities, moments, and tail
bounds for the heteroskedastic Wishart-type matrix under more general
distributions, such as sub-Gaussian and heavy-tailed distributions. Next, we
consider the cases where $Z$ has homoskedastic columns or rows (i.e.,
$\sigma_{ij} \approx \sigma_i$ or $\sigma_{ij} \approx \sigma_j$) and derive
the rate-optimal Wishart-type concentration bounds. Finally, we apply the
developed tools to identify the sharp signal-to-noise ratio threshold for
consistent clustering in the heteroskedastic clustering problem.
"
"stat.TH","  In this paper, we consider an inference problem for the first order
autoregressive process driven by a long memory stationary Gaussian process.
Suppose that the covariance function of the noise can be expressed as
$\abs{k}^{2H-2}$ times a function slowly varying at infinity. The fractional
Gaussian noise and the fractional ARIMA model and some others Gaussian noise
are special examples that satisfy this assumption. We propose a second moment
estimator and prove the strong consistency and give the asymptotic
distribution. Moreover, when the limit distribution is Gaussian, we give the
upper Berry-Ess\'een bound by means of Fourth moment theorem.
"
"stat.TH","  Local differential privacy has become the gold-standard of privacy literature
for gathering or releasing sensitive individual data points in a
privacy-preserving manner. However, locally differential data can twist the
probability density of the data because of the additive noise used to ensure
privacy. In fact, the density of privacy-preserving data (no matter how many
samples we gather) is always flatter in comparison with the density function of
the original data points due to convolution with privacy-preserving noise
density function. The effect is especially more pronounced when using
slow-decaying privacy-preserving noises, such as the Laplace noise. This can
result in under/over-estimation of the heavy-hitters. This is an important
challenge facing social scientists due to the use of differential privacy in
the 2020 Census in the United States. In this paper, we develop density
estimation methods using smoothing kernels. We use the framework of
deconvoluting kernel density estimators to remove the effect of
privacy-preserving noise. This approach also allows us to adapt the results
from non-parameteric regression with errors-in-variables to develop regression
models based on locally differentially private data. We demonstrate the
performance of the developed methods on financial and demographic datasets.
"
"stat.TH","  Let ${\mbox{$\mbox{\boldmath $f$}$}}$ be a square-integrable, zero-mean,
random vector with observable realizations in a Hilbert space $H$, and let
${\mbox{$\mbox{\boldmath $g$}$}}$ be an associated square-integrable,
zero-mean, random vector with realizations, which are not observable, in a
Hilbert space $K$. We seek an optimal filter in the form of a closed linear
operator $X$ acting on the observable realizations of a proximate vector
${\mbox{$\mbox{\boldmath $f$}$}}_{\epsilon} \approx {\mbox{$\mbox{\boldmath
$f$}$}}$ that provides the best estimate $\widehat{{\mbox{$\mbox{\boldmath
$g$}$}}}_{\epsilon} = X {\mbox{$\mbox{\boldmath $f$}$}}_{\epsilon}$ of the
vector ${\mbox{$\mbox{\boldmath $f$}$}}$. We assume the required covariance
operators are known. The results are illustrated with a typical example.
"
"stat.TH","  We refer by threshold Ornstein-Uhlenbeck to a continuous-time threshold
autoregressive process. It follows the Ornstein-Uhlenbeck dynamics when above
or below a fixed level, yet at this level (threshold) its coefficients can be
discontinuous. We discuss (quasi)-maximum likelihood estimation of the drift
parameters, both assuming continuous and discrete time observations. In the
ergodic case, we derive consistency and speed of convergence of these
estimators in long time and high frequency. Based on these results, we develop
a heuristic test for the presence of a threshold in the dynamics. Finally, we
apply these statistical tools to short-term US interest rates modeling.
"
"stat.TH","  The $p$-tensor Ising model is a one-parameter discrete exponential family for
modeling dependent binary data, where the sufficient statistic is a
multi-linear form of degree $p \geq 2$. This is a natural generalization of the
matrix Ising model, that provides a convenient mathematical framework for
capturing higher-order dependencies in complex relational data. In this paper,
we consider the problem of estimating the natural parameter of the $p$-tensor
Ising model given a single sample from the distribution on $N$ nodes. Our
estimate is based on the maximum pseudo-likelihood (MPL) method, which provides
a computationally efficient algorithm for estimating the parameter that avoids
computing the intractable partition function. We derive general conditions
under which the MPL estimate is $\sqrt N$-consistent, that is, it converges to
the true parameter at rate $1/\sqrt N$. In particular, we show the $\sqrt
N$-consistency of the MPL estimate in the $p$-spin Sherrington-Kirkpatrick (SK)
model, spin systems on general $p$-uniform hypergraphs, and Ising models on the
hypergraph stochastic block model (HSBM). In fact, for the HSBM we pin down the
exact location of the phase transition threshold, which is determined by the
positivity of a certain mean-field variational problem, such that above this
threshold the MPL estimate is $\sqrt N$-consistent, while below the threshold
no estimator is consistent. Finally, we derive the precise fluctuations of the
MPL estimate in the special case of the $p$-tensor Curie-Weiss model. An
interesting consequence of our results is that the MPL estimate in the
Curie-Weiss model saturates the Cramer-Rao lower bound at all points above the
estimation threshold, that is, the MPL estimate incurs no loss in asymptotic
efficiency, even though it is obtained by minimizing only an approximation of
the true likelihood function for computational tractability.
"
"stat.TH","  Many scientific and economic applications involve the analysis of
high-dimensional functional time series, which stands at the intersection
between functional time series and high-dimensional statistics gathering
challenges of infinite-dimensionality with serial dependence and
non-asymptotics. In this paper, we model observed functional time series, which
are subject to errors in the sense that each functional datum arises as the sum
of two uncorrelated components, one dynamic and one white noise. Motivated from
a simple fact that the autocovariance function of observed functional time
series automatically filters out the noise term, we propose an
autocovariance-based three-step procedure by first performing
autocovariance-based dimension reduction and then formulating a novel
autocovariance-based block regularized minimum distance (RMD) estimation
framework to produce block sparse estimates, from which we can finally recover
functional sparse estimates. We investigate non-asymptotic properties of
relevant estimated terms under such autocovariance-based dimension reduction
framework. To provide theoretical guarantees for the second step, we also
present convergence analysis of the block RMD estimator. Finally, we illustrate
the proposed autocovariance-based learning framework using applications of
three sparse high-dimensional functional time series models. With derived
theoretical results, we study convergence properties of the associated
estimators. We demonstrate via simulated and real datasets that our proposed
estimators significantly outperform the competitors.
"
"stat.TH","  In this paper, we consider the problem of recovering a sparse signal from
noisy linear measurements using the so called LASSO formulation. We assume a
correlated Gaussian design matrix with additive Gaussian noise. We precisely
analyze the high dimensional asymptotic performance of the LASSO under
correlated design matrices using the Convex Gaussian Min-max Theorem (CGMT). We
define appropriate performance measures such as the mean-square error (MSE),
probability of support recovery, element error rate (EER) and cosine
similarity. Numerical simulations are presented to validate the derived
theoretical results.
"
"stat.TH","  In an instrumental variable model, the score statistic can be stochastically
bounded for any alternative in parts of the parameter space. These regions
involve a constraint on the first-stage regression coefficients and the
reduced-form covariance matrix. As a consequence, the Lagrange Multiplier (LM)
test can have power close to size, despite being efficient under standard
asymptotics. This loss of information limits the power of conditional tests
which use only the Anderson-Rubin (AR) and the score statistic. In particular,
the conditional quasi-likelihood ratio (CQLR) test also suffers severe losses
because its power can be bounded for any alternative.
  A necessary condition for drastic power loss to occur is that the Hermitian
of the reduced-form covariance matrix has eigenvalues of opposite signs. These
cases are denoted impossibility designs or impossibility DGPs (ID). This
restriction cannot be satisfied with homoskedastic errors, but it can happen
with heteroskedastic, autocorrelated, and/or clustered (HAC) errors. We show
these situations can happen in practice, by applying our theory to the problem
of inference on the intertemporal elasticity of substitution (IES) with weak
instruments. Out of eleven countries studied by Yogo (2004) and Andrews (2016),
the data in nine of them are consistent with impossibility designs at the 95%
confidence level. For these countries, the noncentrality parameter of the score
statistic can be very close to zero. Therefore, the power loss is sufficiently
extensive to dissuade practitioners from blindly using LM-based tests with HAC
errors.
"
"stat.TH","  In this paper, we develop asymptotic theories for a class of latent variable
models for large-scale multi-relational networks. In particular, we establish
consistency results and asymptotic error bounds for the (penalized) maximum
likelihood estimators when the size of the network tends to infinity. The basic
technique is to develop a non-asymptotic error bound for the maximum likelihood
estimators through large deviations analysis of random fields. We also show
that these estimators are nearly optimal in terms of minimax risk.
"
"stat.TH","  We consider a sparse linear regression model with unknown symmetric error
under the high-dimensional setting. The true error distribution is assumed to
belong to the locally $\beta$-H\""{o}lder class with an exponentially decreasing
tail, which does not need to be sub-Gaussian. We obtain posterior convergence
rates of the regression coefficient and the error density, which are nearly
optimal and adaptive to the unknown sparsity level. Furthermore, we derive the
semi-parametric Bernstein-von Mises (BvM) theorem to characterize asymptotic
shape of the marginal posterior for regression coefficients. Under the
sub-Gaussianity assumption on the true score function, strong model selection
consistency for regression coefficients are also obtained, which eventually
asserts the frequentist's validity of credible sets.
"
"stat.TH","  In this short note we study how well a Gaussian distribution can be
approximated by distributions supported on $[-a,a]$. Perhaps, the natural
conjecture is that for large $a$ the almost optimal choice is given by
truncating the Gaussian to $[-a,a]$. Indeed, such approximation achieves the
optimal rate of $e^{-\Theta(a^2)}$ in terms of the $L_\infty$-distance between
characteristic functions. However, if we consider the $L_\infty$-distance
between Laplace transforms on a complex disk, the optimal rate is
$e^{-\Theta(a^2 \log a)}$, while truncation still only attains
$e^{-\Theta(a^2)}$. The optimal rate can be attained by the Gauss-Hermite
quadrature. As corollary, we also construct a ``super-flat'' Gaussian mixture
of $\Theta(a^2)$ components with means in $[-a,a]$ and whose density has all
derivatives bounded by $e^{-\Omega(a^2 \log(a))}$ in the $O(1)$-neighborhood of
the origin.
"
"stat.TH","  The question of testing for equality in distribution between two linear
models, each consisting of sums of distinct discrete independent random
variables with unequal numbers of observations, has emerged from the biological
research. In this case, the computation of classical $\chi^2$ statistics, which
would not include all observations, results in loss of power, especially when
sample sizes are small. Here, as an alternative that uses all data, the
nonparametric maximum likelihood estimator for the distribution of sum of
discrete and independent random variables, which we call the convolution
statistic, is proposed and its limiting normal covariance matrix determined. To
challenge null hypotheses about the distribution of this sum, the generalized
Wald's method is applied to define a testing statistic whose distribution is
asymptotic to a $\chi^2$ with as many degrees of freedom as the rank of such
covariance matrix. Rank analysis also reveals a connection with the roots of
the probability generating functions associated to the addend variables of the
linear models. A simulation study is performed to compare the convolution test
with Pearson's $\chi^2$, and to provide usage guidelines.
"
"stat.TH","  We study symmetric spiked matrix models with respect to a general class of
noise distributions. Given a rank-1 deformation of a random noise matrix, whose
entries are independently distributed with zero mean and unit variance, the
goal is to estimate the rank-1 part. For the case of Gaussian noise, the top
eigenvector of the given matrix is a widely-studied estimator known to achieve
optimal statistical guarantees, e.g., in the sense of the celebrated BBP phase
transition. However, this estimator can fail completely for heavy-tailed noise.
In this work, we exhibit an estimator that works for heavy-tailed noise up to
the BBP threshold that is optimal even for Gaussian noise. We give a
non-asymptotic analysis of our estimator which relies only on the variance of
each entry remaining constant as the size of the matrix grows: higher moments
may grow arbitrarily fast or even fail to exist. Previously, it was only known
how to achieve these guarantees if higher-order moments of the noises are
bounded by a constant independent of the size of the matrix. Our estimator can
be evaluated in polynomial time by counting self-avoiding walks via a color
-coding technique. Moreover, we extend our estimator to spiked tensor models
and establish analogous results.
"
"stat.TH","  Existing results for low-rank matrix recovery largely focus on quadratic
loss, which enjoys favorable properties such as restricted strong
convexity/smoothness (RSC/RSM) and well conditioning over all low rank
matrices. However, many interesting problems involve non-quadratic loss do not
satisfy such properties; examples including one-bit matrix sensing, one-bit
matrix completion, and rank aggregation. For these problems, standard nonconvex
approaches such as projected gradient with rank constraint alone (a.k.a.
iterative hard thresholding) and Burer-Monteiro approach may perform badly in
practice and have no satisfactory theory in guaranteeing global and efficient
convergence.
  In this paper, we show that the critical component in low-rank recovery with
non-quadratic loss is a regularity projection oracle, which restricts iterates
to low-rank matrix within an appropriate bounded set, over which the loss
function is well behaved and satisfies a set of relaxed RSC/RSM conditions.
Accordingly, we analyze an (averaged) projected gradient method equipped with
such an oracle, and prove that it converges globally and linearly. Our results
apply to a wide range of non-quadratic problems including rank aggregation, one
bit matrix sensing/completion, and more broadly generalized linear models with
rank constraint.
"
"stat.TH","  Consider a periodic, mean-reverting Ornstein-Uhlenbeck process
$X=\{X_t,t\geq0\}$ of the form $d X_{t}=\left(L(t)+\alpha X_{t}\right) d t+
dB^H_{t}, \quad t \geq 0$, where $L(t)=\sum_{i=1}^{p}\mu_i\phi_i (t)$ is a
periodic parametric function, and $\{B^H_t,t\geq0\}$ is a fractional Brownian
motion of Hurst parameter $\frac12\leq H<1$. In the ""ergodic"" case $\alpha<0$,
the parametric estimation of $(\mu_1,\ldots,\mu_p,\alpha)$ based on
continuous-time observation of $X$ has been considered in Dehling et al.
\cite{DFK}, and in Dehling et al. \cite{DFW} for $H=\frac12$, and
$\frac12<H<1$, respectively. In this paper we consider the ""non-ergodic"" case
$\alpha>0$, and for all $\frac12\leq H<1$. We analyze the strong consistency
and the asymptotic distribution for the estimator of
$(\mu_1,\ldots,\mu_p,\alpha)$ when the whole trajectory of $X$ is observed.
"
"stat.TH","  We prove the large-dimensional Gaussian approximation of a sum of $n$
independent random vectors in $\mathbb{R}^d$ together with fourth-moment error
bounds on convex sets and Euclidean balls. We show that compared with classical
third-moment bounds, our bounds can achieve improved and, in the case of balls,
optimal dependence $d=o(n)$ on dimension. We discuss an application to the
bootstrap. The proof is by recent advances in Stein's method.
"
"stat.TH","  We consider the edge statistics of large dimensional deformed rectangular
matrices of the form $Y_t=Y+\sqrt{t}X,$ where $Y$ is a $p \times n$
deterministic signal matrix whose rank is comparable to $n$, $X$ is a $p\times
n$ random noise matrix with centered i.i.d. entries with variance $n^{-1}$, and
$t>0$ gives the noise level. This model is referred to as the
interference-plus-noise matrix in the study of massive multiple-input
multiple-output (MIMO) system, which belongs to the category of the so-called
signal-plus-noise model. For the case $t=1$, the spectral statistics of this
model have been studied to a certain extent in the literature. In this paper,
we study the singular value and singular vector statistics of $Y_t$ around the
right-most edge of the singular value spectrum in the harder regime
$n^{-2/3}\ll t \ll 1$. This regime is harder than the $t=1$ case, because on
one hand, the edge behavior of the empirical spectral distribution (ESD) of
$YY^\top$ has a strong effect on the edge statistics of $Y_tY_t^\top$ since
$t\ll 1$ is ""small"", while on the other hand, the edge statistics of $Y_t$ is
also not merely a perturbation of those of $Y$ since $t\gg n^{-2/3}$ is
""large"". Under certain regularity assumptions on $Y,$ we prove the edge
universality, eigenvalues rigidity and eigenvector delocalization for the
matrices $Y_tY_t^\top$ and $Y_t^\top Y_t$. These results can be used to
estimate and infer the massive MIMO system. To prove the main results, we
analyze the edge behavior of the asymptotic ESD for $Y_tY_t^\top$, and
establish some sharp local laws on the resolvent of $Y_tY_t^\top$. These
results can be of independent interest, and used as useful inputs for many
other problems regarding the spectral statistics of $Y_t$.
"
"stat.TH","  This article introduces an informative goodness-of-fit (iGOF) approach to
study multivariate distributions. Conversely from standard goodness-of-fit
tests, when the null model is rejected, iGOF allows us to identify the
underlying sources of mismodelling and naturally equip practitioners with
additional insights on the underlying data distribution. The informative
character of the procedure proposed is achieved by introducing the \emph{joint
comparison density}. As a result, the methods presented here naturally extend
the seminal work of Parzen (1979) on univariate comparison distributions to the
multivariate setting. Simulation studies show that iGOF enjoys high power for
different types of alternatives.
"
"stat.TH","  Depth induced multivariate medians (multi-dimensional maximum depth
estimators) in regression serve as robust alternatives to the traditional least
squares and least absolute deviations estimators. The induced median
($\bs{\beta}^*_{RD}$) from regression depth (RD) of Rousseeuw and Hubert (1999)
(RH99) is one of the most prevailing estimators in regression.
  The maximum regression depth median possesses the outstanding robustness
similar to the univariate location counterpart. Indeed, the %maximum depth
estimator induced from $\mbox{RD}$, $\bs{\beta}^*_{RD}$ can, asymptotically,
resist up to $33\%$ contamination without breakdown, in contrast to the $0\%$
for the traditional estimators %(i.e. they could break down by a single bad
point) (see Van Aelst and Rousseeuw, 2000) (VAR00). The results from VAR00 are
pioneering and innovative, yet they are limited to regression symmetric
populations and the $\epsilon$-contamination and maximum bias model.
  With finite fixed sample size practice, the most prevailing measure of
robustness for estimators is the finite sample breakdown point (FSBP) (Donoho
(1982), Donoho and Huber (1983)). A lower bound of the FSBP for the
$\bs{\beta}^*_{RD}$ was given in RH99 (in a corollary of a conjecture).
  This article establishes a sharper lower bound and an upper bound of the FSBP
for the $\bs{\beta}^*_{RD}$, revealing an intrinsic connection between the
regression depth of $\bs{\beta}^*_{RD}$ and its FSBP, justifying the employment
of the $\bs{\beta}^*_{RD}$ as a robust alternative to the traditional
estimators and demonstrating the necessity and the merit of using FSBP in
finite sample real practice instead of an asymptotic breakdown value.
"
"stat.TH","  Wasserman et al. (2020, PNAS, vol. 117, pp. 16880-16890) constructed
estimator agnostic and finite-sample valid confidence sets and hypothesis
tests, using split-data likelihood ratio-based statistics. We demonstrate that
the same approach extends to the use of split-data composite likelihood ratios
as well, and thus establish universal methods for conducting multivariate
inference when the data generating process is only known up to marginal and
conditional relationships between the coordinates. Always-valid sequential
inference is also considered.
"
"stat.TH","  In various applications of regression analysis, in addition to errors in the
dependent observations also errors in the predictor variables play a
substantial role and need to be incorporated in the statistical modeling
process. In this paper we consider a nonparametric measurement error model of
Berkson type with fixed design regressors and centered random errors, which is
in contrast to much existing work in which the predictors are taken as random
observations with random noise. Based on an estimator that takes the error in
the predictor into account and on a suitable Gaussian approximation, we derive
%uniform confidence statements for the function of interest. In particular, we
provide finite sample bounds on the coverage error of uniform confidence bands,
where we circumvent the use of extreme-value theory and rather rely on recent
results on anti-concentration of Gaussian processes. In a simulation study we
investigate the performance of the uniform confidence sets for finite samples.
"
"stat.TH","  Multidimensional unfolding methods are widely used for visualizing item
response data. Such methods project respondents and items simultaneously onto a
low-dimensional Euclidian space, in which respondents and items are represented
by ideal points, with person-person, item-item, and person-item similarities
being captured by the Euclidian distances between the points. In this paper, we
study the visualization of multidimensional unfolding from a statistical
perspective. We cast multidimensional unfolding into an estimation problem,
where the respondent and item ideal points are treated as parameters to be
estimated. An estimator is then proposed for the simultaneous estimation of
these parameters. Asymptotic theory is provided for the recovery of the ideal
points, shedding lights on the validity of model-based visualization. An
alternating projected gradient descent algorithm is proposed for the parameter
estimation. We provide two illustrative examples, one on users' movie rating
and the other on senate roll call voting.
"
"stat.TH","  In this paper, we obtain a measure of inaccuracy between rth concomitant of
generalized order statistic and the parent random variable in Morgenstern
family. Applications of this result are given for concomitants of order
statistics and record values. We also study some results of cumulative past
inaccuracy (CPI) between the distribution function of rth concomitant of order
statistic (record value) and the distribution function of parent random
variable. Finally, we discuss on a problem of estimating the CPI by means of
the empirical CPI in concomitants of generalized order statistics.
"
"stat.TH","  In this lecture note, we discuss a fundamental concept, referred to as the
{\it characteristic rank}, which suggests a general framework for
characterizing the basic properties of various low-dimensional models used in
signal processing. Below, we illustrate this framework using two examples:
matrix and three-way tensor completion problems, and consider basic properties
include identifiability of a matrix or tensor, given partial observations. In
this note, we consider cases without observation noise to illustrate the
principle.
"
"stat.TH","  We propose a rigorous theoretical foundation for incorporating data of
observable and derivatives of any order in a Gaussian-random-field-based
surrogate model using tools in real analysis and probability. We demonstrate
that under some conditions, the random field representing the derivatives is a
Gaussian random field (GRF) given that its structure is derived from the GRF
regressing the data of the observable. We propose an augmented Gaussian random
field (AGRF) framework to unify these GRFs and calculate the prediction of this
surrogate model in a similar manner as the conventional Gaussian process
regression method. A prominent advantage of our method is that it can
incorporate arbitrary order derivatives and deal with missing data.
"
"stat.TH","  In many applications, data and/or parameters are supported on non-Euclidean
manifolds. It is important to take into account the geometric structure of
manifolds in statistical analysis to avoid misleading results. Although there
has been a considerable focus on simple and specific manifolds, there is a lack
of general and easy-to-implement statistical methods for density estimation and
modeling on manifolds. In this article, we consider a very broad class of
manifolds: non-compact Riemannian symmetric spaces. For this class, we provide
a very general mathematical result for easily calculating volume changes of the
exponential and logarithm map between the tangent space and the manifold. This
allows one to define statistical models on the tangent space, push these models
forward onto the manifold, and easily calculate induced distributions by
Jacobians. To illustrate the statistical utility of this theoretical result, we
provide a general method to construct distributions on symmetric spaces. In
particular, we define the log-Gaussian distribution as an analogue of the
multivariate Gaussian distribution in Euclidean space. With these new kernels
on symmetric spaces, we also consider the problem of density estimation. Our
proposed approach can use any existing density estimation approach designed for
Euclidean spaces and push it forward to the manifold with an easy-to-calculate
adjustment. We provide theorems showing that the induced density estimators on
the manifold inherit the statistical optimality properties of the parent
Euclidean density estimator; this holds for both frequentist and Bayesian
nonparametric methods. We illustrate the theory and practical utility of the
proposed approach on the space of positive definite matrices.
"
"stat.TH","  This paper provides a general framework for testing instrument validity in
heterogeneous causal effect models. We first generalize the testable
implications of the instrument validity assumption provided by Balke and Pearl
(1997), Imbens and Rubin (1997), and Heckman and Vytlacil (2005). The
generalization involves the cases where the treatment can be multivalued (and
ordered) or unordered, and there can be conditioning covariates. Based on these
testable implications, we propose a nonparametric test which is proved to be
asymptotically size controlled and consistent. Because of the nonstandard
nature of the problem in question, the test statistic is constructed based on a
nonsmooth map, which causes technical complications. We provide an extended
continuous mapping theorem and an extended delta method, which may be of
independent interest, to establish the asymptotic distribution of the test
statistic under null. We then extend the bootstrap method proposed by Fang and
Santos (2018) to approximate this asymptotic distribution and construct a
critical value for the test. Compared to the test proposed by Kitagawa (2015),
our test can be applied in more general settings and may achieve power
improvement. Evidence that the test performs well on finite samples is provided
via simulations. We revisit the empirical study of Card (1993) and use their
data to demonstrate application of the proposed test in practice. We show that
a valid instrument for a multivalued treatment may not remain valid if the
treatment is coarsened.
"
"stat.TH","  We consider the stochastic contextual bandit problem under the high
dimensional linear model. We focus on the case where the action space is finite
and random, with each action associated with a randomly generated contextual
covariate. This setting finds essential applications such as personalized
recommendation, online advertisement, and personalized medicine. However, it is
very challenging as we need to balance exploration and exploitation. We propose
doubly growing epochs and estimating the parameter using the best subset
selection method, which is easy to implement in practice. This approach
achieves $ \tilde{\mathcal{O}}(s\sqrt{T})$ regret with high probability, which
is nearly independent in the ``ambient'' regression model dimension $d$. We
further attain a sharper $\tilde{\mathcal{O}}(\sqrt{sT})$ regret by using the
\textsc{SupLinUCB} framework and match the minimax lower bound of
low-dimensional linear stochastic bandit problems. Finally, we conduct
extensive numerical experiments to demonstrate the applicability and robustness
of our algorithms empirically.
"
"stat.TH","  In this paper relations among some kinds of cumulative entropies and moments
of order statistics are presented. By using some characterizations and the
symmetry of a non negative and absolutely continuous random variable X, lower
and upper bounds for entropies are obtained and examples are given.
"
"stat.TH","  Recently Qiu et al. (2017) have introduced residual extropy as measure of
uncertainty in residual lifetime distributions analogues to residual entropy
(1996). Also, they obtained some properties and applications of that. In this
paper, we study the extropy to measure the uncertainty in a past lifetime
distribution. This measure of uncertainty is called past extropy. Also it is
showed a characterization result about the past extropy of largest order
statistics.
"
"stat.TH","  The Environment Kuznets Curve (EKC) predicts an inverted U-shaped
relationship between economic growth and environmental pollution. Current
analyses frequently employ models which restrict the nonlinearities in the data
to be explained by the economic growth variable only. We propose a Generalized
Cointegrating Polynomial Regression (GCPR) with flexible time trends to proxy
time effects such as technological progress and/or environmental awareness.
More specifically, a GCPR includes flexible powers of deterministic trends and
integer powers of stochastic trends. We estimate the GCPR by nonlinear least
squares and derive its asymptotic distribution. Endogeneity of the regressors
can introduce nuisance parameters into this limiting distribution but a
simulated approach nevertheless enables us to conduct valid inference.
Moreover, a subsampling KPSS test can be used to check the stationarity of the
errors. A comprehensive simulation study shows good performance of the
simulated inference approach and the subsampling KPSS test. We illustrate the
GCPR approach on a dataset of 18 industrialised countries containing GDP and
CO2 emissions. We conclude that: (1) the evidence for an EKC is significantly
reduced when a nonlinear time trend is included, and (2) a linear cointegrating
relation between GDP and CO2 around a power law trend also provides an accurate
description of the data.
"
"stat.TH","  We propose a new threshold selection method for the nonparametric estimation
of the extremal index of stochastic processes. The so-called discrepancy method
was proposed as a data-driven smoothing tool for estimation of a probability
density function. Now it is modified to select a threshold parameter of an
extremal index estimator. To this end, a specific normalization of the
discrepancy statistic based on the Cram\'{e}r-von Mises-Smirnov statistic
$\omega^2$ is calculated by the $k$ largest order statistics instead of an
entire sample. Its asymptotic distribution as $k\to\infty$ is proved to be the
same as the $\omega^2$-distribution. The quantiles of the latter distribution
are used as discrepancy values. The rate of convergence of an extremal index
estimate coupled with the discrepancy method is derived. The discrepancy method
is used as an automatic threshold selection for the intervals and $K-$gaps
estimators and it may be applied to other estimators of the extremal index.
"
"stat.TH","  We exhibit some strong positivity properties of a certain function which
implies a key inequality that in turn implies the lower bound formula for the
probability of correct selection in the Levin-Robbins-Leu family of sequential
subset selection procedures for binary outcomes. These properties provide a
more direct and comprehensive demonstration of the key inequality than was
discussed in previous work.
"
"stat.TH","  Motivated by models for multiway comparison data, we consider the problem of
estimating a coordinate-wise isotonic function on the domain $[0, 1]^d$ from
noisy observations collected on a uniform lattice, but where the design points
have been permuted along each dimension. While the univariate and bivariate
versions of this problem have received significant attention, our focus is on
the multivariate case $d \geq 3$. We study both the minimax risk of estimation
(in empirical $L_2$ loss) and the fundamental limits of adaptation (quantified
by the adaptivity index) to a family of piecewise constant functions. We
provide a computationally efficient Mirsky partition estimator that is minimax
optimal while also achieving the smallest adaptivity index possible for
polynomial time procedures. Thus, from a worst-case perspective and in sharp
contrast to the bivariate case, the latent permutations in the model do not
introduce significant computational difficulties over and above vanilla
isotonic regression. On the other hand, the fundamental limits of adaptation
are significantly different with and without unknown permutations: Assuming a
hardness conjecture from average-case complexity theory, a
statistical-computational gap manifests in the former case. In a complementary
direction, we show that natural modifications of existing estimators fail to
satisfy at least one of the desiderata of optimal worst-case statistical
performance, computational efficiency, and fast adaptation. Along the way to
showing our results, we improve adaptation results in the special case $d = 2$
and establish some properties of estimators for vanilla isotonic regression,
both of which may be of independent interest.
"
"stat.TH","  In this paper we present the asymptotic analysis of the realised quadratic
variation for multivariate symmetric $\beta$-stable L\'evy processes, $\beta
\in (0,2)$, and certain pure jump semimartingales. The main focus is on
derivation of functional limit theorems for the realised quadratic variation
and its spectrum. We will show that the limiting process is a matrix-valued
$\beta$-stable L\'evy process when the original process is symmetric
$\beta$-stable, while the limit is conditionally $\beta$-stable in case of
integrals with respect to symmetric $\beta$-stable motions. These asymptotic
results are mostly related to the work [5], which investigates the univariate
version of the problem. Furthermore, we will show the implications for
estimation of eigenvalues and eigenvectors of the quadratic variation matrix,
which is a useful result for the principle component analysis. Finally, we
propose a consistent subsampling procedure in the L\'evy setting to obtain
confidence regions.
"
"stat.TH","  E-values have gained attention as potential alternatives to p-values as
measures of uncertainty, significance and evidence. In brief, e-values are
realized by random variables with expectation at most one under the null;
examples include betting scores, (point null) Bayes factors, likelihood ratios
and stopped supermartingales. We design a natural analog of the
Benjamini-Hochberg (BH) procedure for false discovery rate (FDR) control that
utilizes e-values, called the e-BH procedure, and compare it with the standard
procedure for p-values. One of our central results is that, unlike the usual BH
procedure, the e-BH procedure controls the FDR at the desired level---with no
correction---for any dependence structure between the e-values. We illustrate
that the new procedure is convenient in various settings of complicated
dependence, structured and post-selection hypotheses, and multi-armed bandit
problems. Moreover, the BH procedure is a special case of the e-BH procedure
through calibration between p-values and e-values. Overall, the e-BH procedure
is a novel, powerful and general tool for multiple testing under dependence,
that is complementary to the BH procedure, each being an appropriate choice in
different applications.
"
"stat.TH","  We propose an iterative algorithm for low-rank matrix completion that can be
interpreted as both an iteratively reweighted least squares (IRLS) algorithm
and a saddle-escaping smoothing Newton method applied to a non-convex rank
surrogate objective. It combines the favorable data efficiency of previous IRLS
approaches with an improved scalability by several orders of magnitude. Our
method attains a local quadratic convergence rate already for a number of
samples that is close to the information theoretical limit. We show in
numerical experiments that unlike many state-of-the-art approaches, our
approach is able to complete very ill-conditioned matrices with a condition
number of up to $10^{10}$ from few samples.
"
"stat.TH","  Anomaly detection when observing a large number of data streams is essential
in a variety of applications, ranging from epidemiological studies to
monitoring of complex systems. High-dimensional scenarios are usually tackled
with scan-statistics and related methods, requiring stringent modeling
assumptions for proper calibration. In this work we take a non-parametric
stance, and propose a permutation-based variant of the higher criticism
statistic not requiring knowledge of the null distribution. This results in an
exact test in finite samples which is asymptotically optimal in the wide class
of exponential models. We demonstrate the power loss in finite samples is
minimal with respect to the oracle test. Furthermore, since the proposed
statistic does not rely on asymptotic approximations it typically performs
better than popular variants of higher criticism that rely on such
approximations. We include recommendations such that the test can be readily
applied in practice, and demonstrate its applicability in monitoring the daily
number of COVID-19 cases in the Netherlands.
"
"stat.TH","  We consider a hypothesis testing problem where a part of data cannot be
observed. Our helper observes the missed data and can send us a limited amount
of information about them. What kind of this limited information will allow us
to make the best statistical inference? In particular, what is the minimum
information sufficient to obtain the same results as if we directly observed
all the data? We derive estimates for this minimum information and some other
similar results.
"
"stat.TH","  Wald's anytime-valid $p$-values and Robbins' confidence sequences enable
sequential inference for composite and nonparametric classes of distributions
at arbitrary stopping times, as do more recent proposals involving Vovk's
`$e$-values' or Shafer's `betting scores'. Examining the literature, one finds
that at the heart of all these (quite different) approaches has been the
identification of composite nonnegative (super)martingales. Thus, informally,
nonnegative (super)martingales are known to be sufficient for \emph{valid}
sequential inference. Our central contribution is to show that martingales are
also universal---all \emph{admissible} constructions of (composite) anytime
$p$-values, confidence sequences, or $e$-values must necessarily utilize
nonnegative martingales (or so-called max-martingales in the case of
$p$-values). Sufficient conditions for composite admissibility are also
provided. Our proofs utilize a plethora of modern mathematical tools for
composite testing and estimation problems: max-martingales, Snell envelopes,
and new Doob-L\'evy martingales make appearances in previously unencountered
ways. Informally, if one wishes to perform anytime-valid sequential inference,
then any existing approach can be recovered or dominated using martingales. We
provide several sophisticated examples, with special focus on the nonparametric
problem of testing if a distribution is symmetric, where our new constructions
render past methods inadmissible.
"
"stat.TH","  Given observations from a stationary time series, permutation tests allow one
to construct exactly level $\alpha$ tests under the null hypothesis of an
i.i.d. (or, more generally, exchangeable) distribution. On the other hand, when
the null hypothesis of interest is that the underlying process is an
uncorrelated sequence, permutation tests are not necessarily level $\alpha$,
nor are they approximately level $\alpha$ in large samples. In addition,
permutation tests may have large Type 3, or directional, errors, in which a
two-sided test rejects the null hypothesis and concludes that the first-order
autocorrelation is larger than 0, when in fact it is less than 0. In this
paper, under weak assumptions on the mixing coefficients and moments of the
sequence, we provide a test procedure for which the asymptotic validity of the
permutation test holds, while retaining the exact rejection probability
$\alpha$ in finite samples when the observations are independent and
identically distributed. A Monte Carlo simulation study, comparing the
permutation test to other tests of autocorrelation, is also performed, along
with an empirical example of application to financial data.
"
"stat.TH","  This paper introduces a general framework for survival analysis based on
ordinary differential equations (ODE). Specifically, this framework unifies
many existing survival models, including proportional hazards models, linear
transformation models, accelerated failure time models, and time-varying
coefficient models as special cases. Such a unified framework provides a novel
perspective on modeling censored data and offers opportunities for designing
new and more flexible survival model structures. Further, the aforementioned
existing survival models are traditionally estimated by procedures that suffer
from lack of scalability, statistical inefficiency, or implementation
difficulty. Based on well-established numerical solvers and sensitivity
analysis tools for ODEs, we propose a novel, scalable, and easy-to-implement
general estimation procedure that is applicable to a wide range of models. In
particular, we develop a sieve maximum likelihood estimator for a general
semi-parametric class of ODE models as an illustrative example. We also
establish a general sieve M-theorem for bundled parameters and show that the
proposed sieve estimator is consistent and asymptotically normal, and achieves
the semi-parametric efficiency bound. The finite sample performance of the
proposed estimator is examined in simulation studies and a real-world data
example.
"
"stat.TH","  In this paper, we estimate the Shannon entropy $S(f) = -\E[ \log (f(x))]$ of
a one-sided linear process with probability density function $f(x)$. We employ
the integral estimator $S_n(f)$, which utilizes the standard kernel density
estimator $f_n(x)$ of $f(x)$. We show that $S_n (f)$ converges to $S(f)$ almost
surely and in $\L^2$ under reasonable conditions.
"
"stat.TH","  Motivated by practical machine learning applications, we revisit the outlying
sequence detection problem (Li \emph{et al.}, TIT 2014) and derive fundamental
limits of optimal detection when the reject option is allowed for outlying
sequences. In outlying sequence detection (OSD) one is given multiple observed
sequences, where most sequences are generated i.i.d. from a nominal
distribution. The task is to discern the set of outlying sequences that are
generated according to anomalous distributions. In OSD, the nominal and
anomalous distributions are \emph{unknown}. In this paper, we consider the case
where there is a reject option for the OSD, i.e., reject the samples as
insufficient for reliable outlying sequence detection (cf. Bartlett \emph{et
al.}, JMLR 2008). We study the tradeoff among the probabilities of
misclassification error, false alarm and false reject for tests that satisfy
weak conditions on the rate of decrease of these error probabilities as a
function of sequence length. We propose a second-order asymptotically optimal
test which provides a finite sample approximation. We first consider the case
of at most one outlying sequence and then generalize our results to multiple
outlying sequences where each outlying sequence can follow a different
anomalous distribution.
"
"stat.TH","  The purpose of this short note is to show that the Christoffel-Darboux
polynomial, useful in approximation theory and data science, arises naturally
when deriving the dual to the problem of semi-algebraic D-optimal experimental
design in statistics. It uses only elementary notions of convex analysis.
"
"stat.TH","  Combining information both within and across trajectories, we propose a
simple estimator for the local regularity of the trajectories of a stochastic
process. Independent trajectories are measured with errors at randomly sampled
time points. Non-asymptotic bounds for the concentration of the estimator are
derived. Given the estimate of the local regularity, we build a nearly optimal
local polynomial smoother from the curves from a new, possibly very large
sample of noisy trajectories. We derive non-asymptotic pointwise risk bounds
uniformly over the new set of curves. Our estimates perform well in
simulations. Real data sets illustrate the effectiveness of the new approaches.
"
"stat.TH","  This paper is devoted to the determination of the asymptotical optimal input
for the estimation of the drift parameter in a Directly observed but controlled
fractional Ornstein-Uhlenbeck process. Large sample asymptotical properties of
the Maximum Likelihood Estimator is deduced using the Laplace transform
computations.
"
"stat.TH","  Some crucial issues about a recently proposed estimator for the proportion of
true null hypotheses ($\pi_0$) under discrete setup are discussed. An estimator
for $\pi_0$ is introduced under the same setup. The estimator may be seen as a
modification of a very popular estimator for $\pi_0$, originally proposed under
the assumption of continuous test statistics. It is shown that adaptive
Benjamini-Hochberg procedure remains conservative with the new estimator for
$\pi_0$ being plugged in.
"
"stat.TH","  We argue that the likelihood principle (LP) and weak law of likelihood (LL)
generalize naturally to settings in which experimenters are justified only in
making comparative, non-numerical judgments of the form ""$A$ given $B$ is more
likely than $C$ given $D$."" To do so, we first \emph{formulate} qualitative
analogs of those theses. Then, using a framework for qualitative conditional
probability, just as the characterizes when all Bayesians (regardless of prior)
agree that two pieces of evidence are equivalent, so a
qualitative/non-numerical version of LP provides sufficient conditions for
agreement among experimenters' whose degrees of belief satisfy only very weak
""coherence"" constraints. We prove a similar result for LL. We conclude by
discussing the relevance of results to stopping rules.
"
"stat.TH","  We study the convergence rates of empirical Bayes posterior distributions for
nonparametric and high-dimensional inference. We show that as long as the
hyperparameter set is discrete, the empirical Bayes posterior distribution
induced by the maximum marginal likelihood estimator can be regarded as a
variational approximation to a hierarchical Bayes posterior distribution. This
connection between empirical Bayes and variational Bayes allows us to leverage
the recent results in the variational Bayes literature, and directly obtains
the convergence rates of empirical Bayes posterior distributions from a
variational perspective. For a more general hyperparameter set that is not
necessarily discrete, we introduce a new technique called ""prior decomposition""
to deal with prior distributions that can be written as convex combinations of
probability measures whose supports are low-dimensional subspaces. This leads
to generalized versions of the classical ""prior mass and testing"" conditions
for the convergence rates of empirical Bayes. Our theory is applied to a number
of statistical estimation problems including nonparametric density estimation
and sparse linear regression.
"
"stat.TH","  We consider the fundamental problem of matching a template to a signal. We do
so by M-estimation, which encompasses procedures that are robust to gross
errors (i.e., outliers). Using standard results from empirical process theory,
we derive the convergence rate and the asymptotic distribution of the
M-estimator under relatively mild assumptions. We also discuss the optimality
of the estimator, both in finite samples in the minimax sense and in the
large-sample limit in terms of local minimaxity and relative efficiency.
Although most of the paper is dedicated to the study of the basic shift model
in the context of a random design, we consider many extensions towards the end
of the paper, including more flexible templates, fixed designs, the agnostic
setting, and more.
"
"stat.TH","  Concerns have been expressed over the validity of statistical inference under
covariate-adaptive randomization despite the extensive use in clinical trials.
In the literature, the inferential properties under covariate-adaptive
randomization have been mainly studied for continuous responses; in particular,
it is well known that the usual two sample t-test for treatment effect is
typically conservative, in the sense that the actual test size is smaller than
the nominal level. This phenomenon of invalid tests has also been found for
generalized linear models without adjusting for the covariates and are
sometimes more worrisome due to inflated Type I error. The purpose of this
study is to examine the unadjusted test for treatment effect under generalized
linear models and covariate-adaptive randomization. For a large class of
covariate-adaptive randomization methods, we obtain the asymptotic distribution
of the test statistic under the null hypothesis and derive the conditions under
which the test is conservative, valid, or anti-conservative. Several commonly
used generalized linear models, such as logistic regression and Poisson
regression, are discussed in detail. An adjustment method is also proposed to
achieve a valid size based on the asymptotic results. Numerical studies confirm
the theoretical findings and demonstrate the effectiveness of the proposed
adjustment method.
"
"stat.TH","  Accounting for inequality constraints, such as boundedness, monotonicity or
convexity, is challenging when modeling costly-to-evaluate black box functions.
In this regard, finite-dimensional Gaussian process (GP) models bring a
valuable solution, as they guarantee that the inequality constraints are
satisfied everywhere. Nevertheless, these models are currently restricted to
small dimensional situations (up to dimension 5). Addressing this issue, we
introduce the MaxMod algorithm that sequentially inserts one-dimensional knots
or adds active variables, thereby performing at the same time dimension
reduction and efficient knot allocation. We prove the convergence of this
algorithm. In intermediary steps of the proof, we propose the notion of
multi-affine extension and study its properties. We also prove the convergence
of finite-dimensional GPs, when the knots are not dense in the input space,
extending the recent literature. With simulated and real data, we demonstrate
that the MaxMod algorithm remains efficient in higher dimension (at least in
dimension 20), and has a smaller computational complexity than other
constrained GP models from the state-of-the-art, to reach a given approximation
error.
"
"stat.TH","  In this study, we develop nonparametric analysis of deviance tools for
generalized partially linear models based on local polynomial fitting. Assuming
a canonical link, we propose expressions for both local and global analysis of
deviance, which admit an additivity property that reduces to analysis of
variance decompositions in the Gaussian case. Chi-square tests based on
integrated likelihood functions are proposed to formally test whether the
nonparametric term is significant. Simulation results are shown to illustrate
the proposed chi-square tests and to compare them with an existing procedure
based on penalized splines. The methodology is applied to German Bundesbank
Federal Reserve data.
"
"stat.TH","  Since Pearson's correlation was introduced at the end of the 19th century
many dependence measures have appeared in the literature. Recently we have
suggested four simple axioms for dependence measures of random variables that
take values in Hilbert spaces. We showed that distance correlation satisfies
all these axioms. We still need a new measure of dependence because existing
measures either do not work in general metric spaces (that are not Hilbert
spaces) or they do not satisfy our four simple axioms. The earth mover's
correlation introduced in this paper applies in general metric spaces and
satisfies our four axioms (two of them in a weaker form).
"
"stat.TH","  We use information from higher order moments to achieve identification of
non-Gaussian structural vector autoregressive moving average (SVARMA) models,
possibly non-fundamental or non-causal, through a frequency domain criterion
based on a new representation of the higher order spectral density arrays of
vector linear processes. This allows to identify the location of the roots of
the determinantal lag matrix polynomials based on higher order cumulants
dynamics and to identify the rotation of the model errors leading to the
structural shocks up to sign and permutation. We describe sufficient conditions
for global and local parameter identification that rely on simple rank
assumptions on the linear dynamics and on finite order serial and component
independence conditions for the structural innovations. We generalize previous
univariate analysis to develop asymptotically normal and efficient estimates
exploiting second and non-Gaussian higher order dynamics given a particular
structural shocks ordering without assumptions on causality or invertibility.
Bootstrap approximations to finite sample distributions and the properties of
numerical methods are explored with real and simulated data.
"
"stat.TH","  The Wasserstein distance provides a notion of dissimilarities between
probability measures, which has recent applications in learning of structured
data with varying size such as images and text documents. In this work, we
analyze the $k$-nearest neighbor classifier ($k$-NN) under the Wasserstein
distance and establish the universal consistency on families of distributions.
Using previous known results on the consistency of the $k$-NN classifier on
infinite dimensional metric spaces, it suffices to show that the families is a
countable union of finite dimensional components. As a result, we are able to
prove universal consistency of $k$-NN on spaces of finitely supported measures,
the space of finite wavelet series and the spaces of Gaussian measures with
commuting covariance matrices.
"
"stat.TH","  As in other estimation scenarios, likelihood based estimation in the normal
mixture set-up is highly non-robust against model misspecification and presence
of outliers (apart from being an ill-posed optimization problem). We propose a
robust alternative to the ordinary likelihood approach for this estimation
problem which performs simultaneous estimation and data clustering and leads to
subsequent anomaly detection. To invoke robustness, we follow, in spirit, the
methodology based on the minimization of the density power divergence (or
alternatively, the maximization of the $\beta$-likelihood) under suitable
constraints. An iteratively reweighted least squares approach has been followed
in order to compute our estimators for the component means (or equivalently
cluster centers) and component dispersion matrices which leads to simultaneous
data clustering. Some exploratory techniques are also suggested for anomaly
detection, a problem of great importance in the domain of statistics and
machine learning. Existence and consistency of the estimators are established
under the aforesaid constraints. We validate our method with simulation studies
under different set-ups; it is seen to perform competitively or better compared
to the popular existing methods like K-means and TCLUST, especially when the
mixture components (i.e., the clusters) share regions with significant overlap
or outlying clusters exist with small but non-negligible weights. Two real
datasets are also used to illustrate the performance of our method in
comparison with others along with an application in image processing. It is
observed that our method detects the clusters with lower misclassification
rates and successfully points out the outlying (anomalous) observations from
these datasets.
"
"stat.TH","  Under the framework of reproducing kernel Hilbert space (RKHS), we consider
the penalized least-squares of the partially functional linear models (PFLM),
whose predictor contains both functional and traditional multivariate part, and
the multivariate part allows a divergent number of parameters. From the
non-asymptotic point of view, we focus on the rate-optimal upper and lower
bounds of the prediction error. An exact upper bound for the excess prediction
risk is shown in a non-asymptotic form under a more general assumption known as
the effective dimension to the model, by which we also show the prediction
consistency when the number of multivariate covariates $p$ slightly increases
with the sample size $n$. Our new finding implies a trade-off between the
number of non-functional predictors and the effective dimension of the kernel
principal components to ensure the prediction consistency in the
increasing-dimensional setting. The analysis in our proof hinges on the
spectral condition of the sandwich operator of the covariance operator and the
reproducing kernel, and on the concentration inequalities for the random
elements in Hilbert space. Finally, we derive the non-asymptotic minimax lower
bound under the regularity assumption of Kullback-Leibler divergence of the
models.
"
"stat.TH","  First-order separability of a spatio-temporal point process plays a
fundamental role in the analysis of spatio-temporal point pattern data. While
it is often a convenient assumption that simplifies the analysis greatly,
existing non-separable structures should be accounted for in the model
construction. We propose three different tests to investigate this hypothesis
as a step of preliminary data analysis. The first two tests are exact or
asymptotically exact for Poisson processes. The first test based on
permutations and global envelopes allows us to detect at which spatial and
temporal locations or lags the data deviate from the null hypothesis. The
second test is a simple and computationally cheap $\chi^2$-test. The third test
is based on statistical reconstruction method and can be generally applied for
non-Poisson processes. The performance of the first two tests is studied in a
simulation study for Poisson and non-Poisson models. The third test is applied
to the real data of the UK 2001 epidemic foot and mouth disease.
"
"stat.TH","  In this paper, we investigate estimators for symmetric $\alpha$-stable CARMA
processes sampled equidistantly. Simulation studies suggest that the Whittle
estimator and the estimator presented in Garc\'{\i}a et al. (2011) are
consistent estimators for the parameters of stable CARMA processes. For CARMA
processes with finite second moments it is well-known that the Whittle
estimator is consistent and asymptotically normally distributed. Therefore, in
the light-tailed setting the properties of the Whittle estimator for CARMA
processes are similar to those of the Whittle estimator for ARMA processes.
However, in the present paper we prove that, in general, the Whittle estimator
for symmetric $\alpha$-stable CARMA processes sampled at low frequencies is not
consistent and highlight why simulation studies suggest something else. Thus,
in contrast to the light-tailed setting the properties of the Whittle estimator
for heavy-tailed ARMA processes can not be transferred to heavy-tailed CARMA
processes. We elaborate as well that the estimator presented in Garc\'{\i}a et
al. (2011) faces the same problems. However, the Whittle estimator for stable
CAR(1) processes is consistent.
"
"stat.TH","  Many modern applications involve the acquisition of noisy modulo samples of a
function $f$, with the goal being to recover estimates of the original samples
of $f$. For a Lipschitz function $f:[0,1]^d \to \mathbb{R}$, suppose we are
given the samples $y_i = (f(x_i) + \eta_i)\bmod 1; \quad i=1,\dots,n$ where
$\eta_i$ denotes noise. Assuming $\eta_i$ are zero-mean i.i.d Gaussian's, and
$x_i$'s form a uniform grid, we derive a two-stage algorithm that recovers
estimates of the samples $f(x_i)$ with a uniform error rate $O((\frac{\log
n}{n})^{\frac{1}{d+2}})$ holding with high probability. The first stage
involves embedding the points on the unit complex circle, and obtaining
denoised estimates of $f(x_i)\bmod 1$ via a $k$NN (nearest neighbor) estimator.
The second stage involves a sequential unwrapping procedure which unwraps the
denoised mod $1$ estimates from the first stage.
  Recently, Cucuringu and Tyagi proposed an alternative way of denoising modulo
$1$ data which works with their representation on the unit complex circle. They
formulated a smoothness regularized least squares problem on the product
manifold of unit circles, where the smoothness is measured with respect to the
Laplacian of a proximity graph $G$ involving the $x_i$'s. This is a nonconvex
quadratically constrained quadratic program (QCQP) hence they proposed solving
its semidefinite program (SDP) based relaxation. We derive sufficient
conditions under which the SDP is a tight relaxation of the QCQP. Hence under
these conditions, the global solution of QCQP can be obtained in polynomial
time.
"
"stat.TH","  The reversed aging intensity function is defined as the ratio of the
instantaneous reversed hazard rate to the baseline value of the reversed hazard
rate. It analyzes the aging property quantitatively, the higher the reversed
aging intensity, the weaker the tendency of aging. In this paper, a family of
generalized reversed aging intensity functions is introduced and studied. Those
functions depend on a real parameter. If the parameter is positive they
characterize uniquely the distribution functions of univariate positive
absolutely continuous random variables, in the opposite case they characterize
families of distributions. Furthermore, the generalized reversed aging
intensity orders are defined and studied. Finally, several numerical examples
are given.
"
"stat.TH","  In many applications, we are given access to noisy modulo samples of a smooth
function with the goal being to robustly unwrap the samples, i.e., to estimate
the original samples of the function. In a recent work, Cucuringu and Tyagi
proposed denoising the modulo samples by first representing them on the unit
complex circle and then solving a smoothness regularized least squares problem
-- the smoothness measured w.r.t the Laplacian of a suitable proximity graph
$G$ -- on the product manifold of unit circles. This problem is a quadratically
constrained quadratic program (QCQP) which is nonconvex, hence they proposed
solving its sphere-relaxation leading to a trust region subproblem (TRS). In
terms of theoretical guarantees, $\ell_2$ error bounds were derived for (TRS).
These bounds are however weak in general and do not really demonstrate the
denoising performed by (TRS).
  In this work, we analyse the (TRS) as well as an unconstrained relaxation of
(QCQP). For both these estimators we provide a refined analysis in the setting
of Gaussian noise and derive noise regimes where they provably denoise the
modulo observations w.r.t the $\ell_2$ norm. The analysis is performed in a
general setting where $G$ is any connected graph.
"
"stat.TH","  We consider inference for high-dimensional exchangeable arrays where the
dimension may be much larger than the cluster sizes. Specifically, we consider
separately and jointly exchangeable arrays that correspond to multiway
clustered and polyadic data, respectively. Such exchangeable arrays have seen a
surge of applications in empirical economics. However, both exchangeability
concepts induce highly complicated dependence structures, which poses a
significant challenge for inference in high dimensions. In this paper, we first
derive high-dimensional central limit theorems (CLTs) over the rectangles for
the exchangeable arrays. Building on the high-dimensional CLTs, we develop
novel multiplier bootstraps for the exchangeable arrays and derive their finite
sample error bounds in high dimensions. The derivations of these theoretical
results rely on new technical tools such as Hoeffding-type decomposition and
maximal inequalities for the degenerate components in the Hoeffiding-type
decomposition for the exchangeable arrays. We illustrate applications of our
bootstrap methods to robust inference in demand analysis, robust inference in
extended gravity analysis, and penalty choice for $\ell_1$-penalized regression
under multiway cluster sampling.
"
"stat.TH","  Assuming that a reflected Ornstein-Uhlenbeck state process is observed at
discrete time instants, we propose generalized moment estimators to estimate
all drift and diffusion parameters via the celebrated ergodic theorem. With the
sampling time step h > 0 arbitrarily fixed, we prove the strong consistency and
asymptotic normality of our estimators as the sampling size n tends to
infinity. This provides a complete solution to an open problem left in Hu et
al. [5].
"
"stat.TH","  In this paper, we consider possibly misspecified stochastic differential
equation models driven by L\'{e}vy processes. Regardless of whether the driving
noise is Gaussian or not, Gaussian quasi-likelihood estimator can estimate
unknown parameters in the drift and scale coefficients. However, in the
misspecified case, the asymptotic distribution of the estimator varies by the
correction of the misspecification bias, and consistent estimators for the
asymptotic variance proposed in the correctly specified case may lose
theoretical validity. As one of its solutions, we propose a bootstrap method
for approximating the asymptotic distribution. We show that our bootstrap
method theoretically works in both correctly specified case and misspecified
case without assuming the precise distribution of the driving noise.
"
"stat.TH","  The problem of generating random samples of high-dimensional posterior
distributions is considered. The main results consist of non-asymptotic
computational guarantees for Langevin-type MCMC algorithms which scale
polynomially in key quantities such as the dimension of the model, the desired
precision level, and the number of available statistical measurements. As a
direct consequence, it is shown that posterior mean vectors as well as
optimisation based maximum a posteriori (MAP) estimates are computable in
polynomial time, with high probability under the distribution of the data.
These results are complemented by statistical guarantees for recovery of the
ground truth parameter generating the data.
  Our results are derived in a general high-dimensional non-linear regression
setting (with Gaussian process priors) where posterior measures are not
necessarily log-concave, employing a set of local `geometric' assumptions on
the parameter space, and assuming that a good initialiser of the algorithm is
available. The theory is applied to a representative non-linear example from
PDEs involving a steady-state Schr\""odinger equation.
"
"stat.TH","  Motivated by some cutting edge circular data such as from Smart Home
technologies and roulette spins from online and casino, we construct some new
rich classes of discrete distributions on the circle. We give four new general
methods of construction, namely (i) maximum entropy, (ii) centered wrapping,
(iii) marginalized and (iv) conditionalized methods. We motivate these methods
on the line and then work on the circular case and provide some properties to
gain insight into these constructions. We mainly focus on the last two methods
(iii) and (iv) in the context of circular location families, as they are
amenable to general methodology. We show that the marginalized and
conditionalized discrete circular location families inherit important
properties from their parent continuous families. In particular, for the von
Mises and wrapped Cauchy as the parent distribution, we examine their
properties including the maximum likelihood estimators, the hypothesis test for
uniformity and give a test of serial independence. Using our discrete circular
distributions, we demonstrate how to determine changepoint when the data arise
in a sequence and how to fit mixtures of this distribution. Illustrative
examples are given which triggered the work. For example, for roulette data, we
test for uniformity (unbiasedness) , test for serial correlation, detect
changepoint in streaming roulette-spins data, and fit mixtures. We analyse a
smart home data using our mixtures. We examine the effect of ignoring
discreteness of the underlying population, and discuss marginalized versus
conditionalized approaches. We give various extensions of the families with
skewness and kurtosis, to those supported on an irregular lattice, and discuss
potential extension to general manifolds by showing a construction on the torus
"
"stat.TH","  In this paper, we apply doubly robust approach to estimate, when some
covariates are given, the conditional average treatment effect under
parametric, semiparametric and nonparametric structure of the nuisance
propensity score and outcome regression models. We then conduct a systematic
study on the asymptotic distributions of nine estimators with different
combinations of estimated propensity score and outcome regressions. The study
covers the asymptotic properties with all models correctly specified; with
either propensity score or outcome regressions locally / globally misspecified;
and with all models locally / globally misspecified. The asymptotic variances
are compared and the asymptotic bias correction under model-misspecification is
discussed. The phenomenon that the asymptotic variance, with
model-misspecification, could sometimes be even smaller than that with all
models correctly specified is explored. We also conduct a numerical study to
examine the theoretical results.
"
"stat.TH","  We study the stochastic convergence of the Ces\`{a}ro mean of a sequence of
random variables. These arise naturally in statistical problems that have a
sequential component, where the sequence of random variables is typically
derived from a sequence of estimators computed on data. We show that
establishing a rate of convergence in probability for a sequence is not
sufficient in general to establish a rate in probability for its Ces\`{a}ro
mean. We also present several sets of conditions on the sequence of random
variables that are sufficient to guarantee a rate of convergence for its
Ces\`{a}ro mean. We identify common settings in which these sets of conditions
hold.
"
"stat.TH","  Non-asymptotic bounds for Gaussian and bootstrap approximation have recently
attracted significant interest in high-dimensional statistics. This paper
studies Berry-Esseen bounds for such approximations (with respect to the
multivariate Kolmogorov distance), in the context of a sum of $n$ random
vectors that are $p$-dimensional and i.i.d. Up to now, a growing line of work
has established bounds with mild logarithmic dependence on $p$. However, the
problem of developing corresponding bounds with near $n^{-1/2}$ dependence on
$n$ has remained largely unresolved. Within the setting of random vectors that
have sub-Gaussian entries, this paper establishes bounds with near $n^{-1/2}$
dependence, for both Gaussian and bootstrap approximation. In addition, the
proofs are considerably distinct from other recent approaches.
"
"stat.TH","  Conditional heteroscedastic (CH) models are routinely used to analyze
financial datasets. The classical models such as ARCH-GARCH with time-invariant
coefficients are often inadequate to describe frequent changes over time due to
market variability. However we can achieve significantly better insight by
considering the time-varying analogues of these models. In this paper, we
propose a Bayesian approach to the estimation of such models and develop
computationally efficient MCMC algorithm based on Hamiltonian Monte Carlo (HMC)
sampling. We also established posterior contraction rates with increasing
sample size in terms of the average Hellinger metric. The performance of our
method is compared with frequentist estimates and estimates from the time
constant analogues. To conclude the paper we obtain time-varying parameter
estimates for some popular Forex (currency conversion rate) and stock market
datasets.
"
"stat.TH","  Count-valued time series data are routinely collected in many application
areas. We are particularly motivated to study the count time series of daily
new cases, arising from COVID-19 spread. First, we propose a Bayesian framework
to study time-varying semiparametric AR(p) model for count and then extend it
to propose a time-varying INGARCH model considering the rapid changes in the
spread. We calculate posterior contraction rates of the proposed Bayesian
methods with respect to average Hellinger metric. Our proposed structures of
the models are amenable to Hamiltonian Monte Carlo (HMC) sampling for efficient
computation. We substantiate our methods by simulations that show superiority
compared to some of the close existing methods. Finally we analyze the daily
time series data of newly confirmed cases to study its spread through different
government interventions.
"
"stat.TH","  Researchers currently use a number of approaches to predict and substantiate
information-computation gaps in high-dimensional statistical estimation
problems. A prominent approach is to characterize the limits of restricted
models of computation, which on the one hand yields strong computational lower
bounds for powerful classes of algorithms and on the other hand helps guide the
development of efficient algorithms. In this paper, we study two of the most
popular restricted computational models, the statistical query framework and
low-degree polynomials, in the context of high-dimensional hypothesis testing.
Our main result is that under mild conditions on the testing problem, the two
classes of algorithms are essentially equivalent in power. As corollaries, we
obtain new statistical query lower bounds for sparse PCA, tensor PCA and
several variants of the planted clique problem.
"
"stat.TH","  Subgraph counts play a central role in both graph limit theory and network
data analysis. In recent years, substantial progress has been made in the area
of uncertainty quantification for these functionals; several procedures are now
known to be consistent for the problem. In this paper, we propose a new class
of multiplier bootstraps for count functionals. We show that a bootstrap
procedure with a multiplicative weights exhibits higher-order correctness under
appropriate sparsity conditions. Since this bootstrap is computationally
expensive, we propose linear and quadratic approximations to the multiplier
bootstrap, which correspond to the first and second-order Hayek projections of
an approximating U-statistic, respectively. We show that the quadratic
bootstrap procedure achieves higher-order correctness under analogous
conditions to the multiplicative bootstrap while having much better
computational properties. We complement our theoretical results with a
simulation study and verify that our procedure offers state-of-the-art
performance for several functionals.
"
"stat.TH","  It has been observed that certain loss functions can render deep-learning
pipelines robust against flaws in the data. In this paper, we support these
empirical findings with statistical theory. We especially show that
empirical-risk minimization with unbounded, Lipschitz-continuous loss
functions, such as the least-absolute deviation loss, Huber loss, Cauchy loss,
and Tukey's biweight loss, can provide efficient prediction under minimal
assumptions on the data. More generally speaking, our paper provides
theoretical evidence for the benefits of robust loss functions in deep
learning.
"
"stat.TH","  Causal mediation analysis has historically been limited in two important
ways: (i) a focus has traditionally been placed on binary treatments and static
interventions, and (ii) direct and indirect effect decompositions have been
pursued that are only identifiable in the absence of intermediate confounders
affected by treatment. We present a theoretical study of an (in)direct effect
decomposition of the population intervention effect, defined by stochastic
interventions jointly applied to the treatment and mediators. In contrast to
existing proposals, our causal effects can be evaluated regardless of whether a
treatment is categorical or continuous and remain well-defined even in the
presence of intermediate confounders affected by treatment. Our (in)direct
effects are identifiable without a restrictive assumption on cross-world
counterfactual independencies, allowing for substantive conclusions drawn from
them to be validated in randomized controlled trials. Beyond the novel effects
introduced, we provide a careful study of nonparametric efficiency theory
relevant for the construction of flexible, multiply robust estimators of our
(in)direct effects, while avoiding undue restrictions induced by assuming
parametric models of nuisance parameter functionals. To complement our
nonparametric estimation strategy, we introduce inferential techniques for
constructing confidence intervals and hypothesis tests, and discuss open source
software implementing the proposed methodology.
"
"stat.TH","  Copulas are becoming an essential tool in analyzing data and knowing local
copula bounds with a fixed value of a given measure of association is turning
into a prerequisite in the early stage of exploratory data analysis. These
bounds have been computed for Spearman's rho, Kendall's tau, and Blomqvist's
beta. The importance of another two measures of association, Spearman's
footrule and Gini's gamma, has been reconfirmed recently. It is the main
purpose of this paper to fill in the gap and present the mentioned local bounds
for these two measures as well. It turns out that this is a quite non-trivial
endeavor as the bounds are quasi-copulas that are not copulas for certain
values of the two measures. We also give relations between these two measures
of association and Blomqvist's beta.
"
"stat.TH","  We study the problem of testing discrete distributions with a focus on the
high probability regime. Specifically, given samples from one or more discrete
distributions, a property $\mathcal{P}$, and parameters $0< \epsilon, \delta
<1$, we want to distinguish {\em with probability at least $1-\delta$} whether
these distributions satisfy $\mathcal{P}$ or are $\epsilon$-far from
$\mathcal{P}$ in total variation distance. Most prior work in distribution
testing studied the constant confidence case (corresponding to $\delta =
\Omega(1)$), and provided sample-optimal testers for a range of properties.
While one can always boost the confidence probability of any such tester by
black-box amplification, this generic boosting method typically leads to
sub-optimal sample bounds.
  Here we study the following broad question: For a given property
$\mathcal{P}$, can we {\em characterize} the sample complexity of testing
$\mathcal{P}$ as a function of all relevant problem parameters, including the
error probability $\delta$? Prior to this work, uniformity testing was the only
statistical task whose sample complexity had been characterized in this
setting. As our main results, we provide the first algorithms for closeness and
independence testing that are sample-optimal, within constant factors, as a
function of all relevant parameters. We also show matching
information-theoretic lower bounds on the sample complexity of these problems.
Our techniques naturally extend to give optimal testers for related problems.
To illustrate the generality of our methods, we give optimal algorithms for
testing collections of distributions and testing closeness with unequal sized
samples.
"
"stat.TH","  This paper introduces vector copulas and establishes a vector version of
Sklar's theorem. The latter provides a theoretical justification for the use of
vector copulas to characterize nonlinear or rank dependence between a finite
number of random vectors (robust to within vector dependence), and to construct
multivariate distributions with any given non-overlapping multivariate
marginals. We construct Elliptical, Archimedean, and Kendall families of vector
copulas and present algorithms to generate data from them. We introduce a
concordance ordering for two random vectors with given within-dependence
structures and generalize Spearman's rho to random vectors. Finally, we
construct empirical vector copulas and show their consistency under mild
conditions.
"
"stat.TH","  The Frisch--Waugh--Lovell Theorem states the equivalence of the coefficients
from the full and partial regressions. I further show the equivalence between
various standard errors. Applying the new result to stratified experiments
reveals the discrepancy between model-based and design-based standard errors.
"
"stat.TH","  In applications such as rank aggregation, mixture models for permutations are
frequently used when the population exhibits heterogeneity. In this work, we
study the widely used Mallows mixture model. In the high-dimensional setting,
we propose a polynomial-time algorithm that learns a Mallows mixture of
permutations on $n$ elements with the optimal sample complexity that is
proportional to $\log n$, improving upon previous results that scale
polynomially with $n$. In the high-noise regime, we characterize the optimal
dependency of the sample complexity on the noise parameter. Both objectives are
accomplished by first studying demixing permutations under a noiseless query
model using groups of pairwise comparisons, which can be viewed as moments of
the mixing distribution, and then extending these results to the noisy Mallows
model by simulating the noiseless oracle.
"
"stat.TH","  Motivated by the concept of degeneracy in biology (Edelman, Gally 2001), we
establish a first connection between the Multiplicity Principle (Ehresmann,
Vanbremeersch 2007) and mathematical statistics. Specifically, we exhibit two
families of statistical tests that satisfy this principle to achieve the
detection of a signal in noise.
"
"stat.TH","  This article provides an introduction to the asymptotic analysis of
covariance parameter estimation for Gaussian processes. Maximum likelihood
estimation is considered. The aim of this introduction is to be accessible to a
wide audience and to present some existing results and proof techniques from
the literature. The increasing-domain and fixed-domain asymptotic settings are
considered. Under increasing-domain asymptotics, it is shown that in general
all the components of the covariance parameter can be estimated consistently by
maximum likelihood and that asymptotic normality holds. In contrast, under
fixed-domain asymptotics, only some components of the covariance parameter,
constituting the microergodic parameter, can be estimated consistently. Under
fixed-domain asymptotics, the special case of the family of isotropic Mat\'ern
covariance functions is considered. It is shown that only a combination of the
variance and spatial scale parameter is microergodic. A consistency and
asymptotic normality proof is sketched for maximum likelihood estimators.
"
"stat.TH","  In this article, we consider the problem of inverting the exponential Radon
transform of a function in the presence of noise. We propose a kernel estimator
to estimate the true function, analogous to the one proposed by Korostel\""{e}v
and Tsybakov in their article `Optimal rates of convergence of estimators in a
probabilistic setup of tomography problem', Problems of Information
Transmission, 27:73-81,1991. For the estimator proposed in this article, we
then show that it converges to the true function at a minimax optimal rate.
"
"stat.TH","  The Portmanteau test provides the vanilla method for detecting serial
correlations in classical univariate time series analysis. The method is
extended to the case of observations from a locally stationary functional time
series. Asymptotic critical values are obtained by a suitable block multiplier
bootstrap procedure. The test is shown to asymptotically hold its level and to
be consistent against general alternatives.
"
"stat.TH","  We propose forecast encompassing tests for the Expected Shortfall (ES)
jointly with the Value at Risk (VaR) based on flexible link (or combination)
functions. Our setup allows testing encompassing for convex forecast
combinations and for link functions which preclude crossings of the combined
VaR and ES forecasts. As the tests based on these link functions involve
parameters which are on the boundary of the parameter space under the null
hypothesis, we derive and base our tests on nonstandard asymptotic theory on
the boundary. Our simulation study shows that the encompassing tests based on
our new link functions outperform tests based on unrestricted linear link
functions for one-step and multi-step forecasts. We further illustrate the
potential of the proposed tests in a real data analysis for forecasting VaR and
ES of the S&P 500 index.
"
"stat.TH","  A novel framework is developed to intrinsically analyze sparsely observed
Riemannian functional data. It features four innovative components: a
frame-independent covariance function, a smooth vector bundle termed covariance
vector bundle, a parallel transport and a smooth bundle metric on the
covariance vector bundle. The introduced intrinsic covariance function links
estimation of covariance structure to smoothing problems that involve raw
covariance observations derived from sparsely observed Riemannian functional
data, while the covariance vector bundle provides a rigorous mathematical
foundation for formulating the smoothing problems. The parallel transport and
the bundle metric together make it possible to measure fidelity of fit to the
covariance function. They also plays a critical role in quantifying the quality
of estimators for the covariance function. As an illustration, based on the
proposed framework, we develop a local linear smoothing estimator for the
covariance function, analyze its theoretical properties, and provide numerical
demonstration via simulated and real datasets. The intrinsic feature of the
framework makes it applicable to not only Euclidean submanifolds but also
manifolds without a canonical ambient space.
"
"stat.TH","  This paper is a survey of recent contributions on estimation in stochastic
differential equations with mixed-effects. These models involve N stochastic
differential equations with common drift and diffusion functions but random
parameters that allow for differences between processes. The main objective is
to estimate the distribution of the random effects and possibly other fixed
parameters that are common to the N processes. While many algorithms have been
proposed, the theoretical aspects related to estimation have been little
studied. This review article focuses only on theoretical inference for
stochastic differential equations with mixed-effects. It has so far only been
considered in some very specific classes of mixed-effect diffusion models,
observed without measurement error, where explicit estimators can be defined.
Within this framework, the asymptotic properties of several estimators, either
parametric or nonparametric, are discussed. Different schemes of observations
are considered according to the approach, associating a large number of
individuals with, in most cases, high-frequency observations of the
trajectories.
"
"stat.TH","  Despite the rapid development of computational hardware, the treatment of
large and high dimensional data sets is still a challenging problem. This paper
provides a twofold contribution to the topic. First, we propose a Gaussian
Mixture Model in conjunction with a reduction of the dimensionality of the data
in each component of the model by principal component analysis, called PCA-GMM.
To learn the (low dimensional) parameters of the mixture model we propose an EM
algorithm whose M-step requires the solution of constrained optimization
problems. Fortunately, these constrained problems do not depend on the usually
large number of samples and can be solved efficiently by an (inertial) proximal
alternating linearized minimization algorithm. Second, we apply our PCA-GMM for
the superresolution of 2D and 3D material images based on the approach of
Sandeep and Jacob. Numerical results confirm the moderate influence of the
dimensionality reduction on the overall superresolution result.
"
"stat.TH","  Truncated singular value decomposition is a reduced version of the singular
value decomposition in which only a few largest singular values are retained.
This paper presents a perturbation analysis for the truncated singular value
decomposition for real matrices. In the first part, we provide perturbation
expansions for the singular value truncation of order $r$. We extend
perturbation results for the singular subspace decomposition to derive the
first-order perturbation expansion of the truncated operator about a matrix
with rank no less than $r$. Observing that the first-order expansion can be
greatly simplified when the matrix has exact rank $r$, we further show that the
singular value truncation admits a simple second-order perturbation expansion
about a rank-$r$ matrix. In the second part of the paper, we introduce the
first-known error bound on the linear approximation of the truncated singular
value decomposition of a perturbed rank-$r$ matrix. Our bound only depends on
the least singular value of the unperturbed matrix and the norm of the
perturbation matrix. Intriguingly, while the singular subspaces are known to be
extremely sensitive to additive noises, the proposed error bound holds
universally for perturbations with arbitrary magnitude.
"
"stat.TH","  Linear regression is perhaps one of the most popular statistical concepts,
which permeates almost every scientific field of study. Due to the technical
simplicity and wide applicability of linear regression, attention is almost
always quickly directed to the algorithmic or computational side of linear
regression. In particular, the underlying mathematics of stochastic linear
regression itself as an entity usually gets either a peripheral treatment or a
relatively in-depth but ad hoc treatment depending on the type of concerned
problems; in other words, compared to the extensiveness of the study of
mathematical properties of the ""derivatives"" of stochastic linear regression
such as the least squares estimator, the mathematics of stochastic linear
regression itself seems to have not yet received a due intrinsic treatment.
Apart from the conceptual importance, a consequence of an insufficient or
possibly inaccurate understanding of stochastic linear regression would be the
recurrence for the role of stochastic linear regression in the important (and
more sophisticated) context of structural equation modeling to be misperceived
or taught in a misleading way. We believe this pity is rectifiable when the
fundamental concepts are correctly classified. Accompanied by some
illustrative, distinguishing examples and counterexamples, we intend to pave
out the mathematical framework for stochastic linear regression, in a rigorous
but non-technical way, by giving new results and pasting together several
fundamental known results that are, we believe, both enlightening and
conceptually useful, and that had not yet been systematically documented in the
related literature. As a minor contribution, the way we arrange the fundamental
known results would be the first attempt in the related literature.
"
"stat.TH","  In high dimensional setting, the facts that the classical ridge regression
method cannot perform model selection on its own and it introduces large bias
make this method an unsatisfactory tool for analyzing high dimensional linear
models. In this paper, we propose the debiased and threshold ridge regression
method which solves these drawbacks. Besides, focus on performing statistical
inference and prediction of linear combinations of parameters, we provide a
normal approximation theorem for the estimator and propose two bootstrap
algorithms which provide joint confidence regions and prediction regions for
the linear combinations. In statistical inference part, apart from the
dimension of parameters, we allow the number of linear combinations to grow as
sample size increases. From numerical experiments, we can see that the proposed
regression method is robust with the fluctuation in ridge parameter and reduces
estimation errors compared to classical and threshold ridge regression methods.
Apart from theoretical interests, the proposed algorithms can be applied to
disciplines such as econometrics, biology and etc.
"
"stat.TH","  We introduce a new notion of generalization -- Distributional Generalization
-- which roughly states that outputs of a classifier at train and test time are
close *as distributions*, as opposed to close in just their average error. For
example, if we mislabel 30% of dogs as cats in the train set of CIFAR-10, then
a ResNet trained to interpolation will in fact mislabel roughly 30% of dogs as
cats on the *test set* as well, while leaving other classes unaffected. This
behavior is not captured by classical generalization, which would only consider
the average error and not the distribution of errors over the input domain. Our
formal conjectures, which are much more general than this example, characterize
the form of distributional generalization that can be expected in terms of
problem parameters: model architecture, training procedure, number of samples,
and data distribution. We give empirical evidence for these conjectures across
a variety of domains in machine learning, including neural networks, kernel
machines, and decision trees. Our results thus advance our empirical
understanding of interpolating classifiers.
"
"stat.TH","  The concordance signature of a multivariate continuous distribution is the
vector of concordance probabilities for margins of all orders; it underlies the
bivariate and multivariate Kendall's tau measure of concordance. It is shown
that every attainable concordance signature is equal to the concordance
signature of a unique mixture of the extremal copulas, that is the copulas with
extremal correlation matrices consisting exclusively of 1's and -1's. This
result establishes that the set of attainable Kendall rank correlation matrices
of multivariate continuous distributions in arbitrary dimension is the set of
convex combinations of extremal correlation matrices, a set known as the cut
polytope. A methodology for testing the attainability of concordance signatures
using linear optimization and convex analysis is provided. The elliptical
copulas are shown to yield a strict subset of the attainable concordance
signatures as well as a strict subset of the attainable Kendall rank
correlation matrices; the Student t copula is seen to converge to a mixture of
extremal copulas sharing its concordance signature with all elliptical
distributions that have the same correlation matrix. A method of estimating an
attainable concordance signature from data is derived and shown to correspond
to using standard estimates of Kendall's tau in the absence of ties. The
methodology has application to Monte Carlo simulations of dependent random
variables as well as expert elicitation of consistent systems of Kendall's tau
dependence measures.
"
"stat.TH","  We study here several variants of the covariates fine balance problem where
we generalize some of these problems and introduce a number of others. We
present here a comprehensive complexity study of the covariates problems
providing polynomial time algorithms, or a proof of NP-hardness. The polynomial
time algorithms described are mostly combinatorial and rely on network flow
techniques. In addition we present several fixed-parameter tractable results
for problems where the number of covariates and the number of levels of each
covariate are seen as a parameter.
"
"stat.TH","  In this paper an additive regression model for a symmetric positive-definite
matrix valued response and multiple scalar predictors is proposed. The model
exploits the abelian group structure inherited from either the Log-Cholesky
metric or the Log-Euclidean framework that turns the space of symmetric
positive-definite matrices into a Riemannian manifold and further a
bi-invariant Lie group. The additive model for responses in the space of
symmetric positive-definite matrices with either of these metrics is shown to
connect to an additive model on a tangent space. This connection not only
entails an efficient algorithm to estimate the component functions but also
allows to generalize the proposed additive model to general Riemannian
manifolds that might not have a Lie group structure. Optimal asymptotic
convergence rates and normality of the estimated component functions are also
established. Numerical studies show that the proposed model enjoys superior
numerical performance, especially when there are multiple predictors. The
practical merits of the proposed model are demonstrated by analyzing diffusion
tensor brain imaging data.
"
"stat.TH","  In this paper, we consider high-dimensional stationary processes where a new
observation is generated from a compressed version of past observations. The
specific evolution is modeled by an encoder-decoder structure. We estimate the
evolution with an encoder-decoder neural network and give upper bounds for the
expected forecast error under specific structural and sparsity assumptions. The
results are shown separately for conditions either on the absolutely regular
mixing coefficients or the functional dependence measure of the observed
process. In a quantitative simulation we discuss the behavior of the network
estimator under different model assumptions. We corroborate our theory by a
real data example where we consider forecasting temperature data.
"
"stat.TH","  In this note we consider the optimal design problem for estimating the slope
of a polynomial regression with no intercept at a given point, say z. In
contrast to previous work, which considers symmetric design spaces we
investigate the model on the interval $[0, a]$ and characterize those values of
$z$, where an explicit solution of the optimal design is possible.
"
"stat.TH","  The Frechet mean is a useful description of location for a probability
distribution on a metric space that is not necessarily a vector space. This
article considers simultaneous estimation of multiple Frechet means from a
decision-theoretic perspective, and in particular, the extent to which the
unbiased estimator of a Frechet mean can be dominated by a generalization of
the James-Stein shrinkage estimator. It is shown that if the metric space
satisfies a non-positive curvature condition, then this generalized James-Stein
estimator asymptotically dominates the unbiased estimator as the dimension of
the space grows. These results hold for a large class of distributions on a
variety of spaces - including Hilbert spaces - and therefore partially extend
known results on the applicability of the James-Stein estimator to non-normal
distributions on Euclidean spaces. Simulation studies on metric trees and
symmetric-positive-definite matrices are presented, numerically demonstrating
the efficacy of this generalized James-Stein estimator.
"
"stat.TH","  Given a symmetric network with $n$ nodes, how to estimate the number of
communities $K$ is a fundamental problem. We propose Stepwise Goodness-of-Fit
(StGoF) as a new approach to estimating $K$. For $m = 1, 2, \ldots$, StGoF
alternately uses a community detection step (pretending $m$ is the correct
number of communities) and a goodness-of-fit step. We use SCORE \cite{SCORE}
for community detection, and propose a new goodness-of-fit measure. Denote the
goodness-of-fit statistic in step $m$ by $\psi_n^{(m)}$. We show that as $n
\rightarrow \infty$, $\psi_n^{(m)} \rightarrow N(0,1)$ when $m = K$ and
$\psi_n^{(m)} \rightarrow \infty$ in probability when $m < K$. Therefore, with
a proper threshold, StGoF terminates at $m = K$ as desired.
  We consider a broad setting where we allow severe degree heterogeneity, a
wide range of sparsity, and especially weak signals. In particular, we propose
a measure for signal-to-noise ratio (SNR) and show that there is a phase
transition: when $\mathrm{SNR} \rightarrow 0$ as $n \rightarrow \infty$,
consistent estimates for $K$ do not exist, and when $\mathrm{SNR} \rightarrow
\infty$, StGoF is consistent, uniformly for a broad class of settings. In this
sense, StGoF achieves the optimal phase transition. Stepwise testing algorithms
of similar kind (e.g., \cite{wang2017likelihood, ma2018determining}) are known
to face analytical challenges. We overcome the challenges by using a different
design in the stepwise algorithm and by deriving sharp results in the
under-fitting case $(m < K)$ and the null case ($m = K$). The key to our
analysis is to show that SCORE has the {\it Non-Splitting Property (NSP)}. The
NSP is non-obvious, so additional to rigorous proofs, we also provide an
intuitive explanation.
"
"stat.TH","  Recent advances in quantized compressed sensing and high-dimensional
estimation have shown that signal recovery is even feasible under strong
non-linear distortions in the observation process. An important characteristic
of associated guarantees is uniformity, i.e., recovery succeeds for an entire
class of structured signals with a fixed measurement ensemble. However, despite
significant results in various special cases, a general understanding of
uniform recovery from non-linear observations is still missing. This paper
develops a unified approach to this problem under the assumption i.i.d.
sub-Gaussian measurement vectors. Our main result shows that a simple
least-squares estimator with any convex constraint can serve as a universal
recovery strategy, which is outlier robust and does not require explicit
knowledge of the underlying non-linearity. Based on empirical process theory, a
key technical novelty is an approximative increment condition that can be
implemented for all common types of non-linear models. This flexibility allows
us to apply our approach to a variety of problems in quantized compressed
sensing and high-dimensional statistics, leading to several new and improved
guarantees. Each of these applications is accompanied by a conceptually simple
and systematic proof, which does not rely on any deeper properties of the
observation model. On the other hand, known local stability properties can be
incorporated into our framework in a plug-and-play manner, thereby implying
near-optimal error bounds.
"
"stat.TH","  We propose tests for the null hypothesis that the law of a complex-valued
random vector is circularly symmetric. The test criteria are formulated as
$L^2$-type criteria based on empirical characteristic functions, and they are
convenient from the computational point of view. Asymptotic as well as
Monte-Carlo results are presented. Applications on real data are also reported.
An R package called CircSymTest is available from the authors.
"
"stat.TH","  We propose a new model selection method, the posterior averaging information
criterion, for Bayesian model assessment from a predictive perspective. The
theoretical foundation is built on the Kullback-Leibler divergence to quantify
the similarity between the proposed candidate model and the underlying true
model. From a Bayesian perspective, our method evaluates the candidate models
over the entire posterior distribution in terms of predicting a future
independent observation. Without assuming that the true distribution is
contained in the candidate models, the new criterion is developed by correcting
the asymptotic bias of the posterior mean of the log-likelihood against its
expected log-likelihood. It can be generally applied even for Bayesian models
with degenerate non-informative prior. The simulation in both normal and
binomial settings demonstrates decent small sample performance.
"
"stat.TH","  Consider the problem of estimating the local average treatment effect with an
instrument variable, where the instrument unconfoundedness holds after
adjusting for a set of measured covariates. Several unknown functions of the
covariates need to be estimated through regression models, such as instrument
propensity score and treatment and outcome regression models. We develop a
computationally tractable method in high-dimensional settings where the numbers
of regression terms are close to or larger than the sample size. Our method
exploits regularized calibrated estimation, which involves Lasso penalties but
carefully chosen loss functions for estimating coefficient vectors in these
regression models, and then employs a doubly robust estimator for the treatment
parameter through augmented inverse probability weighting. We provide rigorous
theoretical analysis to show that the resulting Wald confidence intervals are
valid for the treatment parameter under suitable sparsity conditions if the
instrument propensity score model is correctly specified, but the treatment and
outcome regression models may be misspecified. For existing high-dimensional
methods, valid confidence intervals are obtained for the treatment parameter if
all three models are correctly specified. We evaluate the proposed methods via
extensive simulation studies and an empirical application to estimate the
returns to education.
"
"stat.TH","  We study the problem of predicting as well as the best linear predictor in a
bounded Euclidean ball with respect to the squared loss. When only boundedness
of the data generating distribution is assumed, we establish that the least
squares estimator constrained to a bounded Euclidean ball does not attain the
classical $O(d/n)$ excess risk rate, where $d$ is the dimension of the
covariates and $n$ is the number of samples. In particular, we construct a
bounded distribution such that the constrained least squares estimator incurs
an excess risk of order $\Omega(d^{3/2}/n)$ hence refuting a recent conjecture
of Ohad Shamir [JMLR 2015]. In contrast, we observe that non-linear predictors
can achieve the optimal rate $O(d/n)$ with no assumptions on the distribution
of the covariates. We discuss additional distributional assumptions sufficient
to guarantee an $O(d/n)$ excess risk rate for the least squares estimator.
Among them are certain moment equivalence assumptions often used in the robust
statistics literature. While such assumptions are central in the analysis of
unbounded and heavy-tailed settings, our work indicates that in some cases,
they also rule out unfavorable bounded distributions.
"
"stat.TH","  Given an inhomogeneous chain embedded in a noisy image, we consider the
conditions under which such an embedded chain is detectable. Many applications,
such as detecting moving objects, detecting ship wakes, can be abstracted as
the detection on the existence of chains. In this work, we provide the
detection algorithm with low order of computation complexity to detect the
chain and the optimal theoretical detectability regarding SNR (signal to noise
ratio) under the normal distribution model. Specifically, we derive an
analytical threshold that specifies what is detectable. We design a longest
significant chain detection algorithm, with computation complexity in the order
of $O(n\log n)$. We also prove that our proposed algorithm is asymptotically
powerful, which means, as the dimension $n \rightarrow \infty$, the probability
of false detection vanishes. We further provide some simulated examples and a
real data example, which validate our theory.
"
"stat.TH","  This paper developed an inference problem for Vasicek model driven by a
general Gaussian process. We construct a least squares estimator and a moment
estimator for the drift parameters of the Vasicek model, and we prove the
consistency and the asymptotic normality. Our approach extended the result of
Xiao and Yu (2018) for the case when noise is a fractional Brownian motion with
Hurst parameter H \in [1/2,1).
"
